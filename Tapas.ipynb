{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "48339e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at ./Tapas and are newly initialized: ['output_bias', 'column_output_bias', 'output_weights', 'column_output_weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "\n",
    "# Load the model and tokenizer from the local directory\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"./Tapas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "455346c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: [ u n u s e d 3 4 ]\n"
     ]
    }
   ],
   "source": [
    "# Ask Questions\n",
    "inputs = tokenizer(\n",
    "    table=order_data,\n",
    "    queries=[\"What segment has the highest sales?\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Process the outputs to get the answer\n",
    "logits = outputs.logits\n",
    "predicted_answer = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(), skip_special_tokens=True)\n",
    "print(f\"Predicted answer: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "59f63600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'BERT',\n",
       " 'BERT.ipynb',\n",
       " 'fine-tuned-tapas',\n",
       " 'fine_tuned_tabert',\n",
       " 'fraudTest.csv',\n",
       " 'fraudTrain.csv',\n",
       " 'Fraudulent transactions-Bkp02Sept2024.ipynb',\n",
       " 'Fraudulent transactions-Copy1.ipynb',\n",
       " 'Fraudulent transactions.ipynb',\n",
       " 'fraud_detection_nn.pth',\n",
       " 'fraud_detection_nn_es.pth',\n",
       " 'LIME_implementation',\n",
       " 'LIME_implementation.zip',\n",
       " 'logs',\n",
       " 'orders.csv',\n",
       " 'orders_updated.csv',\n",
       " 'proj2',\n",
       " 'results',\n",
       " 'Tapas',\n",
       " 'Tapas.ipynb',\n",
       " 'tokenized_data.pt',\n",
       " 'train_df.csv',\n",
       " 'train_df_new.csv',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f2fff661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2018-152156   11/8/2018  11/11/2018    Second Class    CG-12520   \n",
      "1       2  CA-2018-152156   11/8/2018  11/11/2018    Second Class    CG-12520   \n",
      "2       3  CA-2018-138688   6/12/2018   6/16/2018    Second Class    DV-13045   \n",
      "3       4  US-2017-108966  10/11/2017  10/18/2017  Standard Class    SO-20335   \n",
      "4       5  US-2017-108966  10/11/2017  10/18/2017  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment Country/Region             City  ...  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "1      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "\n",
      "  Postal Code  Region       Product ID         Category Sub-Category  \\\n",
      "0     42420.0   South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1     42420.0   South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2     90036.0    West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3     33311.0   South  FUR-TA-10000577        Furniture       Tables   \n",
      "4     33311.0   South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales  Quantity  \\\n",
      "0                  Bush Somerset Collection Bookcase  261.9600         2   \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n",
      "\n",
      "   Discount    Profit  \n",
      "0      0.00   41.9136  \n",
      "1      0.00  219.5820  \n",
      "2      0.00    6.8714  \n",
      "3      0.45 -383.0310  \n",
      "4      0.20    2.5164  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Detect the encoding of the file\n",
    "with open(\"orders.csv\", \"rb\") as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Read the CSV file with the detected encoding\n",
    "order_data = pd.read_csv(\"orders.csv\", encoding=result['encoding'])\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(order_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7ec0dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " Row ID             0\n",
      "Order ID           0\n",
      "Order Date         0\n",
      "Ship Date          0\n",
      "Ship Mode          0\n",
      "Customer ID        0\n",
      "Customer Name      0\n",
      "Segment            0\n",
      "Country/Region     0\n",
      "City               0\n",
      "State              0\n",
      "Postal Code       11\n",
      "Region             0\n",
      "Product ID         0\n",
      "Category           0\n",
      "Sub-Category       0\n",
      "Product Name       0\n",
      "Sales              0\n",
      "Quantity           0\n",
      "Discount           0\n",
      "Profit             0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 0\n",
      "Data types of each column:\n",
      " Row ID              int64\n",
      "Order ID           object\n",
      "Order Date         object\n",
      "Ship Date          object\n",
      "Ship Mode          object\n",
      "Customer ID        object\n",
      "Customer Name      object\n",
      "Segment            object\n",
      "Country/Region     object\n",
      "City               object\n",
      "State              object\n",
      "Postal Code       float64\n",
      "Region             object\n",
      "Product ID         object\n",
      "Category           object\n",
      "Sub-Category       object\n",
      "Product Name       object\n",
      "Sales             float64\n",
      "Quantity            int64\n",
      "Discount          float64\n",
      "Profit            float64\n",
      "dtype: object\n",
      "Basic statistics of the dataset:\n",
      "             Row ID   Postal Code         Sales     Quantity     Discount  \\\n",
      "count  9994.000000   9983.000000   9994.000000  9994.000000  9994.000000   \n",
      "mean   4997.500000  55245.233297    229.858001     3.789574     0.156203   \n",
      "std    2885.163629  32038.715955    623.245101     2.225110     0.206452   \n",
      "min       1.000000   1040.000000      0.444000     1.000000     0.000000   \n",
      "25%    2499.250000  23223.000000     17.280000     2.000000     0.000000   \n",
      "50%    4997.500000  57103.000000     54.490000     3.000000     0.200000   \n",
      "75%    7495.750000  90008.000000    209.940000     5.000000     0.200000   \n",
      "max    9994.000000  99301.000000  22638.480000    14.000000     0.800000   \n",
      "\n",
      "            Profit  \n",
      "count  9994.000000  \n",
      "mean     28.656896  \n",
      "std     234.260108  \n",
      "min   -6599.978000  \n",
      "25%       1.728750  \n",
      "50%       8.666500  \n",
      "75%      29.364000  \n",
      "max    8399.976000  \n",
      "Current length of the dataset: 9994\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = order_data.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = order_data.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicate_rows)\n",
    "\n",
    "# Optionally, remove duplicate rows\n",
    "order_data = order_data.drop_duplicates()\n",
    "\n",
    "# Check data types\n",
    "data_types = order_data.dtypes\n",
    "print(\"Data types of each column:\\n\", data_types)\n",
    "\n",
    "# Generate basic statistics\n",
    "statistics = order_data.describe()\n",
    "print(\"Basic statistics of the dataset:\\n\", statistics)\n",
    "\n",
    "current_length = len(order_data)\n",
    "print(f\"Current length of the dataset: {current_length}\")\n",
    "\n",
    "order_data=order_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d7f79a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current length of the dataset: 9983\n"
     ]
    }
   ],
   "source": [
    "current_length = len(order_data)\n",
    "print(f\"Current length of the dataset: {current_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c2bbbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_data=order_data[['Segment','Category','Sub-Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2354bf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Corporate</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Appliances</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9983 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Segment         Category Sub-Category\n",
       "0      Consumer        Furniture    Bookcases\n",
       "1      Consumer        Furniture       Chairs\n",
       "2     Corporate  Office Supplies       Labels\n",
       "3      Consumer        Furniture       Tables\n",
       "4      Consumer  Office Supplies      Storage\n",
       "...         ...              ...          ...\n",
       "9989   Consumer        Furniture  Furnishings\n",
       "9990   Consumer        Furniture  Furnishings\n",
       "9991   Consumer       Technology       Phones\n",
       "9992   Consumer  Office Supplies        Paper\n",
       "9993   Consumer  Office Supplies   Appliances\n",
       "\n",
       "[9983 rows x 3 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "02e9bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  6903,  2038,  1996,  3284,  4341,  1029,   102,  6903,\n",
      "          4341,  7325,  6694,  5971, 10347,  2188,  2436,  3156,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'numeric_values': tensor([[  nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan, 1000.,   nan, 1500.,   nan,   nan,  500.,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan]]), 'numeric_values_scale': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1.]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TapasTokenizer\n",
    "\n",
    "# Example table data\n",
    "data = {\n",
    "    'Segment': ['Consumer', 'Corporate', 'Home Office'],\n",
    "    'Sales': ['1000', '1500', '500']\n",
    "}\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "# Example question and answer\n",
    "questions = [\"What segment has the highest sales?\"]\n",
    "answer_coordinates = [[(1, 1)]]  # Coordinates for the answer cell\n",
    "answer_texts = [\"Corporate\"]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(\n",
    "    table=table,\n",
    "    queries=questions,\n",
    "    answer_coordinates=answer_coordinates,\n",
    "    answer_text=answer_texts,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7c288d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  6903,  2038,  1996,  3284,  4341,  1029,   102,  6903,\n",
      "          4341,  7325,  6694,  5971, 10347,  2188,  2436,  3156,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'numeric_values': tensor([[  nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan, 1000.,   nan, 1500.,   nan,   nan,  500.,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n",
      "           nan,   nan]]), 'numeric_values_scale': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1.]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TapasTokenizer\n",
    "\n",
    "# Example table data\n",
    "data = {\n",
    "    'Segment': ['Consumer', 'Corporate', 'Home Office'],\n",
    "    'Sales': [\"1000\", \"1500\", \"500\"]\n",
    "}\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "# Example question and answer\n",
    "questions = [\"What segment has the highest sales?\"]\n",
    "answer_coordinates = [[(1, 1)]]  # Coordinates for the answer cell\n",
    "answer_texts = [\"1500\"]  # The text in the answer cell\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\n",
    "\n",
    "# Tokenize the inputs\n",
    "try:\n",
    "    inputs = tokenizer(\n",
    "        table=table,\n",
    "        queries=questions,\n",
    "        answer_coordinates=answer_coordinates,\n",
    "        answer_text=answer_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    print(inputs)\n",
    "except ValueError as e:\n",
    "    print(f\"An error occurred during tokenization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "order_data=order_data.head(500)\n",
    "# Tokenize the entire dataset\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\n",
    "questions = [\"What segment has the highest sales?\"] * len(order_data)\n",
    "answer_coordinates = [[(0, 0)]] * len(order_data)  # Adjust coordinates as needed\n",
    "answer_texts = ['Consumer'] * len(order_data)  # Adjust answers as needed\n",
    "\n",
    "inputs = tokenizer(\n",
    "    table=order_data,\n",
    "    queries=questions,\n",
    "    answer_coordinates=answer_coordinates,\n",
    "    answer_text=answer_texts,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Save the tokenized data\n",
    "torch.save(inputs, \"tokenized_data.pt\")\n",
    "\n",
    "# Custom Dataset class to load pre-tokenized data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokenized_data['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.tokenized_data.items()}\n",
    "\n",
    "# Load the tokenized data\n",
    "tokenized_data = torch.load(\"tokenized_data.pt\")\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = CustomDataset(tokenized_data)\n",
    "eval_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "# Print dataset sizes to verify\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Load the model\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"./Tapas\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-tapas\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-tapas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f43ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TapasTokenizer.from_pretrained(\"./fine-tuned-tapas\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"./fine-tuned-tapas\")\n",
    "\n",
    "# Ask Questions\n",
    "inputs = tokenizer(\n",
    "    table=order_data,\n",
    "    queries=[\"What segment has the highest sales?\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Process the outputs to get the answer\n",
    "logits = outputs.logits\n",
    "predicted_answer = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(), skip_special_tokens=True)\n",
    "print(f\"Predicted answer: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "57ab26b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer:    start  14   end 6\n"
     ]
    }
   ],
   "source": [
    "# Get the logits directly\n",
    "logits = outputs.logits\n",
    "\n",
    "# Assuming logits is a 2D tensor with shape [batch_size, sequence_length]\n",
    "# Get the predicted token indices for start and end positions\n",
    "start_logits, end_logits = logits.split(256, dim=1)  # Adjust the split size based on your model's output\n",
    "\n",
    "# Squeeze the last dimension\n",
    "start_logits = start_logits.squeeze(-1)\n",
    "end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "# Get the predicted start and end positions\n",
    "start_index = torch.argmax(start_logits, dim=1).item()\n",
    "end_index = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "# Decode the answer from the table\n",
    "predicted_answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(inputs['input_ids'][0, start_index:end_index + 1])\n",
    ")\n",
    "print(f\"Predicted answer: {predicted_answer}   start  {start_index}   end {end_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "de97ee81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2054,  6903,  2038,  1996,  3284,  4341,  1029,   102,  6903,\n",
       "          4696,  4942,  1011,  4696,  7325,  7390,  2338, 18382,  2015,  7325,\n",
       "          7390,  8397,  5971,  2436,  6067, 10873,  7325,  7390,  7251,  7325,\n",
       "          2436,  6067,  5527,  7325,  7390, 23127,  7325,  2436,  6067,  2396,\n",
       "          7325,  2974, 11640,  7325,  2436,  6067, 14187,  2545,  7325,  2436,\n",
       "          6067, 22449,  7325,  7390,  7251,  7325,  2974, 11640,  7325,  2436,\n",
       "          6067,  3259,  7325,  2436,  6067, 14187,  2545,  2188,  2436,  2436,\n",
       "          6067, 22449,  2188,  2436,  2436,  6067, 14187,  2545,  7325,  2436,\n",
       "          6067,  5527,  7325,  2436,  6067,  5527,  7325,  2436,  6067,  2396,\n",
       "          7325,  2974, 11640,  7325,  2436,  6067, 14187,  2545,  5971,  2436,\n",
       "          6067,  2396,  5971,  2436,  6067, 22449,  7325,  7390,  8397,  7325,\n",
       "          7390,  7251,  7325,  2436,  6067, 14187,  2545,  7325,  2974, 16611,\n",
       "          7325,  7390,  2338, 18382,  2015,  7325,  2436,  6067, 14187,  2545,\n",
       "          7325,  7390, 23127,  7325,  2436,  6067, 11255,  2015,  7325,  2436,\n",
       "          6067,  2396,  7325,  2436,  6067, 14187,  2545,  7325,  2436,  6067,\n",
       "          2396,  2188,  2436,  2436,  6067,  3259,  5971,  2974, 11640,  5971,\n",
       "          7390, 23127,  2188,  2436,  2436,  6067, 11255,  2015,  2188,  2436,\n",
       "          7390,  2338, 18382,  2015,  2188,  2436,  7390,  8397,  2188,  2436,\n",
       "          2974, 11640,  5971,  2974, 11640,  5971,  2436,  6067,  5527,  5971,\n",
       "          2436,  6067,  5527,  5971,  2974, 16611,  5971,  2436,  6067, 14187,\n",
       "          2545,  7325,  2436,  6067,  5527,  7325,  2974, 16611,  7325,  2974,\n",
       "         11640,  7325,  2436,  6067, 14187,  2545,  7325,  2436,  6067, 10873,\n",
       "          7325,  7390, 23127,  7325,  7390,  8397,  5971,  2436,  6067,  3435,\n",
       "         24454,  2015,  5971,  2974, 11640,  7325,  2436,  6067,  5527,  7325,\n",
       "          2436,  6067,  3259,  7325,  7390,  8397,  7325,  2436,  6067,  3259,\n",
       "          7325,  2974, 16611,  7325,  2436,  6067, 14187,  2545,  7325,  2436,\n",
       "          6067,  2396,  7325,  2974, 16611,  7325,  2436,  6067, 14187,  2545,\n",
       "          7325,  2436,  6067,  3259,  7325,  7390, 23127,  2188,  2436,  7390,\n",
       "          8397,  5971,  2436,  6067,  2396,  5971,  2974, 11640,  7325,  2436,\n",
       "          6067,  3259,  7325,  2436,  6067, 14187,  2545,  7325,  2436,  6067,\n",
       "          3259,  7325,  7390,  8397,  7325,  7390, 23127,  7325,  2436,  6067,\n",
       "          5527,  5971,  2436,  6067, 14187,  2545,  5971,  7390, 23127,  5971,\n",
       "          2436,  6067,  5527,  7325,  7390, 23127,  5971,  2436,  6067, 22449,\n",
       "          5971,  2436,  6067, 14187,  2545,  7325,  2436,  6067,  2396,  7325,\n",
       "          2436,  6067,  5527,  5971,  2436,  6067, 11255,  2015,  2188,  2436,\n",
       "          2436,  6067,  5527,  7325,  7390,  8397,  7325,  2974, 16611,  7325,\n",
       "          2436,  6067, 10873,  2188,  2436,  2436,  6067,  5527,  5971,  2436,\n",
       "          6067,  2396,  5971,  2974, 11640,  5971,  2436,  6067,  3259,  7325,\n",
       "          2436,  6067,  3259,  7325,  7390, 23127,  7325,  2436,  6067, 14187,\n",
       "          2545,  2188,  2436,  2436,  6067, 14187,  2545,  2188,  2436,  7390,\n",
       "         23127,  7325,  2436,  6067, 14187,  2545,  5971,  2436,  6067, 22449,\n",
       "          2188,  2436,  2436,  6067,  3259,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "87d47896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "573c4019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 0\n",
      "Evaluation dataset size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at ./Tapas and are newly initialized: ['output_bias', 'column_output_bias', 'output_weights', 'column_output_weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 101\u001b[0m\n\u001b[0;32m     93\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     94\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     95\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     96\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     97\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[0;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine-tuned-tapas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1551\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[1;32m-> 1553\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_train_dataloader()\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;66;03m# Setting up training control variables:\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;66;03m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m total_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mworld_size\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:850\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    842\u001b[0m dataloader_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size,\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_collator,\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpin_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_pin_memory,\n\u001b[0;32m    847\u001b[0m }\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterableDataset):\n\u001b[1;32m--> 850\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_sampler()\n\u001b[0;32m    851\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[0;32m    852\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:821\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LengthGroupedSampler(\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps,\n\u001b[0;32m    815\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[0;32m    816\u001b[0m         lengths\u001b[38;5;241m=\u001b[39mlengths,\n\u001b[0;32m    817\u001b[0m         model_input_name\u001b[38;5;241m=\u001b[39mmodel_input_name,\n\u001b[0;32m    818\u001b[0m     )\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RandomSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import TapasTokenizer, TapasForQuestionAnswering, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure order_data is defined and loaded\n",
    "order_data = pd.DataFrame({\n",
    "    'Segment': ['Consumer', 'Corporate', 'Home Office'],\n",
    "    'Sales': [1000, 1500, 500]\n",
    "})\n",
    "\n",
    "current_length = len(order_data)\n",
    "rows_to_drop = current_length % 8\n",
    "if rows_to_drop > 0:\n",
    "    order_data = order_data[:-rows_to_drop]\n",
    "\n",
    "# Reset the index and convert all values to strings\n",
    "order_data.reset_index(drop=True, inplace=True)\n",
    "order_data = order_data.astype(str)\n",
    "\n",
    "# Save the updated dataset to a new CSV file\n",
    "order_data.to_csv(\"orders_updated.csv\", index=False)\n",
    "\n",
    "# Generate questions and answers for each row\n",
    "questions = [\"What segment has the highest sales?\"] * len(order_data)\n",
    "answer_coordinates = [[(0, 0)]] * len(order_data)  # Adjust coordinates as needed\n",
    "answer_texts = [\"Consumer\"] * len(order_data)  # Adjust answers as needed\n",
    "\n",
    "assert len(questions) == len(order_data), \"Questions list length does not match order_data length\"\n",
    "assert len(answer_coordinates) == len(order_data), \"Answer coordinates list length does not match order_data length\"\n",
    "assert len(answer_texts) == len(order_data), \"Answer texts list length does not match order_data length\"\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, questions, answer_coordinates, answer_texts):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.questions = questions\n",
    "        self.answer_coordinates = answer_coordinates\n",
    "        self.answer_texts = answer_texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.dataframe):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataframe with length {len(self.dataframe)}\")\n",
    "\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        table = pd.DataFrame([row])\n",
    "        print(f\"Processing row {idx}: {row}\")  # Debugging statement\n",
    "\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                table=table,\n",
    "                queries=[self.questions[idx]],\n",
    "                answer_coordinates=[self.answer_coordinates[idx]],\n",
    "                answer_text=[self.answer_texts[idx]],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization at index {idx}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        return inputs\n",
    "\n",
    "# Create instances of the custom dataset\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\n",
    "train_dataset = CustomDataset(order_data, tokenizer, questions, answer_coordinates, answer_texts)\n",
    "eval_dataset = CustomDataset(order_data, tokenizer, questions, answer_coordinates, answer_texts)\n",
    "\n",
    "# Print dataset sizes to verify\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Load the model from the local directory\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"./Tapas\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the custom DataLoader\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-tapas\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-tapas\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./fine-tuned-tapas\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"./fine-tuned-tapas\")\n",
    "\n",
    "# Ask Questions\n",
    "inputs = tokenizer(\n",
    "    table=order_data,\n",
    "    queries=[\"What segment has the highest sales?\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Process the outputs to get the answer\n",
    "logits = outputs.logits\n",
    "predicted_answer = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(), skip_special_tokens=True)\n",
    "print(f\"Predicted answer: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d1c36e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 9976\n",
      "Evaluation dataset size: 9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at ./Tapas and are newly initialized: ['output_bias', 'column_output_bias', 'output_weights', 'column_output_weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 7400: Segment                Consumer\n",
      "Category        Office Supplies\n",
      "Sub-Category           Supplies\n",
      "Name: 7400, dtype: object\n",
      "Error during tokenization at index 7400: iloc cannot enlarge its target object\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "iloc cannot enlarge its target object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[173], line 94\u001b[0m\n\u001b[0;32m     86\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     87\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     88\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     89\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     90\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[0;32m     97\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine-tuned-tapas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1787\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1784\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1787\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1788\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\accelerate\\data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[173], line 58\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during tokenization at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, idx\n",
      "Cell \u001b[1;32mIn[173], line 47\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging statement\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[0;32m     48\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m     49\u001b[0m         queries\u001b[38;5;241m=\u001b[39mquestions,\n\u001b[0;32m     50\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m     51\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_texts,\n\u001b[0;32m     52\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during tokenization at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:650\u001b[0m, in \u001b[0;36mTapasTokenizer.__call__\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(queries, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m    651\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    652\u001b[0m         queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m    653\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    654\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    655\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    656\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    657\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    658\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    659\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    660\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    661\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    662\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    663\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    664\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    665\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    666\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    667\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    669\u001b[0m     )\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m    672\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    673\u001b[0m         query\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    690\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:768\u001b[0m, in \u001b[0;36mTapasTokenizer.batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    766\u001b[0m     )\n\u001b[1;32m--> 768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    769\u001b[0m     table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    770\u001b[0m     queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m    771\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    772\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    773\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    774\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    775\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    776\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    777\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    778\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    779\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    780\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    781\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    782\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    783\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    784\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    785\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    787\u001b[0m )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:835\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m     queries[idx] \u001b[38;5;241m=\u001b[39m query\n\u001b[0;32m    833\u001b[0m     queries_tokens\u001b[38;5;241m.\u001b[39mappend(query_tokens)\n\u001b[1;32m--> 835\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_prepare_for_model(\n\u001b[0;32m    836\u001b[0m     table,\n\u001b[0;32m    837\u001b[0m     queries,\n\u001b[0;32m    838\u001b[0m     tokenized_table\u001b[38;5;241m=\u001b[39mtable_tokens,\n\u001b[0;32m    839\u001b[0m     queries_tokens\u001b[38;5;241m=\u001b[39mqueries_tokens,\n\u001b[0;32m    840\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    841\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    842\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    843\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    844\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    845\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    846\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    847\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    848\u001b[0m     prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    849\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    850\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    851\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    852\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    853\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    854\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    855\u001b[0m )\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:890\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_prepare_for_model\u001b[1;34m(self, raw_table, raw_queries, tokenized_table, queries_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(raw_queries, queries_tokens, answer_coordinates, answer_text)):\n\u001b[0;32m    889\u001b[0m     raw_query, query_tokens, answer_coords, answer_txt \u001b[38;5;241m=\u001b[39m example\n\u001b[1;32m--> 890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    891\u001b[0m         raw_table,\n\u001b[0;32m    892\u001b[0m         raw_query,\n\u001b[0;32m    893\u001b[0m         tokenized_table\u001b[38;5;241m=\u001b[39mtokenized_table,\n\u001b[0;32m    894\u001b[0m         query_tokens\u001b[38;5;241m=\u001b[39mquery_tokens,\n\u001b[0;32m    895\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coords,\n\u001b[0;32m    896\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_txt,\n\u001b[0;32m    897\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    898\u001b[0m         padding\u001b[38;5;241m=\u001b[39mPaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\u001b[38;5;241m.\u001b[39mvalue,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    899\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    900\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    901\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    902\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    903\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    904\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    905\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    906\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# We convert the whole batch to tensors at the end\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    908\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    909\u001b[0m         prev_answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    910\u001b[0m         prev_answer_text\u001b[38;5;241m=\u001b[39manswer_text[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    911\u001b[0m     )\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m batch_outputs:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1229\u001b[0m, in \u001b[0;36mTapasTokenizer.prepare_for_model\u001b[1;34m(self, raw_table, raw_query, tokenized_table, query_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     prev_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_answer_ids(\n\u001b[0;32m   1224\u001b[0m         column_ids, row_ids, table_data, prev_answer_text, prev_answer_coordinates\n\u001b[0;32m   1225\u001b[0m     )\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;66;03m# FIRST: parse both the table and question in terms of numeric values\u001b[39;00m\n\u001b[1;32m-> 1229\u001b[0m raw_table \u001b[38;5;241m=\u001b[39m add_numeric_table_values(raw_table)\n\u001b[0;32m   1230\u001b[0m raw_query \u001b[38;5;241m=\u001b[39m add_numeric_values_to_question(raw_query)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;66;03m# SECOND: add numeric-related features (and not parse them in these functions):\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:2826\u001b[0m, in \u001b[0;36madd_numeric_table_values\u001b[1;34m(table, min_consolidation_fraction, debug_info)\u001b[0m\n\u001b[0;32m   2824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_index, row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m   2825\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(row):\n\u001b[1;32m-> 2826\u001b[0m         table\u001b[38;5;241m.\u001b[39miloc[row_index, col_index] \u001b[38;5;241m=\u001b[39m Cell(text\u001b[38;5;241m=\u001b[39mcell)\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;66;03m# Third, add numeric_value attributes to these Cell objects\u001b[39;00m\n\u001b[0;32m   2829\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_index, column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:846\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    844\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m    845\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_setitem_indexer(key)\n\u001b[1;32m--> 846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m    849\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1550\u001b[0m, in \u001b[0;36m_iLocIndexer._has_valid_setitem_indexer\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer(i):\n\u001b[0;32m   1549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ax):\n\u001b[1;32m-> 1550\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: iloc cannot enlarge its target object"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import TapasTokenizer, TapasForQuestionAnswering, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "current_length = len(order_data)\n",
    "rows_to_drop = current_length % 8\n",
    "if rows_to_drop > 0:\n",
    "    order_data = order_data[:-rows_to_drop]\n",
    "\n",
    "# Reset the index and convert all values to strings\n",
    "order_data.reset_index(drop=True, inplace=True)\n",
    "order_data = order_data.astype(str)\n",
    "\n",
    "# Save the updated dataset to a new CSV file\n",
    "order_data.to_csv(\"orders_updated.csv\", index=False)\n",
    "\n",
    "# Generate questions and answers for each row\n",
    "questions = [\"What segment has the highest sales?\"] \n",
    "answer_coordinates = [[(0, 0)]]   # Adjust coordinates as needed\n",
    "answer_texts = ['Consumer']  # Adjust answers as needed\n",
    "#assert len(questions) == len(order_data), \"Questions list length does not match order_data length\"\n",
    "#assert len(answer_coordinates) == len(order_data), \"Answer coordinates list length does not match order_data length\"\n",
    "#assert len(answer_texts) == len(order_data), \"Answer texts list length does not match order_data length\"\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, questions, answer_coordinates, answer_texts):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.questions = questions\n",
    "        self.answer_coordinates = answer_coordinates\n",
    "        self.answer_texts = answer_texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.dataframe):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataframe with length {len(self.dataframe)}\")\n",
    "\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        table = pd.DataFrame([row])\n",
    "        print(f\"Processing row {idx}: {row}\")  # Debugging statement\n",
    "\n",
    "        try:\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                table=table,\n",
    "                queries=questions,\n",
    "                answer_coordinates=answer_coordinates,\n",
    "                answer_text=answer_texts,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization at index {idx}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        return inputs, idx\n",
    "\n",
    "# Create instances of the custom dataset\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\n",
    "train_dataset = CustomDataset(order_data, tokenizer, questions, answer_coordinates, answer_texts)\n",
    "eval_dataset = CustomDataset(order_data, tokenizer, questions, answer_coordinates, answer_texts)\n",
    "\n",
    "# Print dataset sizes to verify\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Load the model from the local directory\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"./Tapas\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the custom DataLoader\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-tapas\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-tapas\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./fine-tuned-tapas\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"./fine-tuned-tapas\")\n",
    "\n",
    "# Ask Questions\n",
    "inputs = tokenizer(table=order_data, queries=[\"What segment has the highest sales?\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5da35076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consumer'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b5b55d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segment                Consumer\n",
       "Category        Office Supplies\n",
       "Sub-Category           Supplies\n",
       "Name: 7400, dtype: object"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_data.iloc[7400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1b859f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 1: Segment          Consumer\n",
      "Category        Furniture\n",
      "Sub-Category       Chairs\n",
      "Name: 1, dtype: object\n",
      "Error during tokenization at index 1: iloc cannot enlarge its target object\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "iloc cannot enlarge its target object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[144], line 55\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during tokenization at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, idx\n",
      "Cell \u001b[1;32mIn[144], line 44\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging statement\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[0;32m     45\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m     46\u001b[0m         queries\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestions[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m     47\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_coordinates[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m     48\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_texts[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m     49\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during tokenization at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:650\u001b[0m, in \u001b[0;36mTapasTokenizer.__call__\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(queries, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m    651\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    652\u001b[0m         queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m    653\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    654\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    655\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    656\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    657\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    658\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    659\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    660\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    661\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    662\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    663\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    664\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    665\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    666\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    667\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    669\u001b[0m     )\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m    672\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    673\u001b[0m         query\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    690\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:768\u001b[0m, in \u001b[0;36mTapasTokenizer.batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    766\u001b[0m     )\n\u001b[1;32m--> 768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    769\u001b[0m     table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    770\u001b[0m     queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m    771\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    772\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    773\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    774\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    775\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    776\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    777\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    778\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    779\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    780\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    781\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    782\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    783\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    784\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    785\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    787\u001b[0m )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:835\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m     queries[idx] \u001b[38;5;241m=\u001b[39m query\n\u001b[0;32m    833\u001b[0m     queries_tokens\u001b[38;5;241m.\u001b[39mappend(query_tokens)\n\u001b[1;32m--> 835\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_prepare_for_model(\n\u001b[0;32m    836\u001b[0m     table,\n\u001b[0;32m    837\u001b[0m     queries,\n\u001b[0;32m    838\u001b[0m     tokenized_table\u001b[38;5;241m=\u001b[39mtable_tokens,\n\u001b[0;32m    839\u001b[0m     queries_tokens\u001b[38;5;241m=\u001b[39mqueries_tokens,\n\u001b[0;32m    840\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    841\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    842\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    843\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    844\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    845\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    846\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    847\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    848\u001b[0m     prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    849\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    850\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    851\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    852\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    853\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    854\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    855\u001b[0m )\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:890\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_prepare_for_model\u001b[1;34m(self, raw_table, raw_queries, tokenized_table, queries_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(raw_queries, queries_tokens, answer_coordinates, answer_text)):\n\u001b[0;32m    889\u001b[0m     raw_query, query_tokens, answer_coords, answer_txt \u001b[38;5;241m=\u001b[39m example\n\u001b[1;32m--> 890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    891\u001b[0m         raw_table,\n\u001b[0;32m    892\u001b[0m         raw_query,\n\u001b[0;32m    893\u001b[0m         tokenized_table\u001b[38;5;241m=\u001b[39mtokenized_table,\n\u001b[0;32m    894\u001b[0m         query_tokens\u001b[38;5;241m=\u001b[39mquery_tokens,\n\u001b[0;32m    895\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coords,\n\u001b[0;32m    896\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_txt,\n\u001b[0;32m    897\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    898\u001b[0m         padding\u001b[38;5;241m=\u001b[39mPaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\u001b[38;5;241m.\u001b[39mvalue,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    899\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    900\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    901\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    902\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    903\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    904\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    905\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    906\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# We convert the whole batch to tensors at the end\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    908\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    909\u001b[0m         prev_answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    910\u001b[0m         prev_answer_text\u001b[38;5;241m=\u001b[39manswer_text[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    911\u001b[0m     )\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m batch_outputs:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1229\u001b[0m, in \u001b[0;36mTapasTokenizer.prepare_for_model\u001b[1;34m(self, raw_table, raw_query, tokenized_table, query_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     prev_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_answer_ids(\n\u001b[0;32m   1224\u001b[0m         column_ids, row_ids, table_data, prev_answer_text, prev_answer_coordinates\n\u001b[0;32m   1225\u001b[0m     )\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;66;03m# FIRST: parse both the table and question in terms of numeric values\u001b[39;00m\n\u001b[1;32m-> 1229\u001b[0m raw_table \u001b[38;5;241m=\u001b[39m add_numeric_table_values(raw_table)\n\u001b[0;32m   1230\u001b[0m raw_query \u001b[38;5;241m=\u001b[39m add_numeric_values_to_question(raw_query)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;66;03m# SECOND: add numeric-related features (and not parse them in these functions):\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:2826\u001b[0m, in \u001b[0;36madd_numeric_table_values\u001b[1;34m(table, min_consolidation_fraction, debug_info)\u001b[0m\n\u001b[0;32m   2824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_index, row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m   2825\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(row):\n\u001b[1;32m-> 2826\u001b[0m         table\u001b[38;5;241m.\u001b[39miloc[row_index, col_index] \u001b[38;5;241m=\u001b[39m Cell(text\u001b[38;5;241m=\u001b[39mcell)\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;66;03m# Third, add numeric_value attributes to these Cell objects\u001b[39;00m\n\u001b[0;32m   2829\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_index, column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:846\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    844\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m    845\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_setitem_indexer(key)\n\u001b[1;32m--> 846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m    849\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1550\u001b[0m, in \u001b[0;36m_iLocIndexer._has_valid_setitem_indexer\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer(i):\n\u001b[0;32m   1549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ax):\n\u001b[1;32m-> 1550\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: iloc cannot enlarge its target object"
     ]
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a649dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Load the Fine-Tuned Model\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"./fine-tuned-tapas\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"./fine-tuned-tapas\")\n",
    "\n",
    "# Step 6: Ask Questions\n",
    "# Example question: Is the premium paid higher for males?\n",
    "inputs = tokenizer(table=table, queries=[\"what segament has highest sales?\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(inputs, logits)\n",
    "print(predicted_answer_coordinates)\n",
    "\n",
    "# Example question: What is the average premium paid by gender?\n",
    "inputs = tokenizer(table=table, queries=[\"What is the average sales by category?\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(inputs, logits)\n",
    "print(predicted_answer_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0585f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 7995\n",
      "Evaluation dataset size: 1999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b636c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
