{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12849034,"sourceType":"datasetVersion","datasetId":8126848}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-09-21T11:00:28.789938Z","iopub.execute_input":"2025-09-21T11:00:28.790454Z","iopub.status.idle":"2025-09-21T11:00:28.799875Z","shell.execute_reply.started":"2025-09-21T11:00:28.790429Z","shell.execute_reply":"2025-09-21T11:00:28.799278Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n/kaggle/input/open-minilm-l6-v2/rust_model.ot\n/kaggle/input/open-minilm-l6-v2/config.json\n/kaggle/input/open-minilm-l6-v2/tokenizer.json\n/kaggle/input/open-minilm-l6-v2/tf_model.h5\n/kaggle/input/open-minilm-l6-v2/data_config.json\n/kaggle/input/open-minilm-l6-v2/train_script.py\n/kaggle/input/open-minilm-l6-v2/tokenizer_config.json\n/kaggle/input/open-minilm-l6-v2/sentence_bert_config.json\n/kaggle/input/open-minilm-l6-v2/gitattributes\n/kaggle/input/open-minilm-l6-v2/pytorch_model.bin\n/kaggle/input/open-minilm-l6-v2/config_sentence_transformers.json\n/kaggle/input/open-minilm-l6-v2/model.safetensors\n/kaggle/input/open-minilm-l6-v2/modules.json\n/kaggle/input/open-minilm-l6-v2/special_tokens_map.json\n/kaggle/input/open-minilm-l6-v2/vocab.txt\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"\n\n#!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\n\n#model = SentenceTransformer(\"all-mpnet-base-v2\")\n","metadata":{"execution":{"iopub.status.busy":"2025-09-21T11:00:28.801240Z","iopub.execute_input":"2025-09-21T11:00:28.801429Z","iopub.status.idle":"2025-09-21T11:00:28.810459Z","shell.execute_reply.started":"2025-09-21T11:00:28.801414Z","shell.execute_reply":"2025-09-21T11:00:28.809757Z"},"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"\n#model.save(\"/kaggle/working/all-mpnet-base-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:28.811197Z","iopub.execute_input":"2025-09-21T11:00:28.811389Z","iopub.status.idle":"2025-09-21T11:00:28.823604Z","shell.execute_reply.started":"2025-09-21T11:00:28.811374Z","shell.execute_reply":"2025-09-21T11:00:28.823116Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/qwen-llm\")\n#model = AutoModel.from_pretrained(\"/kaggle/input/qwen-llm\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:28.825027Z","iopub.execute_input":"2025-09-21T11:00:28.825439Z","iopub.status.idle":"2025-09-21T11:00:28.837215Z","shell.execute_reply.started":"2025-09-21T11:00:28.825422Z","shell.execute_reply":"2025-09-21T11:00:28.836502Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"submission_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\nsubmission_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:28.837961Z","iopub.execute_input":"2025-09-21T11:00:28.838203Z","iopub.status.idle":"2025-09-21T11:00:28.857313Z","shell.execute_reply.started":"2025-09-21T11:00:28.838181Z","shell.execute_reply":"2025-09-21T11:00:28.856631Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.333333        0.333333    0.333333\n1   211333        0.333333        0.333333    0.333333\n2  1233961        0.333333        0.333333    0.333333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ndf_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:28.858721Z","iopub.execute_input":"2025-09-21T11:00:28.858930Z","iopub.status.idle":"2025-09-21T11:00:30.561104Z","shell.execute_reply.started":"2025-09-21T11:00:28.858915Z","shell.execute_reply":"2025-09-21T11:00:30.560305Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.562061Z","iopub.execute_input":"2025-09-21T11:00:30.562335Z","iopub.status.idle":"2025-09-21T11:00:30.571545Z","shell.execute_reply.started":"2025-09-21T11:00:30.562316Z","shell.execute_reply":"2025-09-21T11:00:30.570879Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>[\"I have three oranges today, I ate an orange ...</td>\n      <td>[\"You have two oranges today.\"]</td>\n      <td>[\"You still have three oranges. Eating an oran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>[\"You are a mediator in a heated political deb...</td>\n      <td>[\"Thank you for sharing the details of the sit...</td>\n      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>[\"How to initialize the classification head wh...</td>\n      <td>[\"When you want to initialize the classificati...</td>\n      <td>[\"To initialize the classification head when p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":52},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.572366Z","iopub.execute_input":"2025-09-21T11:00:30.572641Z","iopub.status.idle":"2025-09-21T11:00:30.606550Z","shell.execute_reply.started":"2025-09-21T11:00:30.572624Z","shell.execute_reply":"2025-09-21T11:00:30.605901Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              57477 non-null  int64 \n 1   model_a         57477 non-null  object\n 2   model_b         57477 non-null  object\n 3   prompt          57477 non-null  object\n 4   response_a      57477 non-null  object\n 5   response_b      57477 non-null  object\n 6   winner_model_a  57477 non-null  int64 \n 7   winner_model_b  57477 non-null  int64 \n 8   winner_tie      57477 non-null  int64 \ndtypes: int64(4), object(5)\nmemory usage: 3.9+ MB\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"#df_train=df_train.head(30000)\n\n#)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.607347Z","iopub.execute_input":"2025-09-21T11:00:30.607610Z","iopub.status.idle":"2025-09-21T11:00:30.611168Z","shell.execute_reply.started":"2025-09-21T11:00:30.607583Z","shell.execute_reply":"2025-09-21T11:00:30.610375Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.613385Z","iopub.execute_input":"2025-09-21T11:00:30.613614Z","iopub.status.idle":"2025-09-21T11:00:30.631992Z","shell.execute_reply.started":"2025-09-21T11:00:30.613591Z","shell.execute_reply":"2025-09-21T11:00:30.631330Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.632691Z","iopub.execute_input":"2025-09-21T11:00:30.632996Z","iopub.status.idle":"2025-09-21T11:00:30.667909Z","shell.execute_reply.started":"2025-09-21T11:00:30.632971Z","shell.execute_reply":"2025-09-21T11:00:30.667338Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"id                0\nmodel_a           0\nmodel_b           0\nprompt            0\nresponse_a        0\nresponse_b        0\nwinner_model_a    0\nwinner_model_b    0\nwinner_tie        0\ndtype: int64"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"X = df_train.drop(['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie'], axis = 1)\nX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.668504Z","iopub.execute_input":"2025-09-21T11:00:30.668676Z","iopub.status.idle":"2025-09-21T11:00:30.690039Z","shell.execute_reply.started":"2025-09-21T11:00:30.668664Z","shell.execute_reply":"2025-09-21T11:00:30.689400Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n0           30192  [\"Is it morally right to try to have a certain...   \n1           53567  [\"What is the difference between marriage lice...   \n2           65089  [\"explain function calling. how would you call...   \n3           96401  [\"How can I create a test set for a very rare ...   \n4          198779  [\"What is the best way to travel from Tel-Aviv...   \n...           ...                                                ...   \n57472  4294656694  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  4294692063  [\"In python, implement a naive Bayes with gaus...   \n57474  4294710549  [\"is it unethical to work on building weapons?...   \n57475  4294899228  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  4294947231  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  \n0      [\"As an AI, I don't have personal beliefs or o...  \n1      [\"A marriage license and a marriage certificat...  \n2      [\"Function calling is the process of invoking ...  \n3      [\"When building a classifier for a very rare c...  \n4      [\"The best way to travel from Tel-Aviv to Jeru...  \n...                                                  ...  \n57472  [\"Here is how that mnemonic represents the dig...  \n57473  [\"Sure! Here's an implementation of a naive Ba...  \n57474  [\"It depends on the context. Weapons can be us...  \n57475  [\"As an AI language model, I do not promote or...  \n57476  [\"If three kids eat three apples in three days...  \n\n[57477 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"y = df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n\ny = np.argmax(y, axis=1)\ny","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.690656Z","iopub.execute_input":"2025-09-21T11:00:30.690879Z","iopub.status.idle":"2025-09-21T11:00:30.698479Z","shell.execute_reply.started":"2025-09-21T11:00:30.690858Z","shell.execute_reply":"2025-09-21T11:00:30.697944Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"array([0, 1, 2, ..., 0, 1, 0])"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.699152Z","iopub.execute_input":"2025-09-21T11:00:30.699369Z","iopub.status.idle":"2025-09-21T11:00:30.724237Z","shell.execute_reply.started":"2025-09-21T11:00:30.699353Z","shell.execute_reply":"2025-09-21T11:00:30.723529Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.724977Z","iopub.execute_input":"2025-09-21T11:00:30.725145Z","iopub.status.idle":"2025-09-21T11:00:30.733782Z","shell.execute_reply.started":"2025-09-21T11:00:30.725131Z","shell.execute_reply":"2025-09-21T11:00:30.733116Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n30205  2245506736  [\"can you give me some escape room themes with...   \n8763    656268525  [\"Why did digital electronics switch from 5 vo...   \n30501  2266396135  [\"as an acclaimed and visionary author punishe...   \n33619  2497468695  [\"\\n\\nWrite a micro editor plugin that shows t...   \n22573  1679992057  [\"1. 19 year old Emese who is thin (165cm 48kg...   \n...           ...                                                ...   \n54343  4061186081         [\"mega-sena brazilian lottery statistics\"]   \n38158  2840606458                     [\"Draw a dog using ASCII art\"]   \n860      62750940                          [\"How to wear a bikini?\"]   \n15795  1180878567  [\"Can you tell me what was going on with all t...   \n56422  4218036675                          [\"SHOW ME WHAT YOU GOT!\"]   \n\n                                              response_a  \\\n30205  [\"Certainly! Here are several escape room them...   \n8763   [\"There are a few key reasons why digital elec...   \n30501  [\"As an acclaimed and visionary author, Elieze...   \n33619  [\"Here is Python code for a basic micro editor...   \n22573  [\"Certainly, let's address each of your points...   \n...                                                  ...   \n54343  [\"I do not have access to real-time mega-sena ...   \n38158                        [\"\\/_\\/\\\\  \\n( o.o )\\n> ^\"]   \n860    [\"Wearing a bikini can be a fun and comfortabl...   \n15795  [\"The direwolves, which are also known as dire...   \n56422  [\"Of course! As an AI, I can assist with a var...   \n\n                                              response_b  \n30205  [\"Here are some fun and immersive escape room ...  \n8763   [\"The shift from 5 volt logic to 3 volt logic ...  \n30501  [\"Eliezer Yudkowsky is often referred to as th...  \n33619  [\"Micro is a modern and intuitive terminal-bas...  \n22573  [\"1. From the description you provided, it see...  \n...                                                  ...  \n54343  [\"Mega-Sena is a Brazilian lottery game that o...  \n38158  [\"Here is a dog drawn with ASCII art:\\n\\n     ...  \n860    [\"Here are the basic steps to wear a bikini:\\n...  \n15795  [\"Sure! On the night of the Red Wedding, all o...  \n56422  [\"I'm a large language model, so I don't have ...  \n\n[45981 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30205</th>\n      <td>2245506736</td>\n      <td>[\"can you give me some escape room themes with...</td>\n      <td>[\"Certainly! Here are several escape room them...</td>\n      <td>[\"Here are some fun and immersive escape room ...</td>\n    </tr>\n    <tr>\n      <th>8763</th>\n      <td>656268525</td>\n      <td>[\"Why did digital electronics switch from 5 vo...</td>\n      <td>[\"There are a few key reasons why digital elec...</td>\n      <td>[\"The shift from 5 volt logic to 3 volt logic ...</td>\n    </tr>\n    <tr>\n      <th>30501</th>\n      <td>2266396135</td>\n      <td>[\"as an acclaimed and visionary author punishe...</td>\n      <td>[\"As an acclaimed and visionary author, Elieze...</td>\n      <td>[\"Eliezer Yudkowsky is often referred to as th...</td>\n    </tr>\n    <tr>\n      <th>33619</th>\n      <td>2497468695</td>\n      <td>[\"\\n\\nWrite a micro editor plugin that shows t...</td>\n      <td>[\"Here is Python code for a basic micro editor...</td>\n      <td>[\"Micro is a modern and intuitive terminal-bas...</td>\n    </tr>\n    <tr>\n      <th>22573</th>\n      <td>1679992057</td>\n      <td>[\"1. 19 year old Emese who is thin (165cm 48kg...</td>\n      <td>[\"Certainly, let's address each of your points...</td>\n      <td>[\"1. From the description you provided, it see...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54343</th>\n      <td>4061186081</td>\n      <td>[\"mega-sena brazilian lottery statistics\"]</td>\n      <td>[\"I do not have access to real-time mega-sena ...</td>\n      <td>[\"Mega-Sena is a Brazilian lottery game that o...</td>\n    </tr>\n    <tr>\n      <th>38158</th>\n      <td>2840606458</td>\n      <td>[\"Draw a dog using ASCII art\"]</td>\n      <td>[\"\\/_\\/\\\\  \\n( o.o )\\n&gt; ^\"]</td>\n      <td>[\"Here is a dog drawn with ASCII art:\\n\\n     ...</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>62750940</td>\n      <td>[\"How to wear a bikini?\"]</td>\n      <td>[\"Wearing a bikini can be a fun and comfortabl...</td>\n      <td>[\"Here are the basic steps to wear a bikini:\\n...</td>\n    </tr>\n    <tr>\n      <th>15795</th>\n      <td>1180878567</td>\n      <td>[\"Can you tell me what was going on with all t...</td>\n      <td>[\"The direwolves, which are also known as dire...</td>\n      <td>[\"Sure! On the night of the Red Wedding, all o...</td>\n    </tr>\n    <tr>\n      <th>56422</th>\n      <td>4218036675</td>\n      <td>[\"SHOW ME WHAT YOU GOT!\"]</td>\n      <td>[\"Of course! As an AI, I can assist with a var...</td>\n      <td>[\"I'm a large language model, so I don't have ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>45981 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.734583Z","iopub.execute_input":"2025-09-21T11:00:30.734870Z","iopub.status.idle":"2025-09-21T11:00:30.749554Z","shell.execute_reply.started":"2025-09-21T11:00:30.734847Z","shell.execute_reply":"2025-09-21T11:00:30.749001Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n37379  2785062085                     [\"what does hello world mean\"]   \n644      48259531  [\"I ran a marathon in 3:12:00 weighting 84kg. ...   \n48496  3622731894  [\"Below is an instruction that describes a tas...   \n12603   933663766  [\"How do I run static analysis with gcc in QT ...   \n16697  1246768370  [\"What did the music producer \\u00d8fdream die...   \n...           ...                                                ...   \n3151    240261313        [\"How to save a web page into a jpg file?\"]   \n30983  2300512815  [\"provide a short story about a pirate that le...   \n31183  2314277410  [\"Name 3 features of Alain Badiou's ontology t...   \n50634  3781372084  [\"Please give me a list of concise bullet poin...   \n23146  1722610582                      [\"What is funnier than 420?\"]   \n\n                                              response_a  \\\n37379  [\"\\\"Hello, World!\\\" is a phrase used in comput...   \n644    [\"It's difficult to provide an exact answer to...   \n48496  [\"\\u8fd9\\u6b3e\\u7eaf\\u8272\\u4e94\\u5206\\u88e4\\u...   \n12603  [\"To run static analysis with gcc in QT Creato...   \n16697  [\"I am not able to provide information on the ...   \n...                                                  ...   \n3151   [\"Here are a few ways to save a web page as a ...   \n30983  [\"Here is a short story about a pirate who lea...   \n31183  [\"Alain Badiou and Plato have distinct philoso...   \n50634  [\"- Musician who plays guitar and enjoys jammi...   \n23146  [\"The question \\\"What is funnier than 420?\\\" i...   \n\n                                              response_b  \n37379  [\"\\\"Hello, World!\\\" is a common phrase used to...  \n644    [\"To accurately estimate how much faster you w...  \n48496  [\"\\u6839\\u636e\\u63d0\\u4f9b\\u7684\\u4ea7\\u54c1\\u...  \n12603  [\"To run static analysis with gcc in QT Creato...  \n16697  [\"I'm really sorry, but I don't have the abili...  \n...                                                  ...  \n3151   [\"Here are a few ways to save a web page as a ...  \n30983  [\"Once upon a time, in the golden age of pirac...  \n31183  [\"Here are 3 key differences between Alain Bad...  \n50634  [\"* Music: plays guitar, drums, and bass\\n* Sp...  \n23146  [\"Humor is subjective and can vary from person...  \n\n[11496 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37379</th>\n      <td>2785062085</td>\n      <td>[\"what does hello world mean\"]</td>\n      <td>[\"\\\"Hello, World!\\\" is a phrase used in comput...</td>\n      <td>[\"\\\"Hello, World!\\\" is a common phrase used to...</td>\n    </tr>\n    <tr>\n      <th>644</th>\n      <td>48259531</td>\n      <td>[\"I ran a marathon in 3:12:00 weighting 84kg. ...</td>\n      <td>[\"It's difficult to provide an exact answer to...</td>\n      <td>[\"To accurately estimate how much faster you w...</td>\n    </tr>\n    <tr>\n      <th>48496</th>\n      <td>3622731894</td>\n      <td>[\"Below is an instruction that describes a tas...</td>\n      <td>[\"\\u8fd9\\u6b3e\\u7eaf\\u8272\\u4e94\\u5206\\u88e4\\u...</td>\n      <td>[\"\\u6839\\u636e\\u63d0\\u4f9b\\u7684\\u4ea7\\u54c1\\u...</td>\n    </tr>\n    <tr>\n      <th>12603</th>\n      <td>933663766</td>\n      <td>[\"How do I run static analysis with gcc in QT ...</td>\n      <td>[\"To run static analysis with gcc in QT Creato...</td>\n      <td>[\"To run static analysis with gcc in QT Creato...</td>\n    </tr>\n    <tr>\n      <th>16697</th>\n      <td>1246768370</td>\n      <td>[\"What did the music producer \\u00d8fdream die...</td>\n      <td>[\"I am not able to provide information on the ...</td>\n      <td>[\"I'm really sorry, but I don't have the abili...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3151</th>\n      <td>240261313</td>\n      <td>[\"How to save a web page into a jpg file?\"]</td>\n      <td>[\"Here are a few ways to save a web page as a ...</td>\n      <td>[\"Here are a few ways to save a web page as a ...</td>\n    </tr>\n    <tr>\n      <th>30983</th>\n      <td>2300512815</td>\n      <td>[\"provide a short story about a pirate that le...</td>\n      <td>[\"Here is a short story about a pirate who lea...</td>\n      <td>[\"Once upon a time, in the golden age of pirac...</td>\n    </tr>\n    <tr>\n      <th>31183</th>\n      <td>2314277410</td>\n      <td>[\"Name 3 features of Alain Badiou's ontology t...</td>\n      <td>[\"Alain Badiou and Plato have distinct philoso...</td>\n      <td>[\"Here are 3 key differences between Alain Bad...</td>\n    </tr>\n    <tr>\n      <th>50634</th>\n      <td>3781372084</td>\n      <td>[\"Please give me a list of concise bullet poin...</td>\n      <td>[\"- Musician who plays guitar and enjoys jammi...</td>\n      <td>[\"* Music: plays guitar, drums, and bass\\n* Sp...</td>\n    </tr>\n    <tr>\n      <th>23146</th>\n      <td>1722610582</td>\n      <td>[\"What is funnier than 420?\"]</td>\n      <td>[\"The question \\\"What is funnier than 420?\\\" i...</td>\n      <td>[\"Humor is subjective and can vary from person...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11496 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.750281Z","iopub.execute_input":"2025-09-21T11:00:30.750504Z","iopub.status.idle":"2025-09-21T11:00:30.765169Z","shell.execute_reply.started":"2025-09-21T11:00:30.750484Z","shell.execute_reply":"2025-09-21T11:00:30.764566Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"array([2, 1, 1, ..., 1, 1, 0])"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"catagorical_feature = [col for col in X.columns if X[col].dtype == 'object']\ncatagorical_feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.765877Z","iopub.execute_input":"2025-09-21T11:00:30.766204Z","iopub.status.idle":"2025-09-21T11:00:30.777999Z","shell.execute_reply.started":"2025-09-21T11:00:30.766178Z","shell.execute_reply":"2025-09-21T11:00:30.777316Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"['prompt', 'response_a', 'response_b']"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.778750Z","iopub.execute_input":"2025-09-21T11:00:30.779017Z","iopub.status.idle":"2025-09-21T11:00:30.791386Z","shell.execute_reply.started":"2025-09-21T11:00:30.778996Z","shell.execute_reply":"2025-09-21T11:00:30.790871Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport os # Import os module\nimport multiprocessing\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path, batch_size=16, max_length=512, use_fast_tokenizer=True, enable_mixed_precision=True, embedding_type=\"mean\"):\n        self.embedding_type = embedding_type\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision # Add this line to store the parameter\n        self._cpu_count = multiprocessing.cpu_count()\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # optionally use mixed precision on GPUs (speeds up fp16 capable GPUs)\n        try:\n            if self.enable_mixed_precision: # Use self.enable_mixed_precision\n                from tensorflow.keras import mixed_precision\n                mixed_precision.set_global_policy(\"mixed_float16\")\n        except Exception:\n            pass\n\n        # Load tokenizer (prefer fast tokenizer if available)\n        try:\n            if self.use_fast_tokenizer:\n                from transformers import DebertaV2TokenizerFast as _TokFast\n                self.tokenizer = _TokFast(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            else:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n        except Exception:\n            # fallback to original import name/location\n            try:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            except Exception:\n                raise\n\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        config = self._detect_model_config()\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=config[\"num_layers\"],\n            num_heads=config[\"num_heads\"],\n            hidden_dim=config[\"hidden_dim\"],\n            intermediate_dim=config[\"intermediate_dim\"],\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n\n\n        self.model.load_weights(os.path.join(self.model_path, \"model.weights.h5\"),skip_mismatch=True) # Use os.path.join\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n    def _detect_model_config(self):\n            import os\n            import json\n        \n            config_path = os.path.join(self.model_path, \"config.json\")\n            if os.path.exists(config_path):\n                try:\n                    with open(config_path, \"r\") as f:\n                        config = json.load(f)\n                    return {\n                        \"num_layers\": config.get(\"num_layers\", 12),\n                        \"num_heads\": config.get(\"num_attention_heads\", 12),\n                        \"hidden_dim\": config.get(\"hidden_size\", 768),\n                        \"intermediate_dim\": config.get(\"intermediate_size\", 3072),\n                    }\n                except Exception as e:\n                    print(f\"⚠️ Failed to read config.json: {e}\")\n        \n            # Fallback: infer from folder name\n            path_lower = self.model_path.lower()\n            if \"small\" in path_lower:\n                return {\n                    \"num_layers\": 12,\n                    \"num_heads\": 6,\n                    \"hidden_dim\": 768,\n                    \"intermediate_dim\": 3072,\n                }\n            elif \"base\" in path_lower:\n                return {\n                    \"num_layers\": 12,\n                    \"num_heads\": 12,\n                    \"hidden_dim\": 768,\n                    \"intermediate_dim\": 3072,\n                }\n        \n            print(\"⚠️ Could not auto-detect model config. Using default base config.\")\n            return {\n                \"num_layers\": 12,\n                \"num_heads\": 12,\n                \"hidden_dim\": 768,\n                \"intermediate_dim\": 3072,\n            }\n\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n    \n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n    \n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n    \n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            try:\n                hidden_dim = int(self.model.output_shape[-1])\n                return {\n                    \"raw\": np.zeros((0, self.max_length, hidden_dim), dtype=np.float32),\n                    \"mean\": np.zeros((0, hidden_dim), dtype=np.float32),\n                    \"median\": np.zeros((0, hidden_dim), dtype=np.float32),\n                    #\"max\": np.zeros((0, hidden_dim), dtype=np.float32),\n                    #\"min\": np.zeros((0, hidden_dim), dtype=np.float32),\n                }\n            except Exception:\n                return {}\n    \n        # Store outputs\n        raw_outputs = []\n    \n        for i in range(0, n, self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",\n                )\n                token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n                attention_mask_tf = tf.cast(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                outputs_np = outputs.numpy()\n            except Exception:\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                token_ids_tf = tf.convert_to_tensor(tokens[\"input_ids\"], dtype=tf.int32)\n                attention_mask_tf = tf.convert_to_tensor(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                outputs_np = outputs.numpy()\n    \n            raw_outputs.append(outputs_np)\n    \n        # Combine all batches\n        full_output = np.vstack(raw_outputs)  # shape: [n_samples, seq_len, hidden_dim]\n    \n        \n        mean_embeddings = np.mean(full_output, axis=1)\n        median_embeddings = np.median(full_output, axis=1)\n        #max_embeddings = np.max(full_output, axis=1)\n        #min_embeddings = np.min(full_output, axis=1)\n    \n        # Return per-sample dicts\n        return [\n            {       \n                \"text\": texts[i],\n                \"raw\": full_output[i],\n                \"mean\": mean_embeddings[i],\n                \"median\": median_embeddings[i],\n                #\"max\": max_embeddings[i],\n                #\"min\": min_embeddings[i],\n            }\n            for i in range(len(texts))\n        ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.792239Z","iopub.execute_input":"2025-09-21T11:00:30.792444Z","iopub.status.idle":"2025-09-21T11:00:30.815156Z","shell.execute_reply.started":"2025-09-21T11:00:30.792428Z","shell.execute_reply":"2025-09-21T11:00:30.814338Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\nimport os\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\", batch_size=16, max_length=512, use_fast_tokenizer=False, enable_mixed_precision=True, embedding_type=\"mean\"):\n        self.embedding_type = embedding_type\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision\n\n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, use_fast=self.use_fast_tokenizer)\n\n        # Load model\n        self.model = AutoModel.from_pretrained(self.model_path)\n        self.model.eval()\n\n        # Use GPU if available\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            hidden_dim = self.model.config.hidden_size\n            return {\n                \"raw\": np.zeros((0, self.max_length, hidden_dim), dtype=np.float32),\n                \"mean\": np.zeros((0, hidden_dim), dtype=np.float32),\n                \"median\": np.zeros((0, hidden_dim), dtype=np.float32),\n            }\n\n        raw_outputs = []\n\n        with torch.no_grad():\n            for i in range(0, n, self.batch_size):\n                batch = texts[i:i + self.batch_size]\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"pt\"\n                ).to(self.device)\n\n                outputs = self.model(**tokens, output_hidden_states=True)\n                last_hidden = outputs.last_hidden_state  # shape: [batch_size, seq_len, hidden_dim]\n                raw_outputs.append(last_hidden.cpu().numpy())\n\n        full_output = np.vstack(raw_outputs)\n        mean_embeddings = np.mean(full_output, axis=1)\n        median_embeddings = np.median(full_output, axis=1)\n\n        return [\n            {\n                \"text\": texts[i],\n                \"raw\": full_output[i],\n                \"mean\": mean_embeddings[i],\n                \"median\": median_embeddings[i],\n            }\n            for i in range(len(texts))\n        ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.818491Z","iopub.execute_input":"2025-09-21T11:00:30.818958Z","iopub.status.idle":"2025-09-21T11:00:30.833601Z","shell.execute_reply.started":"2025-09-21T11:00:30.818941Z","shell.execute_reply":"2025-09-21T11:00:30.832916Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\", batch_size=16, max_length=512, use_fast_tokenizer=False, enable_mixed_precision=True, embedding_type=\"mean\"):\n        self.embedding_type = embedding_type\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, use_fast=self.use_fast_tokenizer)\n        self.model = AutoModel.from_pretrained(self.model_path)\n        self.model.eval()\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            hidden_dim = self.model.config.hidden_size\n            return {\n                \"raw\": np.zeros((0, self.max_length, hidden_dim), dtype=np.float32),\n                \"mean\": np.zeros((0, hidden_dim), dtype=np.float32),\n                \"median\": np.zeros((0, hidden_dim), dtype=np.float32),\n                \"attention_weighted\": np.zeros((0, hidden_dim), dtype=np.float32),\n            }\n\n        raw_outputs = []\n        attention_weighted_outputs = []\n\n        with torch.no_grad():\n            for i in range(0, n, self.batch_size):\n                batch = texts[i:i + self.batch_size]\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"pt\"\n                ).to(self.device)\n\n                outputs = self.model(**tokens, output_attentions=True)\n                last_hidden = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n                attentions = outputs.attentions  # list of [batch_size, num_heads, seq_len, seq_len]\n\n                raw_outputs.append(last_hidden.cpu().numpy())\n\n                # Use attention from the last layer and average over heads\n                attn_last_layer = attentions[-1]  # [batch_size, num_heads, seq_len, seq_len]\n                attn_avg = attn_last_layer.mean(dim=1)  # [batch_size, seq_len, seq_len]\n\n                # Compute attention-weighted embeddings\n                attn_weighted = torch.bmm(attn_avg, last_hidden)  # [batch_size, seq_len, hidden_dim]\n                attention_weighted_outputs.append(attn_weighted.cpu().numpy())\n\n        full_output = np.vstack(raw_outputs)\n        attention_weighted = np.vstack(attention_weighted_outputs)\n        mean_embeddings = np.mean(full_output, axis=1)\n        median_embeddings = np.median(full_output, axis=1)\n        attention_weighted_embeddings = np.mean(attention_weighted, axis=1)\n\n        return [\n            {\n                \"text\": texts[i],\n                \"raw\": full_output[i],                        # [seq_len, hidden_dim]\n                \"mean\": mean_embeddings[i],                   # [hidden_dim]\n                \"median\": median_embeddings[i],               # [hidden_dim]\n                \"attention_weighted\": attention_weighted_embeddings[i],  # [hidden_dim]\n            }\n            for i in range(len(texts))\n        ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.834306Z","iopub.execute_input":"2025-09-21T11:00:30.834568Z","iopub.status.idle":"2025-09-21T11:00:30.854015Z","shell.execute_reply.started":"2025-09-21T11:00:30.834547Z","shell.execute_reply":"2025-09-21T11:00:30.853360Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"%pip install nltk ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:30.854667Z","iopub.execute_input":"2025-09-21T11:00:30.854916Z","iopub.status.idle":"2025-09-21T11:00:34.063232Z","shell.execute_reply.started":"2025-09-21T11:00:30.854901Z","shell.execute_reply":"2025-09-21T11:00:34.062177Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text_for_common_words(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    tokens = text.split()\n    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    #return [word for word in tokens]\n\nclass CommonWordsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        common_meaningful_words_a = []\n        common_meaningful_words_b = []\n\n        for index, row in X.iterrows():\n            prompt_tokens = clean_text_for_common_words(row['prompt'])\n            response_a_tokens = clean_text_for_common_words(row['response_a'])\n            response_b_tokens = clean_text_for_common_words(row['response_b'])\n\n            common_meaningful_a = len(set(prompt_tokens) & set(response_a_tokens))\n            common_meaningful_b = len(set(prompt_tokens) & set(response_b_tokens))\n\n            common_meaningful_words_a.append(common_meaningful_a)\n            common_meaningful_words_b.append(common_meaningful_b)\n\n        return np.array([common_meaningful_words_a, common_meaningful_words_b]).T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:34.064764Z","iopub.execute_input":"2025-09-21T11:00:34.065070Z","iopub.status.idle":"2025-09-21T11:00:34.332645Z","shell.execute_reply.started":"2025-09-21T11:00:34.065043Z","shell.execute_reply":"2025-09-21T11:00:34.331865Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"X_train[\"prompt_clean\"] = X_train[\"prompt\"]#.apply(clean_text_for_common_words)\nX_train[\"response_a_clean\"] = X_train[\"response_a\"]#.apply(clean_text_for_common_words)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"]#.apply(clean_text_for_common_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:34.333511Z","iopub.execute_input":"2025-09-21T11:00:34.334145Z","iopub.status.idle":"2025-09-21T11:00:34.394259Z","shell.execute_reply.started":"2025-09-21T11:00:34.334125Z","shell.execute_reply":"2025-09-21T11:00:34.393469Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass PromptResponseSimilarity(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # X is a 2D array of shape (n_samples, 3 * embedding_dim)\n        n_samples, total_dim = X.shape\n        embedding_dim = total_dim // 3\n\n        prompt_embeds = X[:, :embedding_dim]\n        resp_a_embeds = X[:, embedding_dim:2*embedding_dim]\n        resp_b_embeds = X[:, 2*embedding_dim:]\n\n        sim_a = np.array([\n            cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0]\n            for p, a in zip(prompt_embeds, resp_a_embeds)\n        ])\n        sim_b = np.array([\n            cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0]\n            for p, b in zip(prompt_embeds, resp_b_embeds)\n        ])\n\n        return np.vstack([sim_a, sim_b]).T\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:34.395093Z","iopub.execute_input":"2025-09-21T11:00:34.395275Z","iopub.status.idle":"2025-09-21T11:00:34.401289Z","shell.execute_reply.started":"2025-09-21T11:00:34.395260Z","shell.execute_reply":"2025-09-21T11:00:34.400634Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"class RawEmbeddingSimilarity(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        prompt_embeds = X[\"prompt_embed\"].apply(lambda x: x[\"raw\"]).tolist()\n        resp_a_embeds = X[\"resp_a_embed\"].apply(lambda x: x[\"raw\"]).tolist()\n        resp_b_embeds = X[\"resp_b_embed\"].apply(lambda x: x[\"raw\"]).tolist()\n    \n        sim_a, sim_b = [], []\n        for p, a, b in zip(prompt_embeds, resp_a_embeds, resp_b_embeds):\n            sim_a.append(cosine_similarity(p.flatten().reshape(1, -1), a.flatten().reshape(1, -1))[0, 0])\n            sim_b.append(cosine_similarity(p.flatten().reshape(1, -1), b.flatten().reshape(1, -1))[0, 0])\n    \n        return np.vstack([sim_a, sim_b]).T\n\n        \nclass AggregatedEmbeddingSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, agg_type=\"mean\"):\n        self.agg_type = agg_type\n\n    def fit(self, X, y=None):\n        return self\n\n    \n    def transform(self, X):\n        prompt_embeds = X[\"prompt_embed\"].apply(lambda x: x[self.agg_type]).tolist()\n        resp_a_embeds = X[\"resp_a_embed\"].apply(lambda x: x[self.agg_type]).tolist()\n        resp_b_embeds = X[\"resp_b_embed\"].apply(lambda x: x[self.agg_type]).tolist()\n    \n        sim_a = [cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0] for p, a in zip(prompt_embeds, resp_a_embeds)]\n        sim_b = [cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0] for p, b in zip(prompt_embeds, resp_b_embeds)]\n    \n        return np.vstack([sim_a, sim_b]).T\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:34.402017Z","iopub.execute_input":"2025-09-21T11:00:34.402435Z","iopub.status.idle":"2025-09-21T11:00:34.418991Z","shell.execute_reply.started":"2025-09-21T11:00:34.402418Z","shell.execute_reply":"2025-09-21T11:00:34.418220Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pandas as pd\n\nclass TFIDFAttentionEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, tokenizer, max_length=512, embedding_dim=768):\n        self.tokenizer = tokenizer\n        self.vectorizer = TfidfVectorizer()\n        self.max_length = max_length\n        self.embedding_dim = embedding_dim\n\n    def fit(self, X, y=None):\n        texts = []\n\n        # Extract texts from various input formats\n        if isinstance(X, (list, np.ndarray)):\n            for row in X:\n                if isinstance(row, (list, tuple)):\n                    for item in row:\n                        if isinstance(item, dict):\n                            text = item.get(\"text\", \"\")\n                            if text.strip():\n                                texts.append(text)\n                elif isinstance(row, dict):\n                    text = row.get(\"text\", \"\")\n                    if text.strip():\n                        texts.append(text)\n        elif isinstance(X, pd.DataFrame):\n            for col in X.columns:\n                col_texts = X[col].apply(lambda x: x.get(\"text\", \"\") if isinstance(x, dict) else \"\").tolist()\n                texts.extend([t for t in col_texts if t.strip()])\n\n        if not texts:\n            raise ValueError(\"No valid texts found for TF-IDF fitting.\")\n\n        self.vectorizer.fit(texts)\n        return self\n\n    def transform(self, X):\n        weighted_embeddings_mean = []\n        weighted_embeddings_median = []\n    \n        for i, sample in enumerate(X):\n            text = sample.get(\"text\", \"\")\n            token_embeddings = sample.get(\"raw\", None)\n    \n            if token_embeddings is None or len(text.strip()) == 0:\n                print(f\"[TFIDF Warning] Row {i} has no token embeddings or empty text.\")\n                print(f\"Text: {text}\\n\")\n                zero_vec = np.zeros(self.embedding_dim)\n                weighted_embeddings_mean.append(zero_vec)\n                weighted_embeddings_median.append(zero_vec)\n                continue\n    \n            tokens = self.tokenizer.tokenize(text)\n            tfidf_vector = self.vectorizer.transform([text]).toarray()[0]\n    \n            token_weights = []\n            missing_tokens = []\n    \n            for token in tokens[:self.max_length]:\n                token_clean = token.lower().replace(\"▁\", \"\")\n                idx = self.vectorizer.vocabulary_.get(token_clean, None)\n                if idx is not None:\n                    token_weights.append(tfidf_vector[idx])\n                else:\n                    token_weights.append(0.0)\n                    missing_tokens.append(token)\n    \n            token_weights = np.array(token_weights)\n            token_embeddings = token_embeddings[:len(token_weights)]\n    \n            if token_weights.sum() == 0 or token_embeddings.shape[0] == 0:\n                print(f\"[TFIDF Warning] Row {i} has zero TF-IDF weights.\")\n                #print(f\"Text: {text}\")\n                #print(f\"Tokens: {tokens}\")\n                #print(f\"Missing from TF-IDF vocab: {missing_tokens}\\n\")\n                zero_vec = np.zeros(token_embeddings.shape[-1])\n                weighted_embeddings_mean.append(zero_vec)\n                weighted_embeddings_median.append(zero_vec)\n                continue\n    \n            # Weighted mean\n            token_weights /= token_weights.sum()\n            weighted_mean = np.average(token_embeddings, axis=0, weights=token_weights)\n            weighted_embeddings_mean.append(weighted_mean)\n    \n            # Weighted median\n            weighted_median = np.zeros(token_embeddings.shape[1])\n            for dim in range(token_embeddings.shape[1]):\n                dim_values = token_embeddings[:, dim]\n                sorted_indices = np.argsort(dim_values)\n                sorted_weights = token_weights[sorted_indices]\n                cumsum_weights = np.cumsum(sorted_weights)\n                median_idx = np.searchsorted(cumsum_weights, 0.5 * cumsum_weights[-1])\n                weighted_median[dim] = dim_values[sorted_indices[median_idx]]\n            weighted_embeddings_median.append(weighted_median)\n    \n        return np.hstack([\n            np.array(weighted_embeddings_mean),\n            np.array(weighted_embeddings_median)\n        ])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:34.419746Z","iopub.execute_input":"2025-09-21T11:00:34.419948Z","iopub.status.idle":"2025-09-21T11:00:34.436634Z","shell.execute_reply.started":"2025-09-21T11:00:34.419932Z","shell.execute_reply":"2025-09-21T11:00:34.435933Z"}},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":"    def transform(self, X):\n        weighted_embeddings_mean = []\n        weighted_embeddings_median = []\n    \n        for sample in X:\n            # If sample is a dict (expected format), extract fields\n            if isinstance(sample, dict):\n                text = sample.get(\"text\", \"\")\n                token_embeddings = sample.get(\"raw\", None)\n            else:\n                # Fallback: treat sample as plain text or unsupported format\n                text = str(sample)\n                token_embeddings = None\n    \n            # Handle empty or invalid cases\n            if token_embeddings is None or len(text.strip()) == 0:\n                print(f\"[TFIDF Warning] Row {i} has no token embeddings or empty text.\")\n                print(f\"Text: {text}\\n\")\n                zero_vec = np.zeros(self.embedding_dim)\n                weighted_embeddings_mean.append(zero_vec)\n                weighted_embeddings_median.append(zero_vec)\n                continue\n    \n            tokens = self.tokenizer.tokenize(text)\n            tfidf_vector = self.vectorizer.transform([text]).toarray()[0]\n    \n            token_weights = []\n            for token in tokens[:self.max_length]:\n                idx = self.vectorizer.vocabulary_.get(token.lower(), None)\n                token_weights.append(tfidf_vector[idx] if idx is not None else 0.0)\n    \n            token_weights = np.array(token_weights)\n            token_embeddings = token_embeddings[:len(token_weights)]\n    \n            if token_weights.sum() > 0:\n                token_weights = token_weights / token_weights.sum()\n    \n            if token_weights.sum() == 0 or token_embeddings.shape[0] == 0:\n                \n                print(f\"[TFIDF Warning] Row {i} has zero TF-IDF weights.\")\n                print(f\"Text: {text}\")\n                print(f\"Tokens: {tokens}\")\n                print(f\"Missing from TF-IDF vocab: {missing_tokens}\\n\")\n                zero_vec = np.zeros(token_embeddings.shape[-1])\n                weighted_embeddings_mean.append(zero_vec)\n                weighted_embeddings_median.append(zero_vec)\n            else:\n                # Calculate weighted mean\n                weighted_mean = np.average(token_embeddings, axis=0, weights=token_weights)\n                weighted_embeddings_mean.append(weighted_mean)\n                \n                # Calculate weighted median\n                # For each dimension, sort values and find the weighted median\n                weighted_median = np.zeros(token_embeddings.shape[1])\n                for dim in range(token_embeddings.shape[1]):\n                    dim_values = token_embeddings[:, dim]\n                    sorted_indices = np.argsort(dim_values)\n                    sorted_weights = token_weights[sorted_indices]\n                    cumsum_weights = np.cumsum(sorted_weights)\n                    median_idx = np.searchsorted(cumsum_weights, 0.5 * cumsum_weights[-1])\n                    weighted_median[dim] = dim_values[sorted_indices[median_idx]]\n                weighted_embeddings_median.append(weighted_median)\n    \n        # Stack and return both mean and median embeddings side by side\n        return np.hstack([\n            np.array(weighted_embeddings_mean),\n            np.array(weighted_embeddings_median)\n        ])\n","metadata":{}},{"cell_type":"code","source":"class DictWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, embedder):\n        self.embedder = embedder\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Returns a 2D array of shape (n_samples, 1), each cell is a dict\n        embeddings = self.embedder.transform(X)\n        return np.array(embeddings).reshape(-1, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:34.437371Z","iopub.execute_input":"2025-09-21T11:00:34.437693Z","iopub.status.idle":"2025-09-21T11:00:34.450863Z","shell.execute_reply.started":"2025-09-21T11:00:34.437667Z","shell.execute_reply":"2025-09-21T11:00:34.450170Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"class EmbeddingStatsExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, fields=(\"mean\", \"median\")):\n        self.fields = fields\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        features = []\n        for row in X:\n            row_features = []\n            for field in self.fields:\n                vec = row.get(field, np.zeros(768))  # fallback to zeros if missing\n                row_features.extend(vec)\n            features.append(row_features)\n        return np.array(features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:00:34.451719Z","iopub.execute_input":"2025-09-21T11:00:34.452033Z","iopub.status.idle":"2025-09-21T11:00:34.466370Z","shell.execute_reply.started":"2025-09-21T11:00:34.452011Z","shell.execute_reply":"2025-09-21T11:00:34.465559Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"## from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nimport gc\n# Convert numpy arrays to sparse matrices if needed\nfrom scipy.sparse import csr_matrix, hstack\n# Initialize the shared embedder\nshared_embedder = HFEmbedder(model_path=\"/kaggle/input/open-minilm-l6-v2\")\n\n# Step 1: Get embeddings for each text column\nembedding_stage = ColumnTransformer([\n    (\"prompt_embed\", DictWrapper(shared_embedder), \"prompt_clean\"),\n    (\"resp_a_embed\", DictWrapper(shared_embedder), \"response_a_clean\"),\n    (\"resp_b_embed\", DictWrapper(shared_embedder), \"response_b_clean\"),\n])\n\n# Fit and transform the embeddings\nprint(\"Step 1: Generating embeddings...\")\n# Configuration\nCHUNK_SIZE = 768  # Adjust based on your memory constraints\nOUTPUT_FILE = \"embedded_features.pkl\"\n\n\ndef process_in_chunks(data, chunk_size=CHUNK_SIZE):\n    num_samples = len(data)\n    results = []\n    \n    for start_idx in range(0, num_samples, chunk_size):\n        print(f\"\\nProcessing chunk {start_idx//chunk_size + 1}/{(num_samples + chunk_size - 1)//chunk_size}\")\n        \n        # Get current chunk\n        end_idx = min(start_idx + chunk_size, num_samples)\n        chunk = data.iloc[start_idx:end_idx]\n        \n        # Generate embeddings for chunk\n        print(\"Generating embeddings...\")\n        embedded_chunk = embedding_stage.fit_transform(chunk)\n        \n        # Convert to DataFrame format\n        chunk_df = pd.DataFrame([{\n            \"prompt_embed\": x[0],\n            \"resp_a_embed\": x[1],\n            \"resp_b_embed\": x[2],\n        } for x in embedded_chunk])\n\n        # Step 2: Extract statistics from embeddings\n        print(\"\\nStep 2: Extracting embedding statistics...\")\n        prompt_raw_mean = np.array([x[\"attention_weighted\"] for x in chunk_df[\"prompt_embed\"]])\n        resp_a_raw_mean = np.array([x[\"attention_weighted\"] for x in chunk_df[\"resp_a_embed\"]])\n        resp_b_raw_mean = np.array([x[\"attention_weighted\"] for x in chunk_df[\"resp_b_embed\"]])\n\n        # Process each type of embedding separately\n        prompt_stats = EmbeddingStatsExtractor().fit_transform(chunk_df[\"prompt_embed\"])\n        resp_a_stats = EmbeddingStatsExtractor().fit_transform(chunk_df[\"resp_a_embed\"])\n        resp_b_stats = EmbeddingStatsExtractor().fit_transform(chunk_df[\"resp_b_embed\"])\n        \n        print(f\"Prompt stats shape: {prompt_stats.shape}\")\n        print(f\"Response A stats shape: {resp_a_stats.shape}\")\n        print(f\"Response B stats shape: {resp_b_stats.shape}\")\n        #Step 3: Calculate similarities\n        print(\"\\nStep 3: Calculating similarities...\")\n        \n        # Raw embedding similarity\n        raw_sim = RawEmbeddingSimilarity().fit_transform(chunk_df)\n        print(f\"Raw similarity shape: {raw_sim.shape}\")\n        \n        # Mean and median similarities\n        mean_sim = AggregatedEmbeddingSimilarity(agg_type=\"mean\").fit_transform(chunk_df)\n        median_sim = AggregatedEmbeddingSimilarity(agg_type=\"median\").fit_transform(chunk_df)\n        \n        print(f\"Mean similarity shape: {mean_sim.shape}\")\n        print(f\"Median similarity shape: {median_sim.shape}\")\n        # Step 4: TF-IDF attention\n        print(\"\\nStep 4: Computing TF-IDF attention...\")\n        \n        # Create and fit TF-IDF embedders\n        tfidf_prompt = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\n        tfidf_resp_a = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\n        tfidf_resp_b = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\n        \n        # Transform each column\n        prompt_tfidf = tfidf_prompt.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in chunk_df['prompt_embed']])\n        resp_a_tfidf = tfidf_resp_a.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in chunk_df['resp_a_embed']])\n        resp_b_tfidf = tfidf_resp_b.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in chunk_df['resp_b_embed']])\n        \n        print(f\"Prompt TF-IDF shape: {prompt_tfidf.shape}\")\n        print(f\"Response A TF-IDF shape: {resp_a_tfidf.shape}\")\n        print(f\"Response B TF-IDF shape: {resp_b_tfidf.shape}\")\n        # Calculate TF-IDF weighted similarities (one-to-one)\n        print(\"\\nCalculating one-to-one TF-IDF weighted similarities...\")\n        half_dim=384\n        prompt_mean = prompt_tfidf[:, :half_dim]\n        prompt_median = prompt_tfidf[:, half_dim:]\n        resp_a_mean = resp_a_tfidf[:, :half_dim]\n        resp_a_median = resp_a_tfidf[:, half_dim:]\n        resp_b_mean = resp_b_tfidf[:, :half_dim]\n        resp_b_median = resp_b_tfidf[:, half_dim:]\n        \n        # Calculate similarities between corresponding pairs\n        mean_sim_a = np.array([\n            cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0]\n            for p, a in zip(prompt_mean, resp_a_mean)\n        ])\n        mean_sim_b = np.array([\n            cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0]\n            for p, b in zip(prompt_mean, resp_b_mean)\n        ])\n        \n        median_sim_a = np.array([\n            cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0]\n            for p, a in zip(prompt_median, resp_a_median)\n        ])\n        median_sim_b = np.array([\n            cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0]\n            for p, b in zip(prompt_median, resp_b_median)\n        ])\n        \n        # Create DataFrame with one-to-one similarities\n        similarities_df = pd.DataFrame({\n            'tfidf_mean_sim_a': mean_sim_a,\n            'tfidf_mean_sim_b': mean_sim_b,\n            'tfidf_median_sim_a': median_sim_a,\n            'tfidf_median_sim_b': median_sim_b\n        })\n        \n        print(\"\\nSimilarity shapes:\")\n        print(f\"Number of rows: {len(similarities_df)}\")\n        \n       \n        def ensure_sparse(x, dtype=np.float32):\n            if x is None:\n                return None\n            # Convert to numpy array first with consistent dtype\n            if not isinstance(x, (csr_matrix, np.ndarray)):\n                x = np.array(x, dtype=dtype)\n            elif isinstance(x, np.ndarray):\n                x = x.astype(dtype)\n            \n            # If already sparse, ensure correct dtype\n            if isinstance(x, csr_matrix):\n                return x.astype(dtype)\n            \n            # Convert to sparse\n            return csr_matrix(x)\n        \n        # Print shapes and dtypes before conversion\n        print(\"\\nFeature shapes and types before conversion:\")\n        features = [\n            (\"prompt_raw_mean\", prompt_raw_mean),\n            (\"resp_a_raw_mean\", resp_a_raw_mean),\n            (\"resp_b_raw_mean\", resp_b_raw_mean),\n            (\"raw_sim\", raw_sim),\n            (\"mean_sim\", mean_sim),\n            (\"median_sim\", median_sim),\n            (\"tfidf_mean_sim_a\", similarities_df['tfidf_mean_sim_a'].values.reshape(-1,1)),\n            (\"tfidf_mean_sim_b\", similarities_df['tfidf_mean_sim_b'].values.reshape(-1,1)),\n            (\"tfidf_median_sim_a\", similarities_df['tfidf_median_sim_a'].values.reshape(-1,1)),\n            (\"tfidf_median_sim_b\",similarities_df['tfidf_median_sim_b'].values.reshape(-1,1)),\n           # (\"common_words\", common_words_features)\n        ]\n        \n        for name, feat in features:\n            if feat is not None:\n                print(f\"{name}: shape={feat.shape}, dtype={feat.dtype}\")\n        \n        # Convert each feature set to sparse format with consistent dtype\n        features_to_stack = []\n        for name, feat in features:\n            if feat is not None:\n                sparse_feat = ensure_sparse(feat)\n                features_to_stack.append(sparse_feat)\n                print(f\"Converted {name}: shape={sparse_feat.shape}, dtype={sparse_feat.dtype}\")\n        \n        # Print shapes before stacking\n        print(\"\\nFeature shapes before stacking:\")\n        for i, feat in enumerate(features_to_stack):\n            print(f\"Feature {i} shape: {feat.shape}\")\n        \n        # Combine all the feature matrices\n        final_features = hstack(features_to_stack)\n        X_final = pd.DataFrame(final_features.toarray())\n        print(f\"\\nFinal combined features shape: {final_features.shape}\")\n         # Save chunk to disk\n        chunk_filename = f\"{OUTPUT_FILE}.chunk_{start_idx//chunk_size}.pkl\"\n        X_final.to_pickle(chunk_filename)\n        print(f\"Saved chunk to {chunk_filename}\")\n        \n\n        # Clear memory\n        del embedded_chunk, X_final\n        gc.collect()\n        \n        # Keep track of chunk files\n        results.append(chunk_filename)\n    \n    return results\n\n# Process training data\nprint(\"Processing training data...\")\nchunk_files = process_in_chunks(X_train)\n\n# Optionally, combine all chunks\ndef combine_chunks(chunk_files):\n    print(\"\\nCombining chunks...\")\n    combined_df = pd.concat([pd.read_pickle(f) for f in chunk_files])\n    combined_df.to_pickle(OUTPUT_FILE)\n    print(f\"Saved combined data to {OUTPUT_FILE}\")\n    return combined_df\n\n# Combine if needed\n# combined_data = combine_chunks(chunk_files)  # Uncomment if you need the combined data\n\nprint(\"\\nProcessing complete!\")\n# Load training data chunks\n\n# Load and combine all chunks into a single DataFrame\ndef load_embedded_df(chunk_files):\n    print(\"Loading and combining chunks...\")\n    chunks = []\n    \n    for chunk_file in chunk_files:\n        try:\n            # Load chunk\n            chunk_df = pd.read_pickle(chunk_file)\n            chunks.append(chunk_df)\n            \n            # Clear memory after appending\n            del chunk_df\n            gc.collect()\n            \n        except Exception as e:\n            print(f\"Error loading chunk {chunk_file}: {e}\")\n    \n    # Combine all chunks\n    print(\"Concatenating all chunks...\")\n    embedded_df = pd.concat(chunks, axis=0, ignore_index=True)\n    print(f\"Final DataFrame shape: {embedded_df.shape}\")\n    \n    # Clear temporary lists\n    del chunks\n    gc.collect()\n    \n    return embedded_df\n\n# Load training data chunks\nembedded_df = load_embedded_df([f\"embedded_features.pkl.chunk_{i}.pkl\" \n                              for i in range((len(X_train) + CHUNK_SIZE - 1) // CHUNK_SIZE)])\n\nprint(\"\\nEmbedded DataFrame columns:\", embedded_df.columns.tolist())\n#print(\"First row prompt embedding keys:\", embedded_df[\"prompt_embed\"].iloc[0].keys())\n","metadata":{"execution":{"iopub.status.busy":"2025-09-21T11:00:34.467265Z","iopub.execute_input":"2025-09-21T11:00:34.467487Z"},"trusted":true},"outputs":[{"name":"stderr","text":"BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"},{"name":"stdout","text":"Step 1: Generating embeddings...\nProcessing training data...\n\nProcessing chunk 1/60\nGenerating embeddings...\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 768)\nResponse A stats shape: (768, 768)\nResponse B stats shape: (768, 768)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 398 has zero TF-IDF weights.\n[TFIDF Warning] Row 25 has zero TF-IDF weights.\n[TFIDF Warning] Row 96 has zero TF-IDF weights.\n[TFIDF Warning] Row 179 has zero TF-IDF weights.\n[TFIDF Warning] Row 260 has zero TF-IDF weights.\n[TFIDF Warning] Row 352 has zero TF-IDF weights.\n[TFIDF Warning] Row 430 has zero TF-IDF weights.\n[TFIDF Warning] Row 510 has zero TF-IDF weights.\n[TFIDF Warning] Row 645 has zero TF-IDF weights.\n[TFIDF Warning] Row 25 has zero TF-IDF weights.\n[TFIDF Warning] Row 41 has zero TF-IDF weights.\n[TFIDF Warning] Row 96 has zero TF-IDF weights.\n[TFIDF Warning] Row 179 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 260 has zero TF-IDF weights.\n[TFIDF Warning] Row 323 has zero TF-IDF weights.\n[TFIDF Warning] Row 352 has zero TF-IDF weights.\n[TFIDF Warning] Row 393 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 768)\nResponse A TF-IDF shape: (768, 768)\nResponse B TF-IDF shape: (768, 768)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nprompt_raw_mean: shape=(768, 384), dtype=float32\nresp_a_raw_mean: shape=(768, 384), dtype=float32\nresp_b_raw_mean: shape=(768, 384), dtype=float32\nraw_sim: shape=(768, 2), dtype=float32\nmean_sim: shape=(768, 2), dtype=float32\nmedian_sim: shape=(768, 2), dtype=float32\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted prompt_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_a_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_b_raw_mean: shape=(768, 384), dtype=float32\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 384)\nFeature 1 shape: (768, 384)\nFeature 2 shape: (768, 384)\nFeature 3 shape: (768, 2)\nFeature 4 shape: (768, 2)\nFeature 5 shape: (768, 2)\nFeature 6 shape: (768, 1)\nFeature 7 shape: (768, 1)\nFeature 8 shape: (768, 1)\nFeature 9 shape: (768, 1)\n\nFinal combined features shape: (768, 1162)\nSaved chunk to embedded_features.pkl.chunk_0.pkl\n\nProcessing chunk 2/60\nGenerating embeddings...\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 768)\nResponse A stats shape: (768, 768)\nResponse B stats shape: (768, 768)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 72 has zero TF-IDF weights.\n[TFIDF Warning] Row 118 has zero TF-IDF weights.\n[TFIDF Warning] Row 211 has zero TF-IDF weights.\n[TFIDF Warning] Row 516 has zero TF-IDF weights.\n[TFIDF Warning] Row 706 has zero TF-IDF weights.\n[TFIDF Warning] Row 737 has zero TF-IDF weights.\n[TFIDF Warning] Row 750 has zero TF-IDF weights.\n[TFIDF Warning] Row 374 has zero TF-IDF weights.\n[TFIDF Warning] Row 573 has zero TF-IDF weights.\n[TFIDF Warning] Row 584 has zero TF-IDF weights.\n[TFIDF Warning] Row 65 has zero TF-IDF weights.\n[TFIDF Warning] Row 374 has zero TF-IDF weights.\n[TFIDF Warning] Row 573 has zero TF-IDF weights.\n[TFIDF Warning] Row 584 has zero TF-IDF weights.\n[TFIDF Warning] Row 670 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 768)\nResponse A TF-IDF shape: (768, 768)\nResponse B TF-IDF shape: (768, 768)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nprompt_raw_mean: shape=(768, 384), dtype=float32\nresp_a_raw_mean: shape=(768, 384), dtype=float32\nresp_b_raw_mean: shape=(768, 384), dtype=float32\nraw_sim: shape=(768, 2), dtype=float32\nmean_sim: shape=(768, 2), dtype=float32\nmedian_sim: shape=(768, 2), dtype=float32\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted prompt_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_a_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_b_raw_mean: shape=(768, 384), dtype=float32\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 384)\nFeature 1 shape: (768, 384)\nFeature 2 shape: (768, 384)\nFeature 3 shape: (768, 2)\nFeature 4 shape: (768, 2)\nFeature 5 shape: (768, 2)\nFeature 6 shape: (768, 1)\nFeature 7 shape: (768, 1)\nFeature 8 shape: (768, 1)\nFeature 9 shape: (768, 1)\n\nFinal combined features shape: (768, 1162)\nSaved chunk to embedded_features.pkl.chunk_1.pkl\n\nProcessing chunk 3/60\nGenerating embeddings...\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 768)\nResponse A stats shape: (768, 768)\nResponse B stats shape: (768, 768)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 91 has zero TF-IDF weights.\n[TFIDF Warning] Row 109 has zero TF-IDF weights.\n[TFIDF Warning] Row 701 has zero TF-IDF weights.\n[TFIDF Warning] Row 13 has zero TF-IDF weights.\n[TFIDF Warning] Row 53 has zero TF-IDF weights.\n[TFIDF Warning] Row 53 has zero TF-IDF weights.\n[TFIDF Warning] Row 96 has zero TF-IDF weights.\n[TFIDF Warning] Row 100 has zero TF-IDF weights.\n[TFIDF Warning] Row 217 has zero TF-IDF weights.\n[TFIDF Warning] Row 250 has zero TF-IDF weights.\n[TFIDF Warning] Row 527 has zero TF-IDF weights.\n[TFIDF Warning] Row 538 has zero TF-IDF weights.\n[TFIDF Warning] Row 753 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 768)\nResponse A TF-IDF shape: (768, 768)\nResponse B TF-IDF shape: (768, 768)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nprompt_raw_mean: shape=(768, 384), dtype=float32\nresp_a_raw_mean: shape=(768, 384), dtype=float32\nresp_b_raw_mean: shape=(768, 384), dtype=float32\nraw_sim: shape=(768, 2), dtype=float32\nmean_sim: shape=(768, 2), dtype=float32\nmedian_sim: shape=(768, 2), dtype=float32\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted prompt_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_a_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_b_raw_mean: shape=(768, 384), dtype=float32\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 384)\nFeature 1 shape: (768, 384)\nFeature 2 shape: (768, 384)\nFeature 3 shape: (768, 2)\nFeature 4 shape: (768, 2)\nFeature 5 shape: (768, 2)\nFeature 6 shape: (768, 1)\nFeature 7 shape: (768, 1)\nFeature 8 shape: (768, 1)\nFeature 9 shape: (768, 1)\n\nFinal combined features shape: (768, 1162)\nSaved chunk to embedded_features.pkl.chunk_2.pkl\n\nProcessing chunk 4/60\nGenerating embeddings...\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 768)\nResponse A stats shape: (768, 768)\nResponse B stats shape: (768, 768)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 191 has zero TF-IDF weights.\n[TFIDF Warning] Row 542 has zero TF-IDF weights.\n[TFIDF Warning] Row 698 has zero TF-IDF weights.\n[TFIDF Warning] Row 81 has zero TF-IDF weights.\n[TFIDF Warning] Row 126 has zero TF-IDF weights.\n[TFIDF Warning] Row 157 has zero TF-IDF weights.\n[TFIDF Warning] Row 253 has zero TF-IDF weights.\n[TFIDF Warning] Row 416 has zero TF-IDF weights.\n[TFIDF Warning] Row 539 has zero TF-IDF weights.\n[TFIDF Warning] Row 691 has zero TF-IDF weights.\n[TFIDF Warning] Row 696 has zero TF-IDF weights.\n[TFIDF Warning] Row 741 has zero TF-IDF weights.\n[TFIDF Warning] Row 127 has zero TF-IDF weights.\n[TFIDF Warning] Row 224 has zero TF-IDF weights.\n[TFIDF Warning] Row 248 has zero TF-IDF weights.\n[TFIDF Warning] Row 373 has zero TF-IDF weights.\n[TFIDF Warning] Row 451 has zero TF-IDF weights.\n[TFIDF Warning] Row 516 has zero TF-IDF weights.\n[TFIDF Warning] Row 539 has zero TF-IDF weights.\n[TFIDF Warning] Row 617 has zero TF-IDF weights.\n[TFIDF Warning] Row 621 has zero TF-IDF weights.\n[TFIDF Warning] Row 691 has zero TF-IDF weights.\n[TFIDF Warning] Row 696 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 768)\nResponse A TF-IDF shape: (768, 768)\nResponse B TF-IDF shape: (768, 768)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nprompt_raw_mean: shape=(768, 384), dtype=float32\nresp_a_raw_mean: shape=(768, 384), dtype=float32\nresp_b_raw_mean: shape=(768, 384), dtype=float32\nraw_sim: shape=(768, 2), dtype=float32\nmean_sim: shape=(768, 2), dtype=float32\nmedian_sim: shape=(768, 2), dtype=float32\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted prompt_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_a_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_b_raw_mean: shape=(768, 384), dtype=float32\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 384)\nFeature 1 shape: (768, 384)\nFeature 2 shape: (768, 384)\nFeature 3 shape: (768, 2)\nFeature 4 shape: (768, 2)\nFeature 5 shape: (768, 2)\nFeature 6 shape: (768, 1)\nFeature 7 shape: (768, 1)\nFeature 8 shape: (768, 1)\nFeature 9 shape: (768, 1)\n\nFinal combined features shape: (768, 1162)\nSaved chunk to embedded_features.pkl.chunk_3.pkl\n\nProcessing chunk 5/60\nGenerating embeddings...\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 768)\nResponse A stats shape: (768, 768)\nResponse B stats shape: (768, 768)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 14 has zero TF-IDF weights.\n[TFIDF Warning] Row 252 has zero TF-IDF weights.\n[TFIDF Warning] Row 277 has zero TF-IDF weights.\n[TFIDF Warning] Row 29 has zero TF-IDF weights.\n[TFIDF Warning] Row 259 has zero TF-IDF weights.\n[TFIDF Warning] Row 489 has zero TF-IDF weights.\n[TFIDF Warning] Row 556 has zero TF-IDF weights.\n[TFIDF Warning] Row 708 has zero TF-IDF weights.\n[TFIDF Warning] Row 29 has zero TF-IDF weights.\n[TFIDF Warning] Row 117 has zero TF-IDF weights.\n[TFIDF Warning] Row 224 has zero TF-IDF weights.\n[TFIDF Warning] Row 294 has zero TF-IDF weights.\n[TFIDF Warning] Row 489 has zero TF-IDF weights.\n[TFIDF Warning] Row 708 has zero TF-IDF weights.\n[TFIDF Warning] Row 743 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 768)\nResponse A TF-IDF shape: (768, 768)\nResponse B TF-IDF shape: (768, 768)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nprompt_raw_mean: shape=(768, 384), dtype=float32\nresp_a_raw_mean: shape=(768, 384), dtype=float32\nresp_b_raw_mean: shape=(768, 384), dtype=float32\nraw_sim: shape=(768, 2), dtype=float32\nmean_sim: shape=(768, 2), dtype=float32\nmedian_sim: shape=(768, 2), dtype=float32\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted prompt_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_a_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_b_raw_mean: shape=(768, 384), dtype=float32\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 384)\nFeature 1 shape: (768, 384)\nFeature 2 shape: (768, 384)\nFeature 3 shape: (768, 2)\nFeature 4 shape: (768, 2)\nFeature 5 shape: (768, 2)\nFeature 6 shape: (768, 1)\nFeature 7 shape: (768, 1)\nFeature 8 shape: (768, 1)\nFeature 9 shape: (768, 1)\n\nFinal combined features shape: (768, 1162)\nSaved chunk to embedded_features.pkl.chunk_4.pkl\n\nProcessing chunk 6/60\nGenerating embeddings...\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 768)\nResponse A stats shape: (768, 768)\nResponse B stats shape: (768, 768)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 222 has zero TF-IDF weights.\n[TFIDF Warning] Row 354 has zero TF-IDF weights.\n[TFIDF Warning] Row 378 has zero TF-IDF weights.\n[TFIDF Warning] Row 745 has zero TF-IDF weights.\n[TFIDF Warning] Row 164 has zero TF-IDF weights.\n[TFIDF Warning] Row 184 has zero TF-IDF weights.\n[TFIDF Warning] Row 203 has zero TF-IDF weights.\n[TFIDF Warning] Row 354 has zero TF-IDF weights.\n[TFIDF Warning] Row 688 has zero TF-IDF weights.\n[TFIDF Warning] Row 745 has zero TF-IDF weights.\n[TFIDF Warning] Row 751 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 768)\nResponse A TF-IDF shape: (768, 768)\nResponse B TF-IDF shape: (768, 768)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nprompt_raw_mean: shape=(768, 384), dtype=float32\nresp_a_raw_mean: shape=(768, 384), dtype=float32\nresp_b_raw_mean: shape=(768, 384), dtype=float32\nraw_sim: shape=(768, 2), dtype=float32\nmean_sim: shape=(768, 2), dtype=float32\nmedian_sim: shape=(768, 2), dtype=float32\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted prompt_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_a_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_b_raw_mean: shape=(768, 384), dtype=float32\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 384)\nFeature 1 shape: (768, 384)\nFeature 2 shape: (768, 384)\nFeature 3 shape: (768, 2)\nFeature 4 shape: (768, 2)\nFeature 5 shape: (768, 2)\nFeature 6 shape: (768, 1)\nFeature 7 shape: (768, 1)\nFeature 8 shape: (768, 1)\nFeature 9 shape: (768, 1)\n\nFinal combined features shape: (768, 1162)\nSaved chunk to embedded_features.pkl.chunk_5.pkl\n\nProcessing chunk 7/60\nGenerating embeddings...\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 768)\nResponse A stats shape: (768, 768)\nResponse B stats shape: (768, 768)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 28 has zero TF-IDF weights.\n[TFIDF Warning] Row 53 has zero TF-IDF weights.\n[TFIDF Warning] Row 61 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 278 has zero TF-IDF weights.\n[TFIDF Warning] Row 341 has zero TF-IDF weights.\n[TFIDF Warning] Row 424 has zero TF-IDF weights.\n[TFIDF Warning] Row 577 has zero TF-IDF weights.\n[TFIDF Warning] Row 621 has zero TF-IDF weights.\n[TFIDF Warning] Row 694 has zero TF-IDF weights.\n[TFIDF Warning] Row 751 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 209 has zero TF-IDF weights.\n[TFIDF Warning] Row 341 has zero TF-IDF weights.\n[TFIDF Warning] Row 349 has zero TF-IDF weights.\n[TFIDF Warning] Row 473 has zero TF-IDF weights.\n[TFIDF Warning] Row 478 has zero TF-IDF weights.\n[TFIDF Warning] Row 621 has zero TF-IDF weights.\n[TFIDF Warning] Row 685 has zero TF-IDF weights.\n[TFIDF Warning] Row 751 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 768)\nResponse A TF-IDF shape: (768, 768)\nResponse B TF-IDF shape: (768, 768)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nprompt_raw_mean: shape=(768, 384), dtype=float32\nresp_a_raw_mean: shape=(768, 384), dtype=float32\nresp_b_raw_mean: shape=(768, 384), dtype=float32\nraw_sim: shape=(768, 2), dtype=float32\nmean_sim: shape=(768, 2), dtype=float32\nmedian_sim: shape=(768, 2), dtype=float32\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted prompt_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_a_raw_mean: shape=(768, 384), dtype=float32\nConverted resp_b_raw_mean: shape=(768, 384), dtype=float32\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 384)\nFeature 1 shape: (768, 384)\nFeature 2 shape: (768, 384)\nFeature 3 shape: (768, 2)\nFeature 4 shape: (768, 2)\nFeature 5 shape: (768, 2)\nFeature 6 shape: (768, 1)\nFeature 7 shape: (768, 1)\nFeature 8 shape: (768, 1)\nFeature 9 shape: (768, 1)\n\nFinal combined features shape: (768, 1162)\nSaved chunk to embedded_features.pkl.chunk_6.pkl\n\nProcessing chunk 8/60\nGenerating embeddings...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Step 5: Common words features\nprint(\"\\nStep 5: Computing common words features...\")\ncommon_words_transformer = CommonWordsTransformer()\ncommon_words_features = common_words_transformer.fit_transform(X_train)\nprint(f\"Common words features shape: {common_words_features.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"common_words_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedded_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#feature_selection = SelectKBest(score_func = chi2, k=6)\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfeature_selection = SelectKBest(score_func=f_classif, k=6)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedded_df.to_csv(\"X_final_step_by_step.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBClassifier(\n    objective=\"multi:softprob\",  \n    num_class=3,                  \n    eval_metric=\"mlogloss\",       \n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=10,\n    random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_final=pd.read_csv(\"X_final_step_by_step.csv\")\n\ny_final=y_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_final.drop(columns=[\"0\"],inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_final[['commword a','commword b']]=common_words_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_final[X_train.columns]=X_train.values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_final['y-train']=y_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_final","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nX_final_run = X_final.drop(columns=X_train.columns).drop(columns=[\"y-train\"])\nX_final_run.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nscores = cross_val_score(model, X_final_run, y_final, cv=3, scoring=\"accuracy\")\nprint(\"Cross-validated Accuracy:\", scores.mean())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"prompt_clean\"] = df_test[\"prompt\"].apply(clean_text_for_common_words)\ndf_test[\"response_a_clean\"] = df_test[\"response_a\"].apply(clean_text_for_common_words)\ndf_test[\"response_b_clean\"] = df_test[\"response_b\"].apply(clean_text_for_common_words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"/kaggle/working/X_final_step_by_step.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}