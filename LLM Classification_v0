{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":12874069,"sourceType":"datasetVersion","datasetId":8144015}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n#!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\n\n#model = SentenceTransformer(\"all-mpnet-base-v2\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#model.save(\"/kaggle/working/all-mpnet-base-v2\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/qwen-llm\")\n#model = AutoModel.from_pretrained(\"/kaggle/input/qwen-llm\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\nsubmission_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ndf_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_train.drop(['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie'], axis = 1)\nX","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n\ny = np.argmax(y, axis=1)\ny","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n    tokens = text.split()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    return \" \".join(tokens)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train[\"response_a_clean\"] = X_train[\"response_a\"].apply(clean_text)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"].apply(clean_text)\n\nX_train[\"prompt_clean\"] = X_train[\"prompt\"].apply(clean_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catagorical_feature = [col for col in X.columns if X[col].dtype == 'object']\ncatagorical_feature","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_name=\"all-mpnet-base-v2\"):\n        self.model_name = model_name  \n        self.model = SentenceTransformer(model_name)\n        if torch.cuda.is_available():\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self.model.encode(X.tolist(), show_progress_bar=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Example usage in ColumnTransformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", HFEmbedder(), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"execution":{"iopub.status.busy":"2025-08-23T12:09:38.196345Z","iopub.execute_input":"2025-08-23T12:09:38.197094Z","iopub.status.idle":"2025-08-23T12:09:43.999261Z","shell.execute_reply.started":"2025-08-23T12:09:38.197069Z","shell.execute_reply":"2025-08-23T12:09:43.998604Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\",use_auth_token=False , local_files_only=True,batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.model = SentenceTransformer(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            embeddings = self.model.encode(\n                batch,\n                show_progress_bar=False,\n                device=\"cuda\" if self.use_cuda else \"cpu\"\n            )\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-24T00:06:01.592096Z","iopub.execute_input":"2025-08-24T00:06:01.592776Z","iopub.status.idle":"2025-08-24T00:06:01.599761Z","shell.execute_reply.started":"2025-08-24T00:06:01.592751Z","shell.execute_reply":"2025-08-24T00:06:01.599010Z"}}},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport keras_nlp\nimport keras\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nimport keras_nlp\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, weights_path=\"/kaggle/input/dbertav3-small/model.weights.h5\", batch_size=8):\n        self.batch_size = batch_size\n        self.backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\"deberta_v3_extra_small_en\")\n        self.backbone.load_weights(weights_path)\n        self.preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\"deberta_v3_extra_small_en\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            tokenized = self.preprocessor(batch)\n            outputs = self.backbone(tokenized)\n            embeddings = np.mean(outputs[\"sequence_output\"].numpy(), axis=1)\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-25T05:30:15.354606Z","iopub.execute_input":"2025-08-25T05:30:15.355146Z","iopub.status.idle":"2025-08-25T05:30:15.362009Z","shell.execute_reply.started":"2025-08-25T05:30:15.355117Z","shell.execute_reply":"2025-08-25T05:30:15.361204Z"}}},{"cell_type":"markdown","source":"import shutil\n\nshutil.copy(\"/kaggle/input/dbertav3-small/vocabulary.spm\", \"/kaggle/working/spm.model\")\n","metadata":{"execution":{"iopub.status.busy":"2025-08-25T08:03:22.074697Z","iopub.execute_input":"2025-08-25T08:03:22.074961Z","iopub.status.idle":"2025-08-25T08:03:22.323216Z","shell.execute_reply.started":"2025-08-25T08:03:22.074943Z","shell.execute_reply":"2025-08-25T08:03:22.321784Z"}}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/multi-qa-mpnet-base-cos-v1\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.model = AutoModel.from_pretrained(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        # Ensure input is a list of strings\n        if isinstance(X, (np.ndarray, list)):\n            texts = list(X)\n        elif hasattr(X, \"tolist\"):\n            texts = X.tolist()\n        else:\n            raise ValueError(\"Unsupported input type for HFEmbedder\")\n    \n        all_embeddings = []\n    \n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            if self.use_cuda:\n                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n            all_embeddings.append(embeddings)\n    \n        return np.vstack(all_embeddings)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nclass PromptResponseSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/multi-qa-mpnet-base-cos-v1\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.model = AutoModel.from_pretrained(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        prompts = X[\"prompt_clean\"].tolist()\n        responses_a = X[\"response_a_clean\"].tolist()\n        responses_b = X[\"response_b_clean\"].tolist()\n\n        def embed(texts):\n            embeddings = []\n            for i in range(0, len(texts), self.batch_size):\n                batch = texts[i:i + self.batch_size]\n                inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n                if self.use_cuda:\n                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n                with torch.no_grad():\n                    outputs = self.model(**inputs)\n                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n                embeddings.append(batch_embeddings)\n            return np.vstack(embeddings)\n\n        prompt_embeds = embed(prompts)\n        a_embeds = embed(responses_a)\n        b_embeds = embed(responses_b)\n\n        sim_a = cosine_similarity(prompt_embeds, a_embeds).diagonal()\n        sim_b = cosine_similarity(prompt_embeds, b_embeds).diagonal()\n\n        sim_diff_squared = (sim_a - sim_b) ** 2\n        return sim_diff_squared.reshape(-1, 1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nembedder = HFEmbedder(model_path=\"Qwen/Qwen3-Embedding-0.6B\")\nX_train_embed = X_train.copy()\nX_train_embed[\"prompt_embed\"] = embedder.transform(X_train[\"prompt\"])\nX_train_embed[\"resp_a_embed\"] = embedder.transform(X_train[\"response_a\"])\nX_train_embed[\"resp_b_embed\"] = embedder.transform(X_train[\"response_b\"])\n\n# Drop original text columns if not needed\nX_train_embed = X_train_embed.drop(columns=[\"prompt\", \"response_a\", \"response_b\"])\n","metadata":{"execution":{"iopub.status.busy":"2025-08-25T08:16:37.995144Z","iopub.execute_input":"2025-08-25T08:16:37.995829Z","iopub.status.idle":"2025-08-25T08:16:41.373893Z","shell.execute_reply.started":"2025-08-25T08:16:37.995780Z","shell.execute_reply":"2025-08-25T08:16:41.372612Z"}}},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/dbertav3-small\", use_fast=False,batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        #self.tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/dbertav3-small\", use_fast=False)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.model = AutoModel.from_pretrained(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            if self.use_cuda:\n                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            # Mean pooling over token embeddings\n            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2025-08-25T07:15:51.743991Z","iopub.execute_input":"2025-08-25T07:15:51.744545Z","iopub.status.idle":"2025-08-25T07:15:51.752028Z","shell.execute_reply.started":"2025-08-25T07:15:51.744521Z","shell.execute_reply":"2025-08-25T07:15:51.751016Z"}}},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass PromptResponseSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.model = AutoModel.from_pretrained(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        prompts = X[\"prompt\"].tolist()\n        responses_a = X[\"response_a\"].tolist()\n        responses_b = X[\"response_b\"].tolist()\n\n\n        def embed(texts):\n            embeddings = []\n            for i in range(0, len(texts), self.batch_size):\n                batch = texts[i:i + self.batch_size]\n                inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n                if self.use_cuda:\n                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n                with torch.no_grad():\n                    outputs = self.model(**inputs)\n                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n                embeddings.append(batch_embeddings)\n            return np.vstack(embeddings)\n\n        prompt_embeds = embed(prompts)\n        a_embeds = embed(responses_a)\n        b_embeds = embed(responses_b)\n\n        sim_a = cosine_similarity(prompt_embeds, a_embeds).diagonal()\n        sim_b = cosine_similarity(prompt_embeds, b_embeds).diagonal()\n\n        return np.vstack([sim_a, sim_b]).T\n","metadata":{"execution":{"iopub.status.busy":"2025-08-24T09:21:23.195073Z","iopub.execute_input":"2025-08-24T09:21:23.195789Z","iopub.status.idle":"2025-08-24T09:21:23.204374Z","shell.execute_reply.started":"2025-08-24T09:21:23.195762Z","shell.execute_reply":"2025-08-24T09:21:23.203667Z"}}},{"cell_type":"markdown","source":"print(os.listdir(\"/kaggle/input/open-minilm-l6-v2\"))\n","metadata":{"execution":{"iopub.status.busy":"2025-08-24T09:14:48.720495Z","iopub.execute_input":"2025-08-24T09:14:48.720724Z","iopub.status.idle":"2025-08-24T09:14:48.738139Z","shell.execute_reply.started":"2025-08-24T09:14:48.720708Z","shell.execute_reply":"2025-08-24T09:14:48.737567Z"}}},{"cell_type":"markdown","source":"\nfrom sklearn.compose import ColumnTransformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", HFEmbedder(\"/kaggle/input/dbertav3-small\"), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(\"/kaggle/input/dbertav3-small\"), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(\"/kaggle/input/dbertav3-small\"), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"execution":{"iopub.status.busy":"2025-08-25T07:15:58.918190Z","iopub.execute_input":"2025-08-25T07:15:58.918974Z","iopub.status.idle":"2025-08-25T07:15:58.997825Z","shell.execute_reply.started":"2025-08-25T07:15:58.918947Z","shell.execute_reply":"2025-08-25T07:15:58.996061Z"}}},{"cell_type":"markdown","source":"\nfrom sklearn.compose import ColumnTransformer\n\n# Define the embedder with the desired model path\nembedder = HFEmbedder(model_path=\"sentence-transformers/all-MiniLM-L6-v2\")\npreprocessing = ColumnTransformer([\n    (\"sim_diff_squared\", PromptResponseSimilarity(\"/kaggle/input/open-minilm-l6-v2\"), [\"prompt_clean\", \"response_a_clean\", \"response_b_clean\"]),\n    (\"prompt_embed\", embedder, \"prompt_clean\"),\n    (\"resp_a_embed\", embedder, \"response_a_clean\"),\n    (\"resp_b_embed\", embedder, \"response_b_clean\"),\n])\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-25T10:37:42.633096Z","iopub.execute_input":"2025-08-25T10:37:42.633331Z","iopub.status.idle":"2025-08-25T10:37:51.159107Z","shell.execute_reply.started":"2025-08-25T10:37:42.633310Z","shell.execute_reply":"2025-08-25T10:37:51.158432Z"}}},{"cell_type":"markdown","source":"preprocessing = ColumnTransformer([\n    (\"similarities\", PromptResponseSimilarity(\"/kaggle/input/open-minilm-l6-v2\"), [\"prompt\", \"response_a\", \"response_b\"]),\n    (\"prompt_embed\", HFEmbedder(\"/kaggle/input/open-minilm-l6-v2\"), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(\"/kaggle/input/open-minilm-l6-v2\"), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(\"/kaggle/input/open-minilm-l6-v2\"), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"execution":{"iopub.status.busy":"2025-08-24T09:57:15.032176Z","iopub.execute_input":"2025-08-24T09:57:15.032819Z","iopub.status.idle":"2025-08-24T09:57:15.489496Z","shell.execute_reply.started":"2025-08-24T09:57:15.032794Z","shell.execute_reply":"2025-08-24T09:57:15.488736Z"}}},{"cell_type":"markdown","source":"preprocessing","metadata":{"execution":{"iopub.status.busy":"2025-08-25T10:37:51.159787Z","iopub.execute_input":"2025-08-25T10:37:51.160013Z","iopub.status.idle":"2025-08-25T10:37:51.176386Z","shell.execute_reply.started":"2025-08-25T10:37:51.159986Z","shell.execute_reply":"2025-08-25T10:37:51.175756Z"}}},{"cell_type":"code","source":"#feature_selection = SelectKBest(score_func = chi2, k=6)\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfeature_selection = SelectKBest(score_func=f_classif, k=6)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBClassifier(\n    objective=\"multi:softprob\",  \n    num_class=3,                  \n    eval_metric=\"mlogloss\",       \n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Use HFEmbedder with your chosen model\nembedder = HFEmbedder(model_path=\"/kaggle/input/multi-qa-mpnet-base-cos-v1\")\n\n# Apply embeddings to each column\nembedding_transformers = ColumnTransformer([\n    (\"prompt_embed\", embedder, \"prompt_clean\"),\n    (\"resp_a_embed\", embedder, \"response_a_clean\"),\n    (\"resp_b_embed\", embedder, \"response_b_clean\"),\n])\n\n# Final pipeline with model\nmy_pipeline = Pipeline([\n    (\"features\", embedding_transformers),\n    (\"model\", model)  # e.g., XGBClassifier or VotingClassifier\n])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = cross_val_score(my_pipeline, X_train, y_train, cv=2, scoring=\"accuracy\")\nprint(\"Accuracy:\", scores.mean())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_pipeline.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"response_a_clean\"] = df_test[\"response_a\"].apply(clean_text)\ndf_test[\"response_b_clean\"] = df_test[\"response_b\"].apply(clean_text)\n\ndf_test[\"prompt_clean\"] = df_test[\"prompt\"].apply(clean_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs = my_pipeline.predict_proba(df_test)\n\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"winner_model_a\": probs[:,0],\n    \"winner_model_b\": probs[:,1],\n    \"winner_tie\": probs[:,2],\n})\nprint(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"","metadata":{}}]}