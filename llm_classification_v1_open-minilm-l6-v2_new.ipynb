{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"include_colab_link":true},"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceId":86518,"sourceType":"competition"},{"datasetId":8144015,"sourceId":12874069,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Dhaneshkp/NLP/blob/main/llm_classification_v1_open-minilm-l6-v2_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"id":"wBE9x8EVvZl6","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.064650Z","iopub.execute_input":"2025-08-30T06:47:22.065409Z","iopub.status.idle":"2025-08-30T06:47:22.077174Z","shell.execute_reply.started":"2025-08-30T06:47:22.065378Z","shell.execute_reply":"2025-08-30T06:47:22.076372Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n\n#!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\n\n#model = SentenceTransformer(\"all-mpnet-base-v2\")\n","metadata":{"id":"rwiCDwE6vZl8","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.078793Z","iopub.execute_input":"2025-08-30T06:47:22.079019Z","iopub.status.idle":"2025-08-30T06:47:22.090050Z","shell.execute_reply.started":"2025-08-30T06:47:22.079001Z","shell.execute_reply":"2025-08-30T06:47:22.089193Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n#model.save(\"/kaggle/working/all-mpnet-base-v2\")\n","metadata":{"id":"ZKkJLQVcvZl9","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.090978Z","iopub.execute_input":"2025-08-30T06:47:22.091273Z","iopub.status.idle":"2025-08-30T06:47:22.105528Z","shell.execute_reply.started":"2025-08-30T06:47:22.091232Z","shell.execute_reply":"2025-08-30T06:47:22.104669Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/qwen-llm\")\n#model = AutoModel.from_pretrained(\"/kaggle/input/qwen-llm\")\n\n","metadata":{"id":"uYvkbBHJvZl-","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.107488Z","iopub.execute_input":"2025-08-30T06:47:22.107800Z","iopub.status.idle":"2025-08-30T06:47:22.123590Z","shell.execute_reply.started":"2025-08-30T06:47:22.107777Z","shell.execute_reply":"2025-08-30T06:47:22.122818Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_train = pd.read_csv(\"/content/train.csv\")\ndf_train","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":805},"id":"yifK71QYvZl-","outputId":"303ae10a-9570-4779-cd3e-9c3ed82305c2","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.124438Z","iopub.execute_input":"2025-08-30T06:47:22.124741Z","iopub.status.idle":"2025-08-30T06:47:22.171711Z","shell.execute_reply.started":"2025-08-30T06:47:22.124716Z","shell.execute_reply":"2025-08-30T06:47:22.170630Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3995601529.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/train.csv'","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"df_test = pd.read_csv(\"/content/test.csv\")\ndf_test","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"Hdqoqi-SvZl_","outputId":"eaf2e8c5-458d-4404-c566-c0b89a18bb75","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.172264Z","iopub.status.idle":"2025-08-30T06:47:22.172511Z","shell.execute_reply.started":"2025-08-30T06:47:22.172399Z","shell.execute_reply":"2025-08-30T06:47:22.172411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7yuwJ4zvZl_","outputId":"20a46c9c-58da-495f-f535-e728c9dedd73","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.173136Z","iopub.status.idle":"2025-08-30T06:47:22.173420Z","shell.execute_reply.started":"2025-08-30T06:47:22.173289Z","shell.execute_reply":"2025-08-30T06:47:22.173305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"id":"Mj5YG1M6vZmA","outputId":"3d357014-c0e8-4cd3-db6b-74350b5b6907","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.174247Z","iopub.status.idle":"2025-08-30T06:47:22.174554Z","shell.execute_reply.started":"2025-08-30T06:47:22.174403Z","shell.execute_reply":"2025-08-30T06:47:22.174416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_train.drop(['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie'], axis = 1)\nX","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":527},"id":"ugzdGm5TvZmA","outputId":"1715934d-eb74-4a3d-8670-8473e9462a59","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.175712Z","iopub.status.idle":"2025-08-30T06:47:22.176014Z","shell.execute_reply.started":"2025-08-30T06:47:22.175864Z","shell.execute_reply":"2025-08-30T06:47:22.175876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n\ny = np.argmax(y, axis=1)\ny","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2phggxcLvZmA","outputId":"163f8ebb-2d82-4123-ecd4-426ac7c10129","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.176921Z","iopub.status.idle":"2025-08-30T06:47:22.177211Z","shell.execute_reply.started":"2025-08-30T06:47:22.177063Z","shell.execute_reply":"2025-08-30T06:47:22.177075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42)","metadata":{"id":"zclkD6mevZmB","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.231636Z","iopub.execute_input":"2025-08-30T06:47:22.232224Z","iopub.status.idle":"2025-08-30T06:47:22.260941Z","shell.execute_reply.started":"2025-08-30T06:47:22.232193Z","shell.execute_reply":"2025-08-30T06:47:22.257932Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3243681195.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"],"ename":"NameError","evalue":"name 'X' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"X_train","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":562},"id":"eXdfG6LpvZmB","outputId":"3dc29d3e-5e11-4204-a11a-5b12770f0413","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.261651Z","iopub.status.idle":"2025-08-30T06:47:22.262002Z","shell.execute_reply.started":"2025-08-30T06:47:22.261840Z","shell.execute_reply":"2025-08-30T06:47:22.261855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n    tokens = text.split()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    return \" \".join(tokens)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tmo2CHamvZmB","outputId":"bc612167-df3a-4817-a4f7-e5744f5636df","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.263350Z","iopub.status.idle":"2025-08-30T06:47:22.263853Z","shell.execute_reply.started":"2025-08-30T06:47:22.263676Z","shell.execute_reply":"2025-08-30T06:47:22.263694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train[\"response_a_clean\"] = X_train[\"response_a\"].apply(clean_text)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"].apply(clean_text)\n\nX_train[\"prompt_clean\"] = X_train[\"prompt\"].apply(clean_text)\n","metadata":{"id":"sByVxKpnvZmB","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.264792Z","iopub.status.idle":"2025-08-30T06:47:22.265019Z","shell.execute_reply.started":"2025-08-30T06:47:22.264920Z","shell.execute_reply":"2025-08-30T06:47:22.264929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":562},"id":"IC6k_RmQvZmB","outputId":"481d34f1-2648-454a-eada-a540c4ede263","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.266618Z","iopub.status.idle":"2025-08-30T06:47:22.266974Z","shell.execute_reply.started":"2025-08-30T06:47:22.266855Z","shell.execute_reply":"2025-08-30T06:47:22.266869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbE9ugZ8vZmB","outputId":"fcb8c587-f96d-4769-df37-12af635f1fc5","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.268041Z","iopub.status.idle":"2025-08-30T06:47:22.268343Z","shell.execute_reply.started":"2025-08-30T06:47:22.268223Z","shell.execute_reply":"2025-08-30T06:47:22.268237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catagorical_feature = [col for col in X.columns if X[col].dtype == 'object']\ncatagorical_feature","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgSYSSnZvZmC","outputId":"5bdbecba-d249-41e8-8d77-0694bbfaeeb6","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.269435Z","iopub.status.idle":"2025-08-30T06:47:22.269745Z","shell.execute_reply.started":"2025-08-30T06:47:22.269553Z","shell.execute_reply":"2025-08-30T06:47:22.269567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch","metadata":{"id":"Hw-K-3wVvZmC","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.270980Z","iopub.status.idle":"2025-08-30T06:47:22.271284Z","shell.execute_reply.started":"2025-08-30T06:47:22.271124Z","shell.execute_reply":"2025-08-30T06:47:22.271140Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_name=\"all-mpnet-base-v2\"):\n        self.model_name = model_name  \n        self.model = SentenceTransformer(model_name)\n        if torch.cuda.is_available():\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self.model.encode(X.tolist(), show_progress_bar=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Example usage in ColumnTransformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", HFEmbedder(), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"id":"h3ccceBovZmC"}},{"cell_type":"code","source":"","metadata":{"id":"3EOfzGVovZmD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\",use_auth_token=False , local_files_only=True,batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.model = SentenceTransformer(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            embeddings = self.model.encode(\n                batch,\n                show_progress_bar=False,\n                device=\"cuda\" if self.use_cuda else \"cpu\"\n            )\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n","metadata":{"id":"dfas4MWXvZmD"}},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport keras_nlp\nimport keras\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nimport keras_nlp\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, weights_path=\"/kaggle/input/dbertav3-small/model.weights.h5\", batch_size=8):\n        self.batch_size = batch_size\n        self.backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\"deberta_v3_extra_small_en\")\n        self.backbone.load_weights(weights_path)\n        self.preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\"deberta_v3_extra_small_en\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            tokenized = self.preprocessor(batch)\n            outputs = self.backbone(tokenized)\n            embeddings = np.mean(outputs[\"sequence_output\"].numpy(), axis=1)\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n","metadata":{"id":"pgzYGuKdvZmD"}},{"cell_type":"markdown","source":"import shutil\n\nshutil.copy(\"/kaggle/input/dbertav3-small/vocabulary.spm\", \"/kaggle/working/spm.model\")\n","metadata":{"id":"StQcr9NQvZmD"}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.model = SentenceTransformer(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        # Ensure input is a list of strings\n        if isinstance(X, (np.ndarray, list)):\n            texts = list(X)\n        elif hasattr(X, \"tolist\"):\n            texts = X.tolist()\n        else:\n            raise ValueError(\"Unsupported input type for HFEmbedder\")\n\n        all_embeddings = self.model.encode(\n            texts,\n            show_progress_bar=False,\n            convert_to_numpy=True,\n            batch_size=self.batch_size,\n            device=\"cuda\" if self.use_cuda else \"cpu\"\n        )\n\n        return all_embeddings","metadata":{"id":"UkUWMyJfvZmE","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.272237Z","iopub.status.idle":"2025-08-30T06:47:22.272466Z","shell.execute_reply.started":"2025-08-30T06:47:22.272355Z","shell.execute_reply":"2025-08-30T06:47:22.272371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nclass PromptResponseSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n\n        self.model = SentenceTransformer(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        prompts = X[\"prompt\"].tolist()\n        responses_a = X[\"response_a\"].tolist()\n        responses_b = X[\"response_b\"].tolist()\n\n        def embed(texts):\n             embeddings = self.model.encode(\n                texts,\n                show_progress_bar=False,\n                convert_to_numpy=True,\n                batch_size=self.batch_size,\n                device=\"cuda\" if self.use_cuda else \"cpu\"\n            )\n             return embeddings\n\n\n        prompt_embeds = embed(prompts)\n        a_embeds = embed(responses_a)\n        b_embeds = embed(responses_b)\n\n        sim_a = cosine_similarity(prompt_embeds, a_embeds).diagonal()\n        sim_b = cosine_similarity(prompt_embeds, b_embeds).diagonal()\n\n        return np.vstack([sim_a, sim_b]).T","metadata":{"id":"F03Ds9k7vZmE","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.273136Z","iopub.status.idle":"2025-08-30T06:47:22.273464Z","shell.execute_reply.started":"2025-08-30T06:47:22.273284Z","shell.execute_reply":"2025-08-30T06:47:22.273298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"YchtDsTT0Gx2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nembedder = HFEmbedder(model_path=\"Qwen/Qwen3-Embedding-0.6B\")\nX_train_embed = X_train.copy()\nX_train_embed[\"prompt_embed\"] = embedder.transform(X_train[\"prompt\"])\nX_train_embed[\"resp_a_embed\"] = embedder.transform(X_train[\"response_a\"])\nX_train_embed[\"resp_b_embed\"] = embedder.transform(X_train[\"response_b\"])\n\n# Drop original text columns if not needed\nX_train_embed = X_train_embed.drop(columns=[\"prompt\", \"response_a\", \"response_b\"])\n","metadata":{"id":"9HgDHkDVvZmE"}},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/dbertav3-small\", use_fast=False,batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        #self.tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/dbertav3-small\", use_fast=False)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.model = AutoModel.from_pretrained(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            if self.use_cuda:\n                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            # Mean pooling over token embeddings\n            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n","metadata":{"id":"MMuWZdW6vZmE"}},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass PromptResponseSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.model = AutoModel.from_pretrained(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        prompts = X[\"prompt\"].tolist()\n        responses_a = X[\"response_a\"].tolist()\n        responses_b = X[\"response_b\"].tolist()\n\n\n        def embed(texts):\n            embeddings = []\n            for i in range(0, len(texts), self.batch_size):\n                batch = texts[i:i + self.batch_size]\n                inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n                if self.use_cuda:\n                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n                with torch.no_grad():\n                    outputs = self.model(**inputs)\n                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n                embeddings.append(batch_embeddings)\n            return np.vstack(embeddings)\n\n        prompt_embeds = embed(prompts)\n        a_embeds = embed(responses_a)\n        b_embeds = embed(responses_b)\n\n        sim_a = cosine_similarity(prompt_embeds, a_embeds).diagonal()\n        sim_b = cosine_similarity(prompt_embeds, b_embeds).diagonal()\n\n        return np.vstack([sim_a, sim_b]).T\n","metadata":{"id":"9GK8ToDivZmF"}},{"cell_type":"markdown","source":"print(os.listdir(\"/kaggle/input/open-minilm-l6-v2\"))\n","metadata":{"id":"38HPhRnTvZmF"}},{"cell_type":"markdown","source":"\nfrom sklearn.compose import ColumnTransformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", HFEmbedder(\"/kaggle/input/dbertav3-small\"), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(\"/kaggle/input/dbertav3-small\"), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(\"/kaggle/input/dbertav3-small\"), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"id":"NjgqbVAnvZmF"}},{"cell_type":"markdown","source":"\nfrom sklearn.compose import ColumnTransformer\n\n# Define the embedder with the desired model path\nembedder = HFEmbedder(model_path=\"sentence-transformers/all-MiniLM-L6-v2\")\npreprocessing = ColumnTransformer([\n    (\"sim_diff_squared\", PromptResponseSimilarity(\"/kaggle/input/open-minilm-l6-v2\"), [\"prompt_clean\", \"response_a_clean\", \"response_b_clean\"]),\n    (\"prompt_embed\", embedder, \"prompt_clean\"),\n    (\"resp_a_embed\", embedder, \"response_a_clean\"),\n    (\"resp_b_embed\", embedder, \"response_b_clean\"),\n])\n\n","metadata":{"id":"BYTlIehJvZmG"}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text_for_common_words(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    tokens = text.split()\n    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n\n\nclass CommonWordsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        common_meaningful_words_a = []\n        common_meaningful_words_b = []\n\n        for index, row in X.iterrows():\n            prompt_tokens = clean_text_for_common_words(row['prompt_clean'])\n            response_a_tokens = clean_text_for_common_words(row['response_a_clean'])\n            response_b_tokens = clean_text_for_common_words(row['response_b_clean'])\n\n            common_meaningful_a = len(set(prompt_tokens) & set(response_a_tokens))\n            common_meaningful_b = len(set(prompt_tokens) & set(response_b_tokens))\n\n            common_meaningful_words_a.append(common_meaningful_a)\n            common_meaningful_words_b.append(common_meaningful_b)\n\n        return np.array([common_meaningful_words_a, common_meaningful_words_b]).T","metadata":{"id":"d5e04fcf","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.274376Z","iopub.status.idle":"2025-08-30T06:47:22.274592Z","shell.execute_reply.started":"2025-08-30T06:47:22.274492Z","shell.execute_reply":"2025-08-30T06:47:22.274502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nimport numpy as np\nimport torch\n\n# Choose a different SentenceTransformer model\n# You can explore other models at https://www.sbert.net/docs/pretrained_models.html\nalternative_model_path = \"sentence-transformers/all-mpnet-base-v2\" # Using a larger SentenceTransformer model\nalternative_model = SentenceTransformer(alternative_model_path)\n\nif torch.cuda.is_available():\n    alternative_model = alternative_model.to(\"cuda\")\n\n\ndef calculate_alternative_similarity(df, model, batch_size=8, use_cuda=True):\n    prompts = df[\"prompt\"].tolist()\n    responses_a = df[\"response_a\"].tolist()\n    responses_b = df[\"response_b\"].tolist()\n\n    def embed(texts):\n        embeddings = model.encode(\n            texts,\n            show_progress_bar=False,\n            convert_to_numpy=True,\n            batch_size=batch_size,\n            device=\"cuda\" if use_cuda else \"cpu\"\n        )\n        return embeddings\n\n    prompt_embeds = embed(prompts)\n    a_embeds = embed(responses_a)\n    b_embeds = embed(responses_b)\n\n    sim_a = cosine_similarity(prompt_embeds, a_embeds).diagonal()\n    sim_b = cosine_similarity(prompt_embeds, b_embeds).diagonal()\n\n    return pd.DataFrame({'alternative_similarity_a': sim_a, 'alternative_similarity_b': sim_b})\n\n# Calculate alternative similarity scores for training and test data\n#alternative_similarity_train = calculate_alternative_similarity(X_train, alternative_model)\n#alternative_similarity_test = calculate_alternative_similarity(X_test, alternative_model)\n\n# You can now add these new similarity scores to your X_train and X_test dataframes\n#X_train = pd.concat([X_train, alternative_similarity_train], axis=1)\n#X_test = pd.concat([X_test, alternative_similarity_test], axis=1)\n\n#display(X_train.head())\n#display(X_test.head())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cca0cdb8","outputId":"83005984-3c47-40fb-9f41-bbbf4c011e00","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.275438Z","iopub.status.idle":"2025-08-30T06:47:22.275853Z","shell.execute_reply.started":"2025-08-30T06:47:22.275639Z","shell.execute_reply":"2025-08-30T06:47:22.275654Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"preprocessing = ColumnTransformer([\n    (\"similarities\", PromptResponseSimilarity(\"/kaggle/input/open-minilm-l6-v2\"), [\"prompt\", \"response_a\", \"response_b\"]),\n    (\"prompt_embed\", HFEmbedder(\"/kaggle/input/open-minilm-l6-v2\"), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(\"/kaggle/input/open-minilm-l6-v2\"), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(\"/kaggle/input/open-minilm-l6-v2\"), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"id":"qpnwVe5xvZmG"}},{"cell_type":"markdown","source":"preprocessing","metadata":{"id":"UnLkBOl8vZmG"}},{"cell_type":"code","source":"#feature_selection = SelectKBest(score_func = chi2, k=6)\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfeature_selection = SelectKBest(score_func=f_classif, k=6)\n","metadata":{"id":"sqroOVfcvZmG","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.276873Z","iopub.status.idle":"2025-08-30T06:47:22.277199Z","shell.execute_reply.started":"2025-08-30T06:47:22.277024Z","shell.execute_reply":"2025-08-30T06:47:22.277035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBClassifier(\n    objective=\"multi:softprob\",\n    num_class=3,\n    eval_metric=\"mlogloss\",\n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42\n)","metadata":{"id":"r2euUG6hvZmG","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.278155Z","iopub.status.idle":"2025-08-30T06:47:22.278422Z","shell.execute_reply.started":"2025-08-30T06:47:22.278304Z","shell.execute_reply":"2025-08-30T06:47:22.278316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Use HFEmbedder and CommonWordsTransformer\nembedder = HFEmbedder(model_path=\"sentence-transformers/all-MiniLM-L6-v2\")\ncommon_words_transformer = CommonWordsTransformer()\nsimilarity_transformer = calculate_alternative_similarity(model_path=\"sentence-transformers/all-mpnet-base-v2\")\n\n# Combine transformers using ColumnTransformer\n# Make sure to include the columns needed by each transformer\nfeature_union = ColumnTransformer(\n    transformers=[\n        (\"prompt_embed\", embedder, \"prompt\"),\n        (\"resp_a_embed\", embedder, \"response_a\"),\n        (\"resp_b_embed\", embedder, \"response_b\"),\n        (\"common_words\", common_words_transformer, [\"prompt_clean\", \"response_a_clean\", \"response_b_clean\"]),\n        (\"similarities\", similarity_transformer, [\"prompt\", \"response_a\", \"response_b\"]), # Corrected to use clean columns\n\n    ],\n    remainder='drop' # Drop other columns\n)\n\n# Define the model (assuming XGBClassifier is defined in a previous cell)\n# from your_model import model\n\n# Create the final pipeline\nmy_pipeline = Pipeline([\n    (\"features\", feature_union),\n    # Adjust indexing for common_words_diff based on the new feature_union output\n    # The common_words transformer now outputs the last two columns before the similarities transformer\n    (\"common_words_diff\", FunctionTransformer(lambda X: (X[:, -2] - X[:, -1]).reshape(-1, 1))), # Adjusted index based on the transformers order\n    (\"model\", model)\n])\n\ndisplay(my_pipeline)","metadata":{"id":"ncprUXqbvZmG","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.279053Z","iopub.status.idle":"2025-08-30T06:47:22.279278Z","shell.execute_reply.started":"2025-08-30T06:47:22.279177Z","shell.execute_reply":"2025-08-30T06:47:22.279187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"Q_Wo65bw01Ft","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = cross_val_score(my_pipeline, X_train, y_train, cv=2, scoring=\"accuracy\")\nprint(\"Accuracy:\", scores.mean())","metadata":{"id":"LVybFCKQvZmG","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.279955Z","iopub.status.idle":"2025-08-30T06:47:22.280211Z","shell.execute_reply.started":"2025-08-30T06:47:22.280101Z","shell.execute_reply":"2025-08-30T06:47:22.280113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"alBDr4PcvZmH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"SJtKiETbvZmL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_pipeline.fit(X_train, y_train)","metadata":{"id":"omrEMi2jvZmL","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.281132Z","iopub.status.idle":"2025-08-30T06:47:22.281612Z","shell.execute_reply.started":"2025-08-30T06:47:22.281497Z","shell.execute_reply":"2025-08-30T06:47:22.281509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"response_a_clean\"] = df_test[\"response_a\"].apply(clean_text)\ndf_test[\"response_b_clean\"] = df_test[\"response_b\"].apply(clean_text)\n\ndf_test[\"prompt_clean\"] = df_test[\"prompt\"].apply(clean_text)","metadata":{"id":"LuxNawNdvZmL","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.282597Z","iopub.status.idle":"2025-08-30T06:47:22.282845Z","shell.execute_reply.started":"2025-08-30T06:47:22.282731Z","shell.execute_reply":"2025-08-30T06:47:22.282743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"id":"fLyEMudNvZmM","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.284260Z","iopub.status.idle":"2025-08-30T06:47:22.284486Z","shell.execute_reply.started":"2025-08-30T06:47:22.284385Z","shell.execute_reply":"2025-08-30T06:47:22.284395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs = my_pipeline.predict_proba(df_test)\n\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"winner_model_a\": probs[:,0],\n    \"winner_model_b\": probs[:,1],\n    \"winner_tie\": probs[:,2],\n})\nprint(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"id":"D8ub3AzfvZmM","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.285212Z","iopub.status.idle":"2025-08-30T06:47:22.285437Z","shell.execute_reply.started":"2025-08-30T06:47:22.285328Z","shell.execute_reply":"2025-08-30T06:47:22.285337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"id":"I922gdVjvZmM","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.286944Z","iopub.status.idle":"2025-08-30T06:47:22.287237Z","shell.execute_reply.started":"2025-08-30T06:47:22.287089Z","shell.execute_reply":"2025-08-30T06:47:22.287101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train[\"response_a_clean\"] = X_train[\"response_a\"].apply(clean_text)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"].apply(clean_text)\nX_train[\"prompt_clean\"] = X_train[\"prompt\"].apply(clean_text)\n\nX_test[\"response_a_clean\"] = X_test[\"response_a\"].apply(clean_text)\nX_test[\"response_b_clean\"] = X_test[\"response_b\"].apply(clean_text)\nX_test[\"prompt_clean\"] = X_test[\"prompt\"].apply(clean_text)\n\n\ncommon_words_transformer = CommonWordsTransformer()\n\nX_train_common_words = common_words_transformer.transform(X_train)\nX_test_common_words = common_words_transformer.transform(X_test)\n\nX_train['common_meaningful_a'] = X_train_common_words[:, 0]\nX_train['common_meaningful_b'] = X_train_common_words[:, 1]\n\nX_test['common_meaningful_a'] = X_test_common_words[:, 0]\nX_test['common_meaningful_b'] = X_test_common_words[:, 1]\n\ndisplay(X_train.head())\ndisplay(X_test.head())","metadata":{"id":"41881256","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T06:47:22.288507Z","iopub.status.idle":"2025-08-30T06:47:22.288820Z","shell.execute_reply.started":"2025-08-30T06:47:22.288653Z","shell.execute_reply":"2025-08-30T06:47:22.288681Z"}},"outputs":[],"execution_count":null}]}