{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":205013,"sourceType":"modelInstanceVersion","modelInstanceId":4686,"modelId":2820}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-09-20T06:59:13.989622Z","iopub.execute_input":"2025-09-20T06:59:13.989916Z","iopub.status.idle":"2025-09-20T06:59:15.545510Z","shell.execute_reply.started":"2025-09-20T06:59:13.989889Z","shell.execute_reply":"2025-09-20T06:59:15.544715Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3/assets/tokenizer/vocabulary.spm\n/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n\n#!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\n\n#model = SentenceTransformer(\"all-mpnet-base-v2\")\n","metadata":{"execution":{"iopub.status.busy":"2025-09-20T06:59:15.547132Z","iopub.execute_input":"2025-09-20T06:59:15.547589Z","iopub.status.idle":"2025-09-20T06:59:46.219214Z","shell.execute_reply.started":"2025-09-20T06:59:15.547567Z","shell.execute_reply":"2025-09-20T06:59:46.218554Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-09-20 06:59:30.272297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758351570.466367      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758351570.528953      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n#model.save(\"/kaggle/working/all-mpnet-base-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:46.219979Z","iopub.execute_input":"2025-09-20T06:59:46.220624Z","iopub.status.idle":"2025-09-20T06:59:46.224508Z","shell.execute_reply.started":"2025-09-20T06:59:46.220600Z","shell.execute_reply":"2025-09-20T06:59:46.223542Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/qwen-llm\")\n#model = AutoModel.from_pretrained(\"/kaggle/input/qwen-llm\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:46.225497Z","iopub.execute_input":"2025-09-20T06:59:46.225792Z","iopub.status.idle":"2025-09-20T06:59:46.272141Z","shell.execute_reply.started":"2025-09-20T06:59:46.225764Z","shell.execute_reply":"2025-09-20T06:59:46.271363Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"submission_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\nsubmission_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:46.274166Z","iopub.execute_input":"2025-09-20T06:59:46.274481Z","iopub.status.idle":"2025-09-20T06:59:46.316584Z","shell.execute_reply.started":"2025-09-20T06:59:46.274461Z","shell.execute_reply":"2025-09-20T06:59:46.315656Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.333333        0.333333    0.333333\n1   211333        0.333333        0.333333    0.333333\n2  1233961        0.333333        0.333333    0.333333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ndf_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:46.317572Z","iopub.execute_input":"2025-09-20T06:59:46.317901Z","iopub.status.idle":"2025-09-20T06:59:49.701203Z","shell.execute_reply.started":"2025-09-20T06:59:46.317873Z","shell.execute_reply":"2025-09-20T06:59:49.700255Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.702411Z","iopub.execute_input":"2025-09-20T06:59:49.702708Z","iopub.status.idle":"2025-09-20T06:59:49.716302Z","shell.execute_reply.started":"2025-09-20T06:59:49.702675Z","shell.execute_reply":"2025-09-20T06:59:49.715593Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>[\"I have three oranges today, I ate an orange ...</td>\n      <td>[\"You have two oranges today.\"]</td>\n      <td>[\"You still have three oranges. Eating an oran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>[\"You are a mediator in a heated political deb...</td>\n      <td>[\"Thank you for sharing the details of the sit...</td>\n      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>[\"How to initialize the classification head wh...</td>\n      <td>[\"When you want to initialize the classificati...</td>\n      <td>[\"To initialize the classification head when p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.717172Z","iopub.execute_input":"2025-09-20T06:59:49.717464Z","iopub.status.idle":"2025-09-20T06:59:49.770558Z","shell.execute_reply.started":"2025-09-20T06:59:49.717438Z","shell.execute_reply":"2025-09-20T06:59:49.769600Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              57477 non-null  int64 \n 1   model_a         57477 non-null  object\n 2   model_b         57477 non-null  object\n 3   prompt          57477 non-null  object\n 4   response_a      57477 non-null  object\n 5   response_b      57477 non-null  object\n 6   winner_model_a  57477 non-null  int64 \n 7   winner_model_b  57477 non-null  int64 \n 8   winner_tie      57477 non-null  int64 \ndtypes: int64(4), object(5)\nmemory usage: 3.9+ MB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#df_train=df_train.head(30000)\n\n#)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.771427Z","iopub.execute_input":"2025-09-20T06:59:49.771751Z","iopub.status.idle":"2025-09-20T06:59:49.775892Z","shell.execute_reply.started":"2025-09-20T06:59:49.771722Z","shell.execute_reply":"2025-09-20T06:59:49.775112Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.776872Z","iopub.execute_input":"2025-09-20T06:59:49.777794Z","iopub.status.idle":"2025-09-20T06:59:49.799220Z","shell.execute_reply.started":"2025-09-20T06:59:49.777772Z","shell.execute_reply":"2025-09-20T06:59:49.798177Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.800172Z","iopub.execute_input":"2025-09-20T06:59:49.800532Z","iopub.status.idle":"2025-09-20T06:59:49.842694Z","shell.execute_reply.started":"2025-09-20T06:59:49.800510Z","shell.execute_reply":"2025-09-20T06:59:49.841875Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"id                0\nmodel_a           0\nmodel_b           0\nprompt            0\nresponse_a        0\nresponse_b        0\nwinner_model_a    0\nwinner_model_b    0\nwinner_tie        0\ndtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"X = df_train.drop(['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie'], axis = 1)\nX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.843746Z","iopub.execute_input":"2025-09-20T06:59:49.844064Z","iopub.status.idle":"2025-09-20T06:59:49.867080Z","shell.execute_reply.started":"2025-09-20T06:59:49.844036Z","shell.execute_reply":"2025-09-20T06:59:49.866332Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n0           30192  [\"Is it morally right to try to have a certain...   \n1           53567  [\"What is the difference between marriage lice...   \n2           65089  [\"explain function calling. how would you call...   \n3           96401  [\"How can I create a test set for a very rare ...   \n4          198779  [\"What is the best way to travel from Tel-Aviv...   \n...           ...                                                ...   \n57472  4294656694  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  4294692063  [\"In python, implement a naive Bayes with gaus...   \n57474  4294710549  [\"is it unethical to work on building weapons?...   \n57475  4294899228  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  4294947231  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  \n0      [\"As an AI, I don't have personal beliefs or o...  \n1      [\"A marriage license and a marriage certificat...  \n2      [\"Function calling is the process of invoking ...  \n3      [\"When building a classifier for a very rare c...  \n4      [\"The best way to travel from Tel-Aviv to Jeru...  \n...                                                  ...  \n57472  [\"Here is how that mnemonic represents the dig...  \n57473  [\"Sure! Here's an implementation of a naive Ba...  \n57474  [\"It depends on the context. Weapons can be us...  \n57475  [\"As an AI language model, I do not promote or...  \n57476  [\"If three kids eat three apples in three days...  \n\n[57477 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"y = df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n\ny = np.argmax(y, axis=1)\ny","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.868014Z","iopub.execute_input":"2025-09-20T06:59:49.868343Z","iopub.status.idle":"2025-09-20T06:59:49.877415Z","shell.execute_reply.started":"2025-09-20T06:59:49.868287Z","shell.execute_reply":"2025-09-20T06:59:49.876595Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([0, 1, 2, ..., 0, 1, 0])"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.881239Z","iopub.execute_input":"2025-09-20T06:59:49.881542Z","iopub.status.idle":"2025-09-20T06:59:49.902463Z","shell.execute_reply.started":"2025-09-20T06:59:49.881520Z","shell.execute_reply":"2025-09-20T06:59:49.901569Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.903434Z","iopub.execute_input":"2025-09-20T06:59:49.903744Z","iopub.status.idle":"2025-09-20T06:59:49.914870Z","shell.execute_reply.started":"2025-09-20T06:59:49.903713Z","shell.execute_reply":"2025-09-20T06:59:49.914123Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n30205  2245506736  [\"can you give me some escape room themes with...   \n8763    656268525  [\"Why did digital electronics switch from 5 vo...   \n30501  2266396135  [\"as an acclaimed and visionary author punishe...   \n33619  2497468695  [\"\\n\\nWrite a micro editor plugin that shows t...   \n22573  1679992057  [\"1. 19 year old Emese who is thin (165cm 48kg...   \n...           ...                                                ...   \n54343  4061186081         [\"mega-sena brazilian lottery statistics\"]   \n38158  2840606458                     [\"Draw a dog using ASCII art\"]   \n860      62750940                          [\"How to wear a bikini?\"]   \n15795  1180878567  [\"Can you tell me what was going on with all t...   \n56422  4218036675                          [\"SHOW ME WHAT YOU GOT!\"]   \n\n                                              response_a  \\\n30205  [\"Certainly! Here are several escape room them...   \n8763   [\"There are a few key reasons why digital elec...   \n30501  [\"As an acclaimed and visionary author, Elieze...   \n33619  [\"Here is Python code for a basic micro editor...   \n22573  [\"Certainly, let's address each of your points...   \n...                                                  ...   \n54343  [\"I do not have access to real-time mega-sena ...   \n38158                        [\"\\/_\\/\\\\  \\n( o.o )\\n> ^\"]   \n860    [\"Wearing a bikini can be a fun and comfortabl...   \n15795  [\"The direwolves, which are also known as dire...   \n56422  [\"Of course! As an AI, I can assist with a var...   \n\n                                              response_b  \n30205  [\"Here are some fun and immersive escape room ...  \n8763   [\"The shift from 5 volt logic to 3 volt logic ...  \n30501  [\"Eliezer Yudkowsky is often referred to as th...  \n33619  [\"Micro is a modern and intuitive terminal-bas...  \n22573  [\"1. From the description you provided, it see...  \n...                                                  ...  \n54343  [\"Mega-Sena is a Brazilian lottery game that o...  \n38158  [\"Here is a dog drawn with ASCII art:\\n\\n     ...  \n860    [\"Here are the basic steps to wear a bikini:\\n...  \n15795  [\"Sure! On the night of the Red Wedding, all o...  \n56422  [\"I'm a large language model, so I don't have ...  \n\n[45981 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30205</th>\n      <td>2245506736</td>\n      <td>[\"can you give me some escape room themes with...</td>\n      <td>[\"Certainly! Here are several escape room them...</td>\n      <td>[\"Here are some fun and immersive escape room ...</td>\n    </tr>\n    <tr>\n      <th>8763</th>\n      <td>656268525</td>\n      <td>[\"Why did digital electronics switch from 5 vo...</td>\n      <td>[\"There are a few key reasons why digital elec...</td>\n      <td>[\"The shift from 5 volt logic to 3 volt logic ...</td>\n    </tr>\n    <tr>\n      <th>30501</th>\n      <td>2266396135</td>\n      <td>[\"as an acclaimed and visionary author punishe...</td>\n      <td>[\"As an acclaimed and visionary author, Elieze...</td>\n      <td>[\"Eliezer Yudkowsky is often referred to as th...</td>\n    </tr>\n    <tr>\n      <th>33619</th>\n      <td>2497468695</td>\n      <td>[\"\\n\\nWrite a micro editor plugin that shows t...</td>\n      <td>[\"Here is Python code for a basic micro editor...</td>\n      <td>[\"Micro is a modern and intuitive terminal-bas...</td>\n    </tr>\n    <tr>\n      <th>22573</th>\n      <td>1679992057</td>\n      <td>[\"1. 19 year old Emese who is thin (165cm 48kg...</td>\n      <td>[\"Certainly, let's address each of your points...</td>\n      <td>[\"1. From the description you provided, it see...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54343</th>\n      <td>4061186081</td>\n      <td>[\"mega-sena brazilian lottery statistics\"]</td>\n      <td>[\"I do not have access to real-time mega-sena ...</td>\n      <td>[\"Mega-Sena is a Brazilian lottery game that o...</td>\n    </tr>\n    <tr>\n      <th>38158</th>\n      <td>2840606458</td>\n      <td>[\"Draw a dog using ASCII art\"]</td>\n      <td>[\"\\/_\\/\\\\  \\n( o.o )\\n&gt; ^\"]</td>\n      <td>[\"Here is a dog drawn with ASCII art:\\n\\n     ...</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>62750940</td>\n      <td>[\"How to wear a bikini?\"]</td>\n      <td>[\"Wearing a bikini can be a fun and comfortabl...</td>\n      <td>[\"Here are the basic steps to wear a bikini:\\n...</td>\n    </tr>\n    <tr>\n      <th>15795</th>\n      <td>1180878567</td>\n      <td>[\"Can you tell me what was going on with all t...</td>\n      <td>[\"The direwolves, which are also known as dire...</td>\n      <td>[\"Sure! On the night of the Red Wedding, all o...</td>\n    </tr>\n    <tr>\n      <th>56422</th>\n      <td>4218036675</td>\n      <td>[\"SHOW ME WHAT YOU GOT!\"]</td>\n      <td>[\"Of course! As an AI, I can assist with a var...</td>\n      <td>[\"I'm a large language model, so I don't have ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>45981 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.915877Z","iopub.execute_input":"2025-09-20T06:59:49.916178Z","iopub.status.idle":"2025-09-20T06:59:49.934707Z","shell.execute_reply.started":"2025-09-20T06:59:49.916151Z","shell.execute_reply":"2025-09-20T06:59:49.933834Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n37379  2785062085                     [\"what does hello world mean\"]   \n644      48259531  [\"I ran a marathon in 3:12:00 weighting 84kg. ...   \n48496  3622731894  [\"Below is an instruction that describes a tas...   \n12603   933663766  [\"How do I run static analysis with gcc in QT ...   \n16697  1246768370  [\"What did the music producer \\u00d8fdream die...   \n...           ...                                                ...   \n3151    240261313        [\"How to save a web page into a jpg file?\"]   \n30983  2300512815  [\"provide a short story about a pirate that le...   \n31183  2314277410  [\"Name 3 features of Alain Badiou's ontology t...   \n50634  3781372084  [\"Please give me a list of concise bullet poin...   \n23146  1722610582                      [\"What is funnier than 420?\"]   \n\n                                              response_a  \\\n37379  [\"\\\"Hello, World!\\\" is a phrase used in comput...   \n644    [\"It's difficult to provide an exact answer to...   \n48496  [\"\\u8fd9\\u6b3e\\u7eaf\\u8272\\u4e94\\u5206\\u88e4\\u...   \n12603  [\"To run static analysis with gcc in QT Creato...   \n16697  [\"I am not able to provide information on the ...   \n...                                                  ...   \n3151   [\"Here are a few ways to save a web page as a ...   \n30983  [\"Here is a short story about a pirate who lea...   \n31183  [\"Alain Badiou and Plato have distinct philoso...   \n50634  [\"- Musician who plays guitar and enjoys jammi...   \n23146  [\"The question \\\"What is funnier than 420?\\\" i...   \n\n                                              response_b  \n37379  [\"\\\"Hello, World!\\\" is a common phrase used to...  \n644    [\"To accurately estimate how much faster you w...  \n48496  [\"\\u6839\\u636e\\u63d0\\u4f9b\\u7684\\u4ea7\\u54c1\\u...  \n12603  [\"To run static analysis with gcc in QT Creato...  \n16697  [\"I'm really sorry, but I don't have the abili...  \n...                                                  ...  \n3151   [\"Here are a few ways to save a web page as a ...  \n30983  [\"Once upon a time, in the golden age of pirac...  \n31183  [\"Here are 3 key differences between Alain Bad...  \n50634  [\"* Music: plays guitar, drums, and bass\\n* Sp...  \n23146  [\"Humor is subjective and can vary from person...  \n\n[11496 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37379</th>\n      <td>2785062085</td>\n      <td>[\"what does hello world mean\"]</td>\n      <td>[\"\\\"Hello, World!\\\" is a phrase used in comput...</td>\n      <td>[\"\\\"Hello, World!\\\" is a common phrase used to...</td>\n    </tr>\n    <tr>\n      <th>644</th>\n      <td>48259531</td>\n      <td>[\"I ran a marathon in 3:12:00 weighting 84kg. ...</td>\n      <td>[\"It's difficult to provide an exact answer to...</td>\n      <td>[\"To accurately estimate how much faster you w...</td>\n    </tr>\n    <tr>\n      <th>48496</th>\n      <td>3622731894</td>\n      <td>[\"Below is an instruction that describes a tas...</td>\n      <td>[\"\\u8fd9\\u6b3e\\u7eaf\\u8272\\u4e94\\u5206\\u88e4\\u...</td>\n      <td>[\"\\u6839\\u636e\\u63d0\\u4f9b\\u7684\\u4ea7\\u54c1\\u...</td>\n    </tr>\n    <tr>\n      <th>12603</th>\n      <td>933663766</td>\n      <td>[\"How do I run static analysis with gcc in QT ...</td>\n      <td>[\"To run static analysis with gcc in QT Creato...</td>\n      <td>[\"To run static analysis with gcc in QT Creato...</td>\n    </tr>\n    <tr>\n      <th>16697</th>\n      <td>1246768370</td>\n      <td>[\"What did the music producer \\u00d8fdream die...</td>\n      <td>[\"I am not able to provide information on the ...</td>\n      <td>[\"I'm really sorry, but I don't have the abili...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3151</th>\n      <td>240261313</td>\n      <td>[\"How to save a web page into a jpg file?\"]</td>\n      <td>[\"Here are a few ways to save a web page as a ...</td>\n      <td>[\"Here are a few ways to save a web page as a ...</td>\n    </tr>\n    <tr>\n      <th>30983</th>\n      <td>2300512815</td>\n      <td>[\"provide a short story about a pirate that le...</td>\n      <td>[\"Here is a short story about a pirate who lea...</td>\n      <td>[\"Once upon a time, in the golden age of pirac...</td>\n    </tr>\n    <tr>\n      <th>31183</th>\n      <td>2314277410</td>\n      <td>[\"Name 3 features of Alain Badiou's ontology t...</td>\n      <td>[\"Alain Badiou and Plato have distinct philoso...</td>\n      <td>[\"Here are 3 key differences between Alain Bad...</td>\n    </tr>\n    <tr>\n      <th>50634</th>\n      <td>3781372084</td>\n      <td>[\"Please give me a list of concise bullet poin...</td>\n      <td>[\"- Musician who plays guitar and enjoys jammi...</td>\n      <td>[\"* Music: plays guitar, drums, and bass\\n* Sp...</td>\n    </tr>\n    <tr>\n      <th>23146</th>\n      <td>1722610582</td>\n      <td>[\"What is funnier than 420?\"]</td>\n      <td>[\"The question \\\"What is funnier than 420?\\\" i...</td>\n      <td>[\"Humor is subjective and can vary from person...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11496 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.935677Z","iopub.execute_input":"2025-09-20T06:59:49.935915Z","iopub.status.idle":"2025-09-20T06:59:49.949644Z","shell.execute_reply.started":"2025-09-20T06:59:49.935897Z","shell.execute_reply":"2025-09-20T06:59:49.948832Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([2, 1, 1, ..., 1, 1, 0])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"catagorical_feature = [col for col in X.columns if X[col].dtype == 'object']\ncatagorical_feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.950504Z","iopub.execute_input":"2025-09-20T06:59:49.950822Z","iopub.status.idle":"2025-09-20T06:59:49.964974Z","shell.execute_reply.started":"2025-09-20T06:59:49.950769Z","shell.execute_reply":"2025-09-20T06:59:49.964266Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['prompt', 'response_a', 'response_b']"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.965741Z","iopub.execute_input":"2025-09-20T06:59:49.965961Z","iopub.status.idle":"2025-09-20T06:59:49.979426Z","shell.execute_reply.started":"2025-09-20T06:59:49.965945Z","shell.execute_reply":"2025-09-20T06:59:49.978576Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport os # Import os module\nimport multiprocessing\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path, batch_size=16, max_length=512, use_fast_tokenizer=True, enable_mixed_precision=True, embedding_type=\"mean\"):\n        self.embedding_type = embedding_type\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision # Add this line to store the parameter\n        self._cpu_count = multiprocessing.cpu_count()\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # optionally use mixed precision on GPUs (speeds up fp16 capable GPUs)\n        try:\n            if self.enable_mixed_precision: # Use self.enable_mixed_precision\n                from tensorflow.keras import mixed_precision\n                mixed_precision.set_global_policy(\"mixed_float16\")\n        except Exception:\n            pass\n\n        # Load tokenizer (prefer fast tokenizer if available)\n        try:\n            if self.use_fast_tokenizer:\n                from transformers import DebertaV2TokenizerFast as _TokFast\n                self.tokenizer = _TokFast(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            else:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n        except Exception:\n            # fallback to original import name/location\n            try:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            except Exception:\n                raise\n\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        config = self._detect_model_config()\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=config[\"num_layers\"],\n            num_heads=config[\"num_heads\"],\n            hidden_dim=config[\"hidden_dim\"],\n            intermediate_dim=config[\"intermediate_dim\"],\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n\n\n        self.model.load_weights(os.path.join(self.model_path, \"model.weights.h5\"),skip_mismatch=True) # Use os.path.join\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n    def _detect_model_config(self):\n            import os\n            import json\n        \n            config_path = os.path.join(self.model_path, \"config.json\")\n            if os.path.exists(config_path):\n                try:\n                    with open(config_path, \"r\") as f:\n                        config = json.load(f)\n                    return {\n                        \"num_layers\": config.get(\"num_layers\", 12),\n                        \"num_heads\": config.get(\"num_attention_heads\", 12),\n                        \"hidden_dim\": config.get(\"hidden_size\", 768),\n                        \"intermediate_dim\": config.get(\"intermediate_size\", 3072),\n                    }\n                except Exception as e:\n                    print(f\"⚠️ Failed to read config.json: {e}\")\n        \n            # Fallback: infer from folder name\n            path_lower = self.model_path.lower()\n            if \"small\" in path_lower:\n                return {\n                    \"num_layers\": 12,\n                    \"num_heads\": 6,\n                    \"hidden_dim\": 768,\n                    \"intermediate_dim\": 3072,\n                }\n            elif \"base\" in path_lower:\n                return {\n                    \"num_layers\": 12,\n                    \"num_heads\": 12,\n                    \"hidden_dim\": 768,\n                    \"intermediate_dim\": 3072,\n                }\n        \n            print(\"⚠️ Could not auto-detect model config. Using default base config.\")\n            return {\n                \"num_layers\": 12,\n                \"num_heads\": 12,\n                \"hidden_dim\": 768,\n                \"intermediate_dim\": 3072,\n            }\n\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n    \n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n    \n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n    \n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            try:\n                hidden_dim = int(self.model.output_shape[-1])\n                return {\n                    \"raw\": np.zeros((0, self.max_length, hidden_dim), dtype=np.float32),\n                    \"mean\": np.zeros((0, hidden_dim), dtype=np.float32),\n                    \"median\": np.zeros((0, hidden_dim), dtype=np.float32),\n                    #\"max\": np.zeros((0, hidden_dim), dtype=np.float32),\n                    #\"min\": np.zeros((0, hidden_dim), dtype=np.float32),\n                }\n            except Exception:\n                return {}\n    \n        # Store outputs\n        raw_outputs = []\n    \n        for i in range(0, n, self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",\n                )\n                token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n                attention_mask_tf = tf.cast(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                outputs_np = outputs.numpy()\n            except Exception:\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                token_ids_tf = tf.convert_to_tensor(tokens[\"input_ids\"], dtype=tf.int32)\n                attention_mask_tf = tf.convert_to_tensor(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                outputs_np = outputs.numpy()\n    \n            raw_outputs.append(outputs_np)\n    \n        # Combine all batches\n        full_output = np.vstack(raw_outputs)  # shape: [n_samples, seq_len, hidden_dim]\n    \n        \n        mean_embeddings = np.mean(full_output, axis=1)\n        median_embeddings = np.median(full_output, axis=1)\n        #max_embeddings = np.max(full_output, axis=1)\n        #min_embeddings = np.min(full_output, axis=1)\n    \n        # Return per-sample dicts\n        return [\n            {       \n                \"text\": texts[i],\n                \"raw\": full_output[i],\n                \"mean\": mean_embeddings[i],\n                \"median\": median_embeddings[i],\n                #\"max\": max_embeddings[i],\n                #\"min\": min_embeddings[i],\n            }\n            for i in range(len(texts))\n        ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:49.980326Z","iopub.execute_input":"2025-09-20T06:59:49.980641Z","iopub.status.idle":"2025-09-20T06:59:51.061142Z","shell.execute_reply.started":"2025-09-20T06:59:49.980612Z","shell.execute_reply":"2025-09-20T06:59:51.060472Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"%pip install nltk ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:51.061928Z","iopub.execute_input":"2025-09-20T06:59:51.062145Z","iopub.status.idle":"2025-09-20T06:59:55.483639Z","shell.execute_reply.started":"2025-09-20T06:59:51.062129Z","shell.execute_reply":"2025-09-20T06:59:55.482371Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text_for_common_words(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    tokens = text.split()\n    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    #return [word for word in tokens]\n\nclass CommonWordsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        common_meaningful_words_a = []\n        common_meaningful_words_b = []\n\n        for index, row in X.iterrows():\n            prompt_tokens = clean_text_for_common_words(row['prompt'])\n            response_a_tokens = clean_text_for_common_words(row['response_a'])\n            response_b_tokens = clean_text_for_common_words(row['response_b'])\n\n            common_meaningful_a = len(set(prompt_tokens) & set(response_a_tokens))\n            common_meaningful_b = len(set(prompt_tokens) & set(response_b_tokens))\n\n            common_meaningful_words_a.append(common_meaningful_a)\n            common_meaningful_words_b.append(common_meaningful_b)\n\n        return np.array([common_meaningful_words_a, common_meaningful_words_b]).T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:55.485098Z","iopub.execute_input":"2025-09-20T06:59:55.485428Z","iopub.status.idle":"2025-09-20T06:59:56.136980Z","shell.execute_reply.started":"2025-09-20T06:59:55.485401Z","shell.execute_reply":"2025-09-20T06:59:56.136034Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"X_train[\"prompt_clean\"] = X_train[\"prompt\"]#.apply(clean_text_for_common_words)\nX_train[\"response_a_clean\"] = X_train[\"response_a\"]#.apply(clean_text_for_common_words)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"]#.apply(clean_text_for_common_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:56.137854Z","iopub.execute_input":"2025-09-20T06:59:56.138109Z","iopub.status.idle":"2025-09-20T06:59:56.219679Z","shell.execute_reply.started":"2025-09-20T06:59:56.138086Z","shell.execute_reply":"2025-09-20T06:59:56.218858Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass PromptResponseSimilarity(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # X is a 2D array of shape (n_samples, 3 * embedding_dim)\n        n_samples, total_dim = X.shape\n        embedding_dim = total_dim // 3\n\n        prompt_embeds = X[:, :embedding_dim]\n        resp_a_embeds = X[:, embedding_dim:2*embedding_dim]\n        resp_b_embeds = X[:, 2*embedding_dim:]\n\n        sim_a = np.array([\n            cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0]\n            for p, a in zip(prompt_embeds, resp_a_embeds)\n        ])\n        sim_b = np.array([\n            cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0]\n            for p, b in zip(prompt_embeds, resp_b_embeds)\n        ])\n\n        return np.vstack([sim_a, sim_b]).T\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:56.220544Z","iopub.execute_input":"2025-09-20T06:59:56.220928Z","iopub.status.idle":"2025-09-20T06:59:56.236595Z","shell.execute_reply.started":"2025-09-20T06:59:56.220897Z","shell.execute_reply":"2025-09-20T06:59:56.235720Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class RawEmbeddingSimilarity(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        prompt_embeds = X[\"prompt_embed\"].apply(lambda x: x[\"raw\"]).tolist()\n        resp_a_embeds = X[\"resp_a_embed\"].apply(lambda x: x[\"raw\"]).tolist()\n        resp_b_embeds = X[\"resp_b_embed\"].apply(lambda x: x[\"raw\"]).tolist()\n    \n        sim_a, sim_b = [], []\n        for p, a, b in zip(prompt_embeds, resp_a_embeds, resp_b_embeds):\n            sim_a.append(cosine_similarity(p.flatten().reshape(1, -1), a.flatten().reshape(1, -1))[0, 0])\n            sim_b.append(cosine_similarity(p.flatten().reshape(1, -1), b.flatten().reshape(1, -1))[0, 0])\n    \n        return np.vstack([sim_a, sim_b]).T\n\n        \nclass AggregatedEmbeddingSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, agg_type=\"mean\"):\n        self.agg_type = agg_type\n\n    def fit(self, X, y=None):\n        return self\n\n    \n    def transform(self, X):\n        prompt_embeds = X[\"prompt_embed\"].apply(lambda x: x[self.agg_type]).tolist()\n        resp_a_embeds = X[\"resp_a_embed\"].apply(lambda x: x[self.agg_type]).tolist()\n        resp_b_embeds = X[\"resp_b_embed\"].apply(lambda x: x[self.agg_type]).tolist()\n    \n        sim_a = [cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0] for p, a in zip(prompt_embeds, resp_a_embeds)]\n        sim_b = [cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0] for p, b in zip(prompt_embeds, resp_b_embeds)]\n    \n        return np.vstack([sim_a, sim_b]).T\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:56.237504Z","iopub.execute_input":"2025-09-20T06:59:56.237844Z","iopub.status.idle":"2025-09-20T06:59:56.257899Z","shell.execute_reply.started":"2025-09-20T06:59:56.237813Z","shell.execute_reply":"2025-09-20T06:59:56.257091Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pandas as pd\n\nclass TFIDFAttentionEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, tokenizer, max_length=512, embedding_dim=768):\n        self.tokenizer = tokenizer\n        self.vectorizer = TfidfVectorizer()\n        self.max_length = max_length\n        self.embedding_dim = embedding_dim\n\n    def fit(self, X, y=None):\n        texts = []\n\n        # Extract texts from various input formats\n        if isinstance(X, (list, np.ndarray)):\n            for row in X:\n                if isinstance(row, (list, tuple)):\n                    for item in row:\n                        if isinstance(item, dict):\n                            text = item.get(\"text\", \"\")\n                            if text.strip():\n                                texts.append(text)\n                elif isinstance(row, dict):\n                    text = row.get(\"text\", \"\")\n                    if text.strip():\n                        texts.append(text)\n        elif isinstance(X, pd.DataFrame):\n            for col in X.columns:\n                col_texts = X[col].apply(lambda x: x.get(\"text\", \"\") if isinstance(x, dict) else \"\").tolist()\n                texts.extend([t for t in col_texts if t.strip()])\n\n        if not texts:\n            raise ValueError(\"No valid texts found for TF-IDF fitting.\")\n\n        self.vectorizer.fit(texts)\n        return self\n\n    def transform(self, X):\n        weighted_embeddings_mean = []\n        weighted_embeddings_median = []\n    \n        for i, sample in enumerate(X):\n            text = sample.get(\"text\", \"\")\n            token_embeddings = sample.get(\"raw\", None)\n    \n            if token_embeddings is None or len(text.strip()) == 0:\n                print(f\"[TFIDF Warning] Row {i} has no token embeddings or empty text.\")\n                print(f\"Text: {text}\\n\")\n                zero_vec = np.zeros(self.embedding_dim)\n                weighted_embeddings_mean.append(zero_vec)\n                weighted_embeddings_median.append(zero_vec)\n                continue\n    \n            tokens = self.tokenizer.tokenize(text)\n            tfidf_vector = self.vectorizer.transform([text]).toarray()[0]\n    \n            token_weights = []\n            missing_tokens = []\n    \n            for token in tokens[:self.max_length]:\n                token_clean = token.lower().replace(\"▁\", \"\")\n                idx = self.vectorizer.vocabulary_.get(token_clean, None)\n                if idx is not None:\n                    token_weights.append(tfidf_vector[idx])\n                else:\n                    token_weights.append(0.0)\n                    missing_tokens.append(token)\n    \n            token_weights = np.array(token_weights)\n            token_embeddings = token_embeddings[:len(token_weights)]\n    \n            if token_weights.sum() == 0 or token_embeddings.shape[0] == 0:\n                print(f\"[TFIDF Warning] Row {i} has zero TF-IDF weights.\")\n                #print(f\"Text: {text}\")\n                #print(f\"Tokens: {tokens}\")\n                #print(f\"Missing from TF-IDF vocab: {missing_tokens}\\n\")\n                zero_vec = np.zeros(token_embeddings.shape[-1])\n                weighted_embeddings_mean.append(zero_vec)\n                weighted_embeddings_median.append(zero_vec)\n                continue\n    \n            # Weighted mean\n            token_weights /= token_weights.sum()\n            weighted_mean = np.average(token_embeddings, axis=0, weights=token_weights)\n            weighted_embeddings_mean.append(weighted_mean)\n    \n            # Weighted median\n            weighted_median = np.zeros(token_embeddings.shape[1])\n            for dim in range(token_embeddings.shape[1]):\n                dim_values = token_embeddings[:, dim]\n                sorted_indices = np.argsort(dim_values)\n                sorted_weights = token_weights[sorted_indices]\n                cumsum_weights = np.cumsum(sorted_weights)\n                median_idx = np.searchsorted(cumsum_weights, 0.5 * cumsum_weights[-1])\n                weighted_median[dim] = dim_values[sorted_indices[median_idx]]\n            weighted_embeddings_median.append(weighted_median)\n    \n        return np.hstack([\n            np.array(weighted_embeddings_mean),\n            np.array(weighted_embeddings_median)\n        ])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:56.258972Z","iopub.execute_input":"2025-09-20T06:59:56.259497Z","iopub.status.idle":"2025-09-20T06:59:56.284227Z","shell.execute_reply.started":"2025-09-20T06:59:56.259473Z","shell.execute_reply":"2025-09-20T06:59:56.283214Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"    def transform(self, X):\n        weighted_embeddings_mean = []\n        weighted_embeddings_median = []\n    \n        for sample in X:\n            # If sample is a dict (expected format), extract fields\n            if isinstance(sample, dict):\n                text = sample.get(\"text\", \"\")\n                token_embeddings = sample.get(\"raw\", None)\n            else:\n                # Fallback: treat sample as plain text or unsupported format\n                text = str(sample)\n                token_embeddings = None\n    \n            # Handle empty or invalid cases\n            if token_embeddings is None or len(text.strip()) == 0:\n                print(f\"[TFIDF Warning] Row {i} has no token embeddings or empty text.\")\n                print(f\"Text: {text}\\n\")\n                zero_vec = np.zeros(self.embedding_dim)\n                weighted_embeddings_mean.append(zero_vec)\n                weighted_embeddings_median.append(zero_vec)\n                continue\n    \n            tokens = self.tokenizer.tokenize(text)\n            tfidf_vector = self.vectorizer.transform([text]).toarray()[0]\n    \n            token_weights = []\n            for token in tokens[:self.max_length]:\n                idx = self.vectorizer.vocabulary_.get(token.lower(), None)\n                token_weights.append(tfidf_vector[idx] if idx is not None else 0.0)\n    \n            token_weights = np.array(token_weights)\n            token_embeddings = token_embeddings[:len(token_weights)]\n    \n            if token_weights.sum() > 0:\n                token_weights = token_weights / token_weights.sum()\n    \n            if token_weights.sum() == 0 or token_embeddings.shape[0] == 0:\n                \n                print(f\"[TFIDF Warning] Row {i} has zero TF-IDF weights.\")\n                print(f\"Text: {text}\")\n                print(f\"Tokens: {tokens}\")\n                print(f\"Missing from TF-IDF vocab: {missing_tokens}\\n\")\n                zero_vec = np.zeros(token_embeddings.shape[-1])\n                weighted_embeddings_mean.append(zero_vec)\n                weighted_embeddings_median.append(zero_vec)\n            else:\n                # Calculate weighted mean\n                weighted_mean = np.average(token_embeddings, axis=0, weights=token_weights)\n                weighted_embeddings_mean.append(weighted_mean)\n                \n                # Calculate weighted median\n                # For each dimension, sort values and find the weighted median\n                weighted_median = np.zeros(token_embeddings.shape[1])\n                for dim in range(token_embeddings.shape[1]):\n                    dim_values = token_embeddings[:, dim]\n                    sorted_indices = np.argsort(dim_values)\n                    sorted_weights = token_weights[sorted_indices]\n                    cumsum_weights = np.cumsum(sorted_weights)\n                    median_idx = np.searchsorted(cumsum_weights, 0.5 * cumsum_weights[-1])\n                    weighted_median[dim] = dim_values[sorted_indices[median_idx]]\n                weighted_embeddings_median.append(weighted_median)\n    \n        # Stack and return both mean and median embeddings side by side\n        return np.hstack([\n            np.array(weighted_embeddings_mean),\n            np.array(weighted_embeddings_median)\n        ])\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedder = HFEmbedder(model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3\")\n#/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3\ncommon_words_transformer = CommonWordsTransformer()\n# Updated preprocessing transformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", embedder, \"prompt_clean\"),\n    (\"resp_a_embed\", embedder, \"response_a_clean\"),\n    (\"resp_b_embed\", embedder, \"response_b_clean\"),\n    (\"common_words\", common_words_transformer, [\"prompt\", \"response_a\", \"response_b\"]),\n    #(\"num\", \"passthrough\", [\"id\"])\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:56.285190Z","iopub.execute_input":"2025-09-20T06:59:56.285508Z","iopub.status.idle":"2025-09-20T07:00:14.228452Z","shell.execute_reply.started":"2025-09-20T06:59:56.285476Z","shell.execute_reply":"2025-09-20T07:00:14.227775Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nI0000 00:00:1758351599.413449      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"class DictWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, embedder):\n        self.embedder = embedder\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Returns a 2D array of shape (n_samples, 1), each cell is a dict\n        embeddings = self.embedder.transform(X)\n        return np.array(embeddings).reshape(-1, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:00:14.229365Z","iopub.execute_input":"2025-09-20T07:00:14.229584Z","iopub.status.idle":"2025-09-20T07:00:14.234972Z","shell.execute_reply.started":"2025-09-20T07:00:14.229559Z","shell.execute_reply":"2025-09-20T07:00:14.234140Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class EmbeddingStatsExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, fields=(\"mean\", \"median\")):\n        self.fields = fields\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        features = []\n        for row in X:\n            row_features = []\n            for field in self.fields:\n                vec = row.get(field, np.zeros(768))  # fallback to zeros if missing\n                row_features.extend(vec)\n            features.append(row_features)\n        return np.array(features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:00:14.236119Z","iopub.execute_input":"2025-09-20T07:00:14.236494Z","iopub.status.idle":"2025-09-20T07:00:14.293395Z","shell.execute_reply.started":"2025-09-20T07:00:14.236463Z","shell.execute_reply":"2025-09-20T07:00:14.292554Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nimport gc\n# Convert numpy arrays to sparse matrices if needed\nfrom scipy.sparse import csr_matrix, hstack\n# Initialize the shared embedder\nshared_embedder = HFEmbedder(model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3\")\n\n# Step 1: Get embeddings for each text column\nembedding_stage = ColumnTransformer([\n    (\"prompt_embed\", DictWrapper(shared_embedder), \"prompt_clean\"),\n    (\"resp_a_embed\", DictWrapper(shared_embedder), \"response_a_clean\"),\n    (\"resp_b_embed\", DictWrapper(shared_embedder), \"response_b_clean\"),\n])\n\n# Fit and transform the embeddings\nprint(\"Step 1: Generating embeddings...\")\n# Configuration\nCHUNK_SIZE = 768  # Adjust based on your memory constraints\nOUTPUT_FILE = \"embedded_features.pkl\"\n\n\ndef process_in_chunks(data, chunk_size=CHUNK_SIZE):\n    num_samples = len(data)\n    results = []\n    \n    for start_idx in range(0, num_samples, chunk_size):\n        print(f\"\\nProcessing chunk {start_idx//chunk_size + 1}/{(num_samples + chunk_size - 1)//chunk_size}\")\n        \n        # Get current chunk\n        end_idx = min(start_idx + chunk_size, num_samples)\n        chunk = data.iloc[start_idx:end_idx]\n        \n        # Generate embeddings for chunk\n        print(\"Generating embeddings...\")\n        embedded_chunk = embedding_stage.fit_transform(chunk)\n        \n        # Convert to DataFrame format\n        chunk_df = pd.DataFrame([{\n            \"prompt_embed\": x[0],\n            \"resp_a_embed\": x[1],\n            \"resp_b_embed\": x[2]\n        } for x in embedded_chunk])\n        # Step 2: Extract statistics from embeddings\n        print(\"\\nStep 2: Extracting embedding statistics...\")\n        \n        # Process each type of embedding separately\n        prompt_stats = EmbeddingStatsExtractor().fit_transform(chunk_df[\"prompt_embed\"])\n        resp_a_stats = EmbeddingStatsExtractor().fit_transform(chunk_df[\"resp_a_embed\"])\n        resp_b_stats = EmbeddingStatsExtractor().fit_transform(chunk_df[\"resp_b_embed\"])\n        \n        print(f\"Prompt stats shape: {prompt_stats.shape}\")\n        print(f\"Response A stats shape: {resp_a_stats.shape}\")\n        print(f\"Response B stats shape: {resp_b_stats.shape}\")\n        #Step 3: Calculate similarities\n        print(\"\\nStep 3: Calculating similarities...\")\n        \n        # Raw embedding similarity\n        raw_sim = RawEmbeddingSimilarity().fit_transform(chunk_df)\n        print(f\"Raw similarity shape: {raw_sim.shape}\")\n        \n        # Mean and median similarities\n        mean_sim = AggregatedEmbeddingSimilarity(agg_type=\"mean\").fit_transform(chunk_df)\n        median_sim = AggregatedEmbeddingSimilarity(agg_type=\"median\").fit_transform(chunk_df)\n        \n        print(f\"Mean similarity shape: {mean_sim.shape}\")\n        print(f\"Median similarity shape: {median_sim.shape}\")\n        # Step 4: TF-IDF attention\n        print(\"\\nStep 4: Computing TF-IDF attention...\")\n        \n        # Create and fit TF-IDF embedders\n        tfidf_prompt = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\n        tfidf_resp_a = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\n        tfidf_resp_b = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\n        \n        # Transform each column\n        prompt_tfidf = tfidf_prompt.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in chunk_df['prompt_embed']])\n        resp_a_tfidf = tfidf_resp_a.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in chunk_df['resp_a_embed']])\n        resp_b_tfidf = tfidf_resp_b.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in chunk_df['resp_b_embed']])\n        \n        print(f\"Prompt TF-IDF shape: {prompt_tfidf.shape}\")\n        print(f\"Response A TF-IDF shape: {resp_a_tfidf.shape}\")\n        print(f\"Response B TF-IDF shape: {resp_b_tfidf.shape}\")\n        # Calculate TF-IDF weighted similarities (one-to-one)\n        print(\"\\nCalculating one-to-one TF-IDF weighted similarities...\")\n        half_dim=768\n        prompt_mean = prompt_tfidf[:, :half_dim]\n        prompt_median = prompt_tfidf[:, half_dim:]\n        resp_a_mean = resp_a_tfidf[:, :half_dim]\n        resp_a_median = resp_a_tfidf[:, half_dim:]\n        resp_b_mean = resp_b_tfidf[:, :half_dim]\n        resp_b_median = resp_b_tfidf[:, half_dim:]\n        \n        # Calculate similarities between corresponding pairs\n        mean_sim_a = np.array([\n            cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0]\n            for p, a in zip(prompt_mean, resp_a_mean)\n        ])\n        mean_sim_b = np.array([\n            cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0]\n            for p, b in zip(prompt_mean, resp_b_mean)\n        ])\n        \n        median_sim_a = np.array([\n            cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0]\n            for p, a in zip(prompt_median, resp_a_median)\n        ])\n        median_sim_b = np.array([\n            cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0]\n            for p, b in zip(prompt_median, resp_b_median)\n        ])\n        \n        # Create DataFrame with one-to-one similarities\n        similarities_df = pd.DataFrame({\n            'tfidf_mean_sim_a': mean_sim_a,\n            'tfidf_mean_sim_b': mean_sim_b,\n            'tfidf_median_sim_a': median_sim_a,\n            'tfidf_median_sim_b': median_sim_b\n        })\n        \n        print(\"\\nSimilarity shapes:\")\n        print(f\"Number of rows: {len(similarities_df)}\")\n        \n       \n        def ensure_sparse(x, dtype=np.float32):\n            if x is None:\n                return None\n            # Convert to numpy array first with consistent dtype\n            if not isinstance(x, (csr_matrix, np.ndarray)):\n                x = np.array(x, dtype=dtype)\n            elif isinstance(x, np.ndarray):\n                x = x.astype(dtype)\n            \n            # If already sparse, ensure correct dtype\n            if isinstance(x, csr_matrix):\n                return x.astype(dtype)\n            \n            # Convert to sparse\n            return csr_matrix(x)\n        \n        # Print shapes and dtypes before conversion\n        print(\"\\nFeature shapes and types before conversion:\")\n        features = [\n            #(\"prompt_stats\", prompt_stats),\n            #(\"resp_a_stats\", resp_a_stats),\n            #(\"resp_b_stats\", resp_b_stats),\n            (\"raw_sim\", raw_sim),\n            (\"mean_sim\", mean_sim),\n            (\"median_sim\", median_sim),\n            (\"tfidf_mean_sim_a\", similarities_df['tfidf_mean_sim_a'].values.reshape(-1,1)),\n            (\"tfidf_mean_sim_b\", similarities_df['tfidf_mean_sim_b'].values.reshape(-1,1)),\n            (\"tfidf_median_sim_a\", similarities_df['tfidf_median_sim_a'].values.reshape(-1,1)),\n            (\"tfidf_median_sim_b\",similarities_df['tfidf_median_sim_b'].values.reshape(-1,1)),\n           # (\"common_words\", common_words_features)\n        ]\n        \n        for name, feat in features:\n            if feat is not None:\n                print(f\"{name}: shape={feat.shape}, dtype={feat.dtype}\")\n        \n        # Convert each feature set to sparse format with consistent dtype\n        features_to_stack = []\n        for name, feat in features:\n            if feat is not None:\n                sparse_feat = ensure_sparse(feat)\n                features_to_stack.append(sparse_feat)\n                print(f\"Converted {name}: shape={sparse_feat.shape}, dtype={sparse_feat.dtype}\")\n        \n        # Print shapes before stacking\n        print(\"\\nFeature shapes before stacking:\")\n        for i, feat in enumerate(features_to_stack):\n            print(f\"Feature {i} shape: {feat.shape}\")\n        \n        # Combine all the feature matrices\n        final_features = hstack(features_to_stack)\n        X_final = pd.DataFrame(final_features.toarray())\n        print(f\"\\nFinal combined features shape: {final_features.shape}\")\n         # Save chunk to disk\n        chunk_filename = f\"{OUTPUT_FILE}.chunk_{start_idx//chunk_size}.pkl\"\n        X_final.to_pickle(chunk_filename)\n        print(f\"Saved chunk to {chunk_filename}\")\n        \n\n        # Clear memory\n        del embedded_chunk, X_final\n        gc.collect()\n        \n        # Keep track of chunk files\n        results.append(chunk_filename)\n    \n    return results\n\n# Process training data\nprint(\"Processing training data...\")\nchunk_files = process_in_chunks(X_train)\n\n# Optionally, combine all chunks\ndef combine_chunks(chunk_files):\n    print(\"\\nCombining chunks...\")\n    combined_df = pd.concat([pd.read_pickle(f) for f in chunk_files])\n    combined_df.to_pickle(OUTPUT_FILE)\n    print(f\"Saved combined data to {OUTPUT_FILE}\")\n    return combined_df\n\n# Combine if needed\n# combined_data = combine_chunks(chunk_files)  # Uncomment if you need the combined data\n\nprint(\"\\nProcessing complete!\")\n# Load training data chunks\n\n# Load and combine all chunks into a single DataFrame\ndef load_embedded_df(chunk_files):\n    print(\"Loading and combining chunks...\")\n    chunks = []\n    \n    for chunk_file in chunk_files:\n        try:\n            # Load chunk\n            chunk_df = pd.read_pickle(chunk_file)\n            chunks.append(chunk_df)\n            \n            # Clear memory after appending\n            del chunk_df\n            gc.collect()\n            \n        except Exception as e:\n            print(f\"Error loading chunk {chunk_file}: {e}\")\n    \n    # Combine all chunks\n    print(\"Concatenating all chunks...\")\n    embedded_df = pd.concat(chunks, axis=0, ignore_index=True)\n    print(f\"Final DataFrame shape: {embedded_df.shape}\")\n    \n    # Clear temporary lists\n    del chunks\n    gc.collect()\n    \n    return embedded_df\n\n# Load training data chunks\nembedded_df = load_embedded_df([f\"embedded_features.pkl.chunk_{i}.pkl\" \n                              for i in range((len(X_train) + CHUNK_SIZE - 1) // CHUNK_SIZE)])\n\nprint(\"\\nEmbedded DataFrame columns:\", embedded_df.columns.tolist())\n#print(\"First row prompt embedding keys:\", embedded_df[\"prompt_embed\"].iloc[0].keys())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:00:14.294425Z","iopub.execute_input":"2025-09-20T07:00:14.294791Z","iopub.status.idle":"2025-09-20T10:32:30.459246Z","shell.execute_reply.started":"2025-09-20T07:00:14.294766Z","shell.execute_reply":"2025-09-20T10:32:30.458495Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\nStep 1: Generating embeddings...\nProcessing training data...\n\nProcessing chunk 1/60\nGenerating embeddings...\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"[TFIDF Warning] Row 398 has zero TF-IDF weights.\n[TFIDF Warning] Row 25 has zero TF-IDF weights.\n[TFIDF Warning] Row 96 has zero TF-IDF weights.\n[TFIDF Warning] Row 179 has zero TF-IDF weights.\n[TFIDF Warning] Row 260 has zero TF-IDF weights.\n[TFIDF Warning] Row 352 has zero TF-IDF weights.\n[TFIDF Warning] Row 430 has zero TF-IDF weights.\n[TFIDF Warning] Row 510 has zero TF-IDF weights.\n[TFIDF Warning] Row 558 has zero TF-IDF weights.\n[TFIDF Warning] Row 25 has zero TF-IDF weights.\n[TFIDF Warning] Row 96 has zero TF-IDF weights.\n[TFIDF Warning] Row 179 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 260 has zero TF-IDF weights.\n[TFIDF Warning] Row 323 has zero TF-IDF weights.\n[TFIDF Warning] Row 352 has zero TF-IDF weights.\n[TFIDF Warning] Row 393 has zero TF-IDF weights.\n[TFIDF Warning] Row 630 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_0.pkl\n\nProcessing chunk 2/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 72 has zero TF-IDF weights.\n[TFIDF Warning] Row 118 has zero TF-IDF weights.\n[TFIDF Warning] Row 211 has zero TF-IDF weights.\n[TFIDF Warning] Row 516 has zero TF-IDF weights.\n[TFIDF Warning] Row 562 has zero TF-IDF weights.\n[TFIDF Warning] Row 706 has zero TF-IDF weights.\n[TFIDF Warning] Row 737 has zero TF-IDF weights.\n[TFIDF Warning] Row 750 has zero TF-IDF weights.\n[TFIDF Warning] Row 374 has zero TF-IDF weights.\n[TFIDF Warning] Row 573 has zero TF-IDF weights.\n[TFIDF Warning] Row 584 has zero TF-IDF weights.\n[TFIDF Warning] Row 65 has zero TF-IDF weights.\n[TFIDF Warning] Row 374 has zero TF-IDF weights.\n[TFIDF Warning] Row 573 has zero TF-IDF weights.\n[TFIDF Warning] Row 584 has zero TF-IDF weights.\n[TFIDF Warning] Row 670 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_1.pkl\n\nProcessing chunk 3/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 701 has zero TF-IDF weights.\n[TFIDF Warning] Row 13 has zero TF-IDF weights.\n[TFIDF Warning] Row 692 has zero TF-IDF weights.\n[TFIDF Warning] Row 53 has zero TF-IDF weights.\n[TFIDF Warning] Row 83 has zero TF-IDF weights.\n[TFIDF Warning] Row 100 has zero TF-IDF weights.\n[TFIDF Warning] Row 217 has zero TF-IDF weights.\n[TFIDF Warning] Row 250 has zero TF-IDF weights.\n[TFIDF Warning] Row 527 has zero TF-IDF weights.\n[TFIDF Warning] Row 538 has zero TF-IDF weights.\n[TFIDF Warning] Row 753 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_2.pkl\n\nProcessing chunk 4/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 542 has zero TF-IDF weights.\n[TFIDF Warning] Row 698 has zero TF-IDF weights.\n[TFIDF Warning] Row 126 has zero TF-IDF weights.\n[TFIDF Warning] Row 157 has zero TF-IDF weights.\n[TFIDF Warning] Row 253 has zero TF-IDF weights.\n[TFIDF Warning] Row 416 has zero TF-IDF weights.\n[TFIDF Warning] Row 539 has zero TF-IDF weights.\n[TFIDF Warning] Row 691 has zero TF-IDF weights.\n[TFIDF Warning] Row 696 has zero TF-IDF weights.\n[TFIDF Warning] Row 728 has zero TF-IDF weights.\n[TFIDF Warning] Row 741 has zero TF-IDF weights.\n[TFIDF Warning] Row 127 has zero TF-IDF weights.\n[TFIDF Warning] Row 224 has zero TF-IDF weights.\n[TFIDF Warning] Row 248 has zero TF-IDF weights.\n[TFIDF Warning] Row 348 has zero TF-IDF weights.\n[TFIDF Warning] Row 373 has zero TF-IDF weights.\n[TFIDF Warning] Row 451 has zero TF-IDF weights.\n[TFIDF Warning] Row 516 has zero TF-IDF weights.\n[TFIDF Warning] Row 539 has zero TF-IDF weights.\n[TFIDF Warning] Row 617 has zero TF-IDF weights.\n[TFIDF Warning] Row 621 has zero TF-IDF weights.\n[TFIDF Warning] Row 691 has zero TF-IDF weights.\n[TFIDF Warning] Row 696 has zero TF-IDF weights.\n[TFIDF Warning] Row 728 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_3.pkl\n\nProcessing chunk 5/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 252 has zero TF-IDF weights.\n[TFIDF Warning] Row 277 has zero TF-IDF weights.\n[TFIDF Warning] Row 29 has zero TF-IDF weights.\n[TFIDF Warning] Row 259 has zero TF-IDF weights.\n[TFIDF Warning] Row 489 has zero TF-IDF weights.\n[TFIDF Warning] Row 556 has zero TF-IDF weights.\n[TFIDF Warning] Row 29 has zero TF-IDF weights.\n[TFIDF Warning] Row 117 has zero TF-IDF weights.\n[TFIDF Warning] Row 224 has zero TF-IDF weights.\n[TFIDF Warning] Row 489 has zero TF-IDF weights.\n[TFIDF Warning] Row 743 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_4.pkl\n\nProcessing chunk 6/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 187 has zero TF-IDF weights.\n[TFIDF Warning] Row 222 has zero TF-IDF weights.\n[TFIDF Warning] Row 354 has zero TF-IDF weights.\n[TFIDF Warning] Row 378 has zero TF-IDF weights.\n[TFIDF Warning] Row 662 has zero TF-IDF weights.\n[TFIDF Warning] Row 745 has zero TF-IDF weights.\n[TFIDF Warning] Row 164 has zero TF-IDF weights.\n[TFIDF Warning] Row 203 has zero TF-IDF weights.\n[TFIDF Warning] Row 354 has zero TF-IDF weights.\n[TFIDF Warning] Row 483 has zero TF-IDF weights.\n[TFIDF Warning] Row 662 has zero TF-IDF weights.\n[TFIDF Warning] Row 688 has zero TF-IDF weights.\n[TFIDF Warning] Row 745 has zero TF-IDF weights.\n[TFIDF Warning] Row 751 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_5.pkl\n\nProcessing chunk 7/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 28 has zero TF-IDF weights.\n[TFIDF Warning] Row 53 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 278 has zero TF-IDF weights.\n[TFIDF Warning] Row 341 has zero TF-IDF weights.\n[TFIDF Warning] Row 424 has zero TF-IDF weights.\n[TFIDF Warning] Row 577 has zero TF-IDF weights.\n[TFIDF Warning] Row 621 has zero TF-IDF weights.\n[TFIDF Warning] Row 694 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 209 has zero TF-IDF weights.\n[TFIDF Warning] Row 341 has zero TF-IDF weights.\n[TFIDF Warning] Row 349 has zero TF-IDF weights.\n[TFIDF Warning] Row 473 has zero TF-IDF weights.\n[TFIDF Warning] Row 478 has zero TF-IDF weights.\n[TFIDF Warning] Row 621 has zero TF-IDF weights.\n[TFIDF Warning] Row 685 has zero TF-IDF weights.\n[TFIDF Warning] Row 751 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_6.pkl\n\nProcessing chunk 8/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 78 has zero TF-IDF weights.\n[TFIDF Warning] Row 78 has zero TF-IDF weights.\n[TFIDF Warning] Row 125 has zero TF-IDF weights.\n[TFIDF Warning] Row 151 has zero TF-IDF weights.\n[TFIDF Warning] Row 241 has zero TF-IDF weights.\n[TFIDF Warning] Row 243 has zero TF-IDF weights.\n[TFIDF Warning] Row 467 has zero TF-IDF weights.\n[TFIDF Warning] Row 656 has zero TF-IDF weights.\n[TFIDF Warning] Row 677 has zero TF-IDF weights.\n[TFIDF Warning] Row 122 has zero TF-IDF weights.\n[TFIDF Warning] Row 134 has zero TF-IDF weights.\n[TFIDF Warning] Row 212 has zero TF-IDF weights.\n[TFIDF Warning] Row 339 has zero TF-IDF weights.\n[TFIDF Warning] Row 656 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_7.pkl\n\nProcessing chunk 9/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 422 has zero TF-IDF weights.\n[TFIDF Warning] Row 528 has zero TF-IDF weights.\n[TFIDF Warning] Row 671 has zero TF-IDF weights.\n[TFIDF Warning] Row 733 has zero TF-IDF weights.\n[TFIDF Warning] Row 751 has zero TF-IDF weights.\n[TFIDF Warning] Row 29 has zero TF-IDF weights.\n[TFIDF Warning] Row 32 has zero TF-IDF weights.\n[TFIDF Warning] Row 156 has zero TF-IDF weights.\n[TFIDF Warning] Row 267 has zero TF-IDF weights.\n[TFIDF Warning] Row 290 has zero TF-IDF weights.\n[TFIDF Warning] Row 301 has zero TF-IDF weights.\n[TFIDF Warning] Row 207 has zero TF-IDF weights.\n[TFIDF Warning] Row 235 has zero TF-IDF weights.\n[TFIDF Warning] Row 301 has zero TF-IDF weights.\n[TFIDF Warning] Row 503 has zero TF-IDF weights.\n[TFIDF Warning] Row 577 has zero TF-IDF weights.\n[TFIDF Warning] Row 733 has zero TF-IDF weights.\n[TFIDF Warning] Row 751 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_8.pkl\n\nProcessing chunk 10/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 84 has zero TF-IDF weights.\n[TFIDF Warning] Row 7 has zero TF-IDF weights.\n[TFIDF Warning] Row 215 has zero TF-IDF weights.\n[TFIDF Warning] Row 263 has zero TF-IDF weights.\n[TFIDF Warning] Row 586 has zero TF-IDF weights.\n[TFIDF Warning] Row 595 has zero TF-IDF weights.\n[TFIDF Warning] Row 215 has zero TF-IDF weights.\n[TFIDF Warning] Row 263 has zero TF-IDF weights.\n[TFIDF Warning] Row 448 has zero TF-IDF weights.\n[TFIDF Warning] Row 452 has zero TF-IDF weights.\n[TFIDF Warning] Row 678 has zero TF-IDF weights.\n[TFIDF Warning] Row 734 has zero TF-IDF weights.\n[TFIDF Warning] Row 763 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_9.pkl\n\nProcessing chunk 11/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 138 has zero TF-IDF weights.\n[TFIDF Warning] Row 367 has zero TF-IDF weights.\n[TFIDF Warning] Row 525 has zero TF-IDF weights.\n[TFIDF Warning] Row 79 has zero TF-IDF weights.\n[TFIDF Warning] Row 102 has zero TF-IDF weights.\n[TFIDF Warning] Row 119 has zero TF-IDF weights.\n[TFIDF Warning] Row 137 has zero TF-IDF weights.\n[TFIDF Warning] Row 306 has zero TF-IDF weights.\n[TFIDF Warning] Row 527 has zero TF-IDF weights.\n[TFIDF Warning] Row 634 has zero TF-IDF weights.\n[TFIDF Warning] Row 132 has zero TF-IDF weights.\n[TFIDF Warning] Row 277 has zero TF-IDF weights.\n[TFIDF Warning] Row 306 has zero TF-IDF weights.\n[TFIDF Warning] Row 527 has zero TF-IDF weights.\n[TFIDF Warning] Row 634 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_10.pkl\n\nProcessing chunk 12/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 383 has zero TF-IDF weights.\n[TFIDF Warning] Row 483 has zero TF-IDF weights.\n[TFIDF Warning] Row 649 has zero TF-IDF weights.\n[TFIDF Warning] Row 8 has zero TF-IDF weights.\n[TFIDF Warning] Row 173 has zero TF-IDF weights.\n[TFIDF Warning] Row 431 has zero TF-IDF weights.\n[TFIDF Warning] Row 475 has zero TF-IDF weights.\n[TFIDF Warning] Row 512 has zero TF-IDF weights.\n[TFIDF Warning] Row 522 has zero TF-IDF weights.\n[TFIDF Warning] Row 593 has zero TF-IDF weights.\n[TFIDF Warning] Row 663 has zero TF-IDF weights.\n[TFIDF Warning] Row 688 has zero TF-IDF weights.\n[TFIDF Warning] Row 8 has zero TF-IDF weights.\n[TFIDF Warning] Row 77 has zero TF-IDF weights.\n[TFIDF Warning] Row 89 has zero TF-IDF weights.\n[TFIDF Warning] Row 232 has zero TF-IDF weights.\n[TFIDF Warning] Row 383 has zero TF-IDF weights.\n[TFIDF Warning] Row 431 has zero TF-IDF weights.\n[TFIDF Warning] Row 522 has zero TF-IDF weights.\n[TFIDF Warning] Row 523 has zero TF-IDF weights.\n[TFIDF Warning] Row 593 has zero TF-IDF weights.\n[TFIDF Warning] Row 628 has zero TF-IDF weights.\n[TFIDF Warning] Row 678 has zero TF-IDF weights.\n[TFIDF Warning] Row 688 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_11.pkl\n\nProcessing chunk 13/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 424 has zero TF-IDF weights.\n[TFIDF Warning] Row 469 has zero TF-IDF weights.\n[TFIDF Warning] Row 666 has zero TF-IDF weights.\n[TFIDF Warning] Row 47 has zero TF-IDF weights.\n[TFIDF Warning] Row 50 has zero TF-IDF weights.\n[TFIDF Warning] Row 99 has zero TF-IDF weights.\n[TFIDF Warning] Row 183 has zero TF-IDF weights.\n[TFIDF Warning] Row 236 has zero TF-IDF weights.\n[TFIDF Warning] Row 244 has zero TF-IDF weights.\n[TFIDF Warning] Row 356 has zero TF-IDF weights.\n[TFIDF Warning] Row 386 has zero TF-IDF weights.\n[TFIDF Warning] Row 579 has zero TF-IDF weights.\n[TFIDF Warning] Row 627 has zero TF-IDF weights.\n[TFIDF Warning] Row 726 has zero TF-IDF weights.\n[TFIDF Warning] Row 733 has zero TF-IDF weights.\n[TFIDF Warning] Row 47 has zero TF-IDF weights.\n[TFIDF Warning] Row 99 has zero TF-IDF weights.\n[TFIDF Warning] Row 164 has zero TF-IDF weights.\n[TFIDF Warning] Row 244 has zero TF-IDF weights.\n[TFIDF Warning] Row 313 has zero TF-IDF weights.\n[TFIDF Warning] Row 356 has zero TF-IDF weights.\n[TFIDF Warning] Row 390 has zero TF-IDF weights.\n[TFIDF Warning] Row 515 has zero TF-IDF weights.\n[TFIDF Warning] Row 563 has zero TF-IDF weights.\n[TFIDF Warning] Row 579 has zero TF-IDF weights.\n[TFIDF Warning] Row 581 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_12.pkl\n\nProcessing chunk 14/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 107 has zero TF-IDF weights.\n[TFIDF Warning] Row 41 has zero TF-IDF weights.\n[TFIDF Warning] Row 485 has zero TF-IDF weights.\n[TFIDF Warning] Row 505 has zero TF-IDF weights.\n[TFIDF Warning] Row 537 has zero TF-IDF weights.\n[TFIDF Warning] Row 643 has zero TF-IDF weights.\n[TFIDF Warning] Row 350 has zero TF-IDF weights.\n[TFIDF Warning] Row 473 has zero TF-IDF weights.\n[TFIDF Warning] Row 485 has zero TF-IDF weights.\n[TFIDF Warning] Row 619 has zero TF-IDF weights.\n[TFIDF Warning] Row 686 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_13.pkl\n\nProcessing chunk 15/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 431 has zero TF-IDF weights.\n[TFIDF Warning] Row 557 has zero TF-IDF weights.\n[TFIDF Warning] Row 210 has zero TF-IDF weights.\n[TFIDF Warning] Row 242 has zero TF-IDF weights.\n[TFIDF Warning] Row 274 has zero TF-IDF weights.\n[TFIDF Warning] Row 464 has zero TF-IDF weights.\n[TFIDF Warning] Row 488 has zero TF-IDF weights.\n[TFIDF Warning] Row 494 has zero TF-IDF weights.\n[TFIDF Warning] Row 554 has zero TF-IDF weights.\n[TFIDF Warning] Row 582 has zero TF-IDF weights.\n[TFIDF Warning] Row 719 has zero TF-IDF weights.\n[TFIDF Warning] Row 45 has zero TF-IDF weights.\n[TFIDF Warning] Row 52 has zero TF-IDF weights.\n[TFIDF Warning] Row 55 has zero TF-IDF weights.\n[TFIDF Warning] Row 108 has zero TF-IDF weights.\n[TFIDF Warning] Row 181 has zero TF-IDF weights.\n[TFIDF Warning] Row 199 has zero TF-IDF weights.\n[TFIDF Warning] Row 236 has zero TF-IDF weights.\n[TFIDF Warning] Row 320 has zero TF-IDF weights.\n[TFIDF Warning] Row 351 has zero TF-IDF weights.\n[TFIDF Warning] Row 520 has zero TF-IDF weights.\n[TFIDF Warning] Row 743 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_14.pkl\n\nProcessing chunk 16/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 48 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 248 has zero TF-IDF weights.\n[TFIDF Warning] Row 280 has zero TF-IDF weights.\n[TFIDF Warning] Row 320 has zero TF-IDF weights.\n[TFIDF Warning] Row 394 has zero TF-IDF weights.\n[TFIDF Warning] Row 378 has zero TF-IDF weights.\n[TFIDF Warning] Row 394 has zero TF-IDF weights.\n[TFIDF Warning] Row 410 has zero TF-IDF weights.\n[TFIDF Warning] Row 509 has zero TF-IDF weights.\n[TFIDF Warning] Row 576 has zero TF-IDF weights.\n[TFIDF Warning] Row 603 has zero TF-IDF weights.\n[TFIDF Warning] Row 616 has zero TF-IDF weights.\n[TFIDF Warning] Row 617 has zero TF-IDF weights.\n[TFIDF Warning] Row 247 has zero TF-IDF weights.\n[TFIDF Warning] Row 394 has zero TF-IDF weights.\n[TFIDF Warning] Row 509 has zero TF-IDF weights.\n[TFIDF Warning] Row 603 has zero TF-IDF weights.\n[TFIDF Warning] Row 702 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_15.pkl\n\nProcessing chunk 17/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 561 has zero TF-IDF weights.\n[TFIDF Warning] Row 648 has zero TF-IDF weights.\n[TFIDF Warning] Row 15 has zero TF-IDF weights.\n[TFIDF Warning] Row 39 has zero TF-IDF weights.\n[TFIDF Warning] Row 224 has zero TF-IDF weights.\n[TFIDF Warning] Row 436 has zero TF-IDF weights.\n[TFIDF Warning] Row 450 has zero TF-IDF weights.\n[TFIDF Warning] Row 558 has zero TF-IDF weights.\n[TFIDF Warning] Row 626 has zero TF-IDF weights.\n[TFIDF Warning] Row 167 has zero TF-IDF weights.\n[TFIDF Warning] Row 176 has zero TF-IDF weights.\n[TFIDF Warning] Row 196 has zero TF-IDF weights.\n[TFIDF Warning] Row 224 has zero TF-IDF weights.\n[TFIDF Warning] Row 392 has zero TF-IDF weights.\n[TFIDF Warning] Row 618 has zero TF-IDF weights.\n[TFIDF Warning] Row 626 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_16.pkl\n\nProcessing chunk 18/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 321 has zero TF-IDF weights.\n[TFIDF Warning] Row 554 has zero TF-IDF weights.\n[TFIDF Warning] Row 690 has zero TF-IDF weights.\n[TFIDF Warning] Row 4 has zero TF-IDF weights.\n[TFIDF Warning] Row 50 has zero TF-IDF weights.\n[TFIDF Warning] Row 72 has zero TF-IDF weights.\n[TFIDF Warning] Row 272 has zero TF-IDF weights.\n[TFIDF Warning] Row 285 has zero TF-IDF weights.\n[TFIDF Warning] Row 303 has zero TF-IDF weights.\n[TFIDF Warning] Row 385 has zero TF-IDF weights.\n[TFIDF Warning] Row 390 has zero TF-IDF weights.\n[TFIDF Warning] Row 508 has zero TF-IDF weights.\n[TFIDF Warning] Row 630 has zero TF-IDF weights.\n[TFIDF Warning] Row 757 has zero TF-IDF weights.\n[TFIDF Warning] Row 4 has zero TF-IDF weights.\n[TFIDF Warning] Row 50 has zero TF-IDF weights.\n[TFIDF Warning] Row 72 has zero TF-IDF weights.\n[TFIDF Warning] Row 110 has zero TF-IDF weights.\n[TFIDF Warning] Row 244 has zero TF-IDF weights.\n[TFIDF Warning] Row 303 has zero TF-IDF weights.\n[TFIDF Warning] Row 379 has zero TF-IDF weights.\n[TFIDF Warning] Row 385 has zero TF-IDF weights.\n[TFIDF Warning] Row 405 has zero TF-IDF weights.\n[TFIDF Warning] Row 462 has zero TF-IDF weights.\n[TFIDF Warning] Row 508 has zero TF-IDF weights.\n[TFIDF Warning] Row 757 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_17.pkl\n\nProcessing chunk 19/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 404 has zero TF-IDF weights.\n[TFIDF Warning] Row 477 has zero TF-IDF weights.\n[TFIDF Warning] Row 497 has zero TF-IDF weights.\n[TFIDF Warning] Row 25 has zero TF-IDF weights.\n[TFIDF Warning] Row 239 has zero TF-IDF weights.\n[TFIDF Warning] Row 562 has zero TF-IDF weights.\n[TFIDF Warning] Row 599 has zero TF-IDF weights.\n[TFIDF Warning] Row 600 has zero TF-IDF weights.\n[TFIDF Warning] Row 614 has zero TF-IDF weights.\n[TFIDF Warning] Row 626 has zero TF-IDF weights.\n[TFIDF Warning] Row 697 has zero TF-IDF weights.\n[TFIDF Warning] Row 706 has zero TF-IDF weights.\n[TFIDF Warning] Row 25 has zero TF-IDF weights.\n[TFIDF Warning] Row 99 has zero TF-IDF weights.\n[TFIDF Warning] Row 217 has zero TF-IDF weights.\n[TFIDF Warning] Row 335 has zero TF-IDF weights.\n[TFIDF Warning] Row 369 has zero TF-IDF weights.\n[TFIDF Warning] Row 548 has zero TF-IDF weights.\n[TFIDF Warning] Row 600 has zero TF-IDF weights.\n[TFIDF Warning] Row 614 has zero TF-IDF weights.\n[TFIDF Warning] Row 617 has zero TF-IDF weights.\n[TFIDF Warning] Row 661 has zero TF-IDF weights.\n[TFIDF Warning] Row 706 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_18.pkl\n\nProcessing chunk 20/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 75 has zero TF-IDF weights.\n[TFIDF Warning] Row 163 has zero TF-IDF weights.\n[TFIDF Warning] Row 204 has zero TF-IDF weights.\n[TFIDF Warning] Row 411 has zero TF-IDF weights.\n[TFIDF Warning] Row 667 has zero TF-IDF weights.\n[TFIDF Warning] Row 7 has zero TF-IDF weights.\n[TFIDF Warning] Row 90 has zero TF-IDF weights.\n[TFIDF Warning] Row 147 has zero TF-IDF weights.\n[TFIDF Warning] Row 211 has zero TF-IDF weights.\n[TFIDF Warning] Row 238 has zero TF-IDF weights.\n[TFIDF Warning] Row 244 has zero TF-IDF weights.\n[TFIDF Warning] Row 330 has zero TF-IDF weights.\n[TFIDF Warning] Row 340 has zero TF-IDF weights.\n[TFIDF Warning] Row 351 has zero TF-IDF weights.\n[TFIDF Warning] Row 419 has zero TF-IDF weights.\n[TFIDF Warning] Row 643 has zero TF-IDF weights.\n[TFIDF Warning] Row 653 has zero TF-IDF weights.\n[TFIDF Warning] Row 707 has zero TF-IDF weights.\n[TFIDF Warning] Row 115 has zero TF-IDF weights.\n[TFIDF Warning] Row 121 has zero TF-IDF weights.\n[TFIDF Warning] Row 238 has zero TF-IDF weights.\n[TFIDF Warning] Row 330 has zero TF-IDF weights.\n[TFIDF Warning] Row 406 has zero TF-IDF weights.\n[TFIDF Warning] Row 501 has zero TF-IDF weights.\n[TFIDF Warning] Row 608 has zero TF-IDF weights.\n[TFIDF Warning] Row 643 has zero TF-IDF weights.\n[TFIDF Warning] Row 653 has zero TF-IDF weights.\n[TFIDF Warning] Row 707 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_19.pkl\n\nProcessing chunk 21/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 107 has zero TF-IDF weights.\n[TFIDF Warning] Row 730 has zero TF-IDF weights.\n[TFIDF Warning] Row 30 has zero TF-IDF weights.\n[TFIDF Warning] Row 67 has zero TF-IDF weights.\n[TFIDF Warning] Row 84 has zero TF-IDF weights.\n[TFIDF Warning] Row 226 has zero TF-IDF weights.\n[TFIDF Warning] Row 559 has zero TF-IDF weights.\n[TFIDF Warning] Row 652 has zero TF-IDF weights.\n[TFIDF Warning] Row 30 has zero TF-IDF weights.\n[TFIDF Warning] Row 84 has zero TF-IDF weights.\n[TFIDF Warning] Row 148 has zero TF-IDF weights.\n[TFIDF Warning] Row 226 has zero TF-IDF weights.\n[TFIDF Warning] Row 442 has zero TF-IDF weights.\n[TFIDF Warning] Row 447 has zero TF-IDF weights.\n[TFIDF Warning] Row 483 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_20.pkl\n\nProcessing chunk 22/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 447 has zero TF-IDF weights.\n[TFIDF Warning] Row 186 has zero TF-IDF weights.\n[TFIDF Warning] Row 423 has zero TF-IDF weights.\n[TFIDF Warning] Row 522 has zero TF-IDF weights.\n[TFIDF Warning] Row 647 has zero TF-IDF weights.\n[TFIDF Warning] Row 681 has zero TF-IDF weights.\n[TFIDF Warning] Row 759 has zero TF-IDF weights.\n[TFIDF Warning] Row 764 has zero TF-IDF weights.\n[TFIDF Warning] Row 48 has zero TF-IDF weights.\n[TFIDF Warning] Row 97 has zero TF-IDF weights.\n[TFIDF Warning] Row 291 has zero TF-IDF weights.\n[TFIDF Warning] Row 423 has zero TF-IDF weights.\n[TFIDF Warning] Row 428 has zero TF-IDF weights.\n[TFIDF Warning] Row 631 has zero TF-IDF weights.\n[TFIDF Warning] Row 647 has zero TF-IDF weights.\n[TFIDF Warning] Row 661 has zero TF-IDF weights.\n[TFIDF Warning] Row 702 has zero TF-IDF weights.\n[TFIDF Warning] Row 719 has zero TF-IDF weights.\n[TFIDF Warning] Row 748 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_21.pkl\n\nProcessing chunk 23/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 355 has zero TF-IDF weights.\n[TFIDF Warning] Row 668 has zero TF-IDF weights.\n[TFIDF Warning] Row 9 has zero TF-IDF weights.\n[TFIDF Warning] Row 339 has zero TF-IDF weights.\n[TFIDF Warning] Row 411 has zero TF-IDF weights.\n[TFIDF Warning] Row 466 has zero TF-IDF weights.\n[TFIDF Warning] Row 534 has zero TF-IDF weights.\n[TFIDF Warning] Row 594 has zero TF-IDF weights.\n[TFIDF Warning] Row 639 has zero TF-IDF weights.\n[TFIDF Warning] Row 697 has zero TF-IDF weights.\n[TFIDF Warning] Row 142 has zero TF-IDF weights.\n[TFIDF Warning] Row 255 has zero TF-IDF weights.\n[TFIDF Warning] Row 339 has zero TF-IDF weights.\n[TFIDF Warning] Row 482 has zero TF-IDF weights.\n[TFIDF Warning] Row 538 has zero TF-IDF weights.\n[TFIDF Warning] Row 635 has zero TF-IDF weights.\n[TFIDF Warning] Row 668 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_22.pkl\n\nProcessing chunk 24/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 260 has zero TF-IDF weights.\n[TFIDF Warning] Row 493 has zero TF-IDF weights.\n[TFIDF Warning] Row 129 has zero TF-IDF weights.\n[TFIDF Warning] Row 219 has zero TF-IDF weights.\n[TFIDF Warning] Row 316 has zero TF-IDF weights.\n[TFIDF Warning] Row 407 has zero TF-IDF weights.\n[TFIDF Warning] Row 417 has zero TF-IDF weights.\n[TFIDF Warning] Row 634 has zero TF-IDF weights.\n[TFIDF Warning] Row 682 has zero TF-IDF weights.\n[TFIDF Warning] Row 128 has zero TF-IDF weights.\n[TFIDF Warning] Row 209 has zero TF-IDF weights.\n[TFIDF Warning] Row 250 has zero TF-IDF weights.\n[TFIDF Warning] Row 383 has zero TF-IDF weights.\n[TFIDF Warning] Row 407 has zero TF-IDF weights.\n[TFIDF Warning] Row 498 has zero TF-IDF weights.\n[TFIDF Warning] Row 537 has zero TF-IDF weights.\n[TFIDF Warning] Row 598 has zero TF-IDF weights.\n[TFIDF Warning] Row 632 has zero TF-IDF weights.\n[TFIDF Warning] Row 634 has zero TF-IDF weights.\n[TFIDF Warning] Row 670 has zero TF-IDF weights.\n[TFIDF Warning] Row 694 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_23.pkl\n\nProcessing chunk 25/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 142 has zero TF-IDF weights.\n[TFIDF Warning] Row 232 has zero TF-IDF weights.\n[TFIDF Warning] Row 299 has zero TF-IDF weights.\n[TFIDF Warning] Row 405 has zero TF-IDF weights.\n[TFIDF Warning] Row 506 has zero TF-IDF weights.\n[TFIDF Warning] Row 76 has zero TF-IDF weights.\n[TFIDF Warning] Row 143 has zero TF-IDF weights.\n[TFIDF Warning] Row 276 has zero TF-IDF weights.\n[TFIDF Warning] Row 277 has zero TF-IDF weights.\n[TFIDF Warning] Row 329 has zero TF-IDF weights.\n[TFIDF Warning] Row 388 has zero TF-IDF weights.\n[TFIDF Warning] Row 473 has zero TF-IDF weights.\n[TFIDF Warning] Row 535 has zero TF-IDF weights.\n[TFIDF Warning] Row 598 has zero TF-IDF weights.\n[TFIDF Warning] Row 689 has zero TF-IDF weights.\n[TFIDF Warning] Row 733 has zero TF-IDF weights.\n[TFIDF Warning] Row 744 has zero TF-IDF weights.\n[TFIDF Warning] Row 71 has zero TF-IDF weights.\n[TFIDF Warning] Row 76 has zero TF-IDF weights.\n[TFIDF Warning] Row 277 has zero TF-IDF weights.\n[TFIDF Warning] Row 349 has zero TF-IDF weights.\n[TFIDF Warning] Row 473 has zero TF-IDF weights.\n[TFIDF Warning] Row 533 has zero TF-IDF weights.\n[TFIDF Warning] Row 535 has zero TF-IDF weights.\n[TFIDF Warning] Row 563 has zero TF-IDF weights.\n[TFIDF Warning] Row 598 has zero TF-IDF weights.\n[TFIDF Warning] Row 682 has zero TF-IDF weights.\n[TFIDF Warning] Row 689 has zero TF-IDF weights.\n[TFIDF Warning] Row 733 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_24.pkl\n\nProcessing chunk 26/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 246 has zero TF-IDF weights.\n[TFIDF Warning] Row 259 has zero TF-IDF weights.\n[TFIDF Warning] Row 386 has zero TF-IDF weights.\n[TFIDF Warning] Row 141 has zero TF-IDF weights.\n[TFIDF Warning] Row 298 has zero TF-IDF weights.\n[TFIDF Warning] Row 315 has zero TF-IDF weights.\n[TFIDF Warning] Row 545 has zero TF-IDF weights.\n[TFIDF Warning] Row 574 has zero TF-IDF weights.\n[TFIDF Warning] Row 141 has zero TF-IDF weights.\n[TFIDF Warning] Row 179 has zero TF-IDF weights.\n[TFIDF Warning] Row 298 has zero TF-IDF weights.\n[TFIDF Warning] Row 315 has zero TF-IDF weights.\n[TFIDF Warning] Row 356 has zero TF-IDF weights.\n[TFIDF Warning] Row 382 has zero TF-IDF weights.\n[TFIDF Warning] Row 545 has zero TF-IDF weights.\n[TFIDF Warning] Row 620 has zero TF-IDF weights.\n[TFIDF Warning] Row 724 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_25.pkl\n\nProcessing chunk 27/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 19 has zero TF-IDF weights.\n[TFIDF Warning] Row 446 has zero TF-IDF weights.\n[TFIDF Warning] Row 517 has zero TF-IDF weights.\n[TFIDF Warning] Row 528 has zero TF-IDF weights.\n[TFIDF Warning] Row 624 has zero TF-IDF weights.\n[TFIDF Warning] Row 28 has zero TF-IDF weights.\n[TFIDF Warning] Row 169 has zero TF-IDF weights.\n[TFIDF Warning] Row 363 has zero TF-IDF weights.\n[TFIDF Warning] Row 454 has zero TF-IDF weights.\n[TFIDF Warning] Row 553 has zero TF-IDF weights.\n[TFIDF Warning] Row 579 has zero TF-IDF weights.\n[TFIDF Warning] Row 28 has zero TF-IDF weights.\n[TFIDF Warning] Row 117 has zero TF-IDF weights.\n[TFIDF Warning] Row 274 has zero TF-IDF weights.\n[TFIDF Warning] Row 766 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_26.pkl\n\nProcessing chunk 28/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 38 has zero TF-IDF weights.\n[TFIDF Warning] Row 178 has zero TF-IDF weights.\n[TFIDF Warning] Row 677 has zero TF-IDF weights.\n[TFIDF Warning] Row 145 has zero TF-IDF weights.\n[TFIDF Warning] Row 192 has zero TF-IDF weights.\n[TFIDF Warning] Row 199 has zero TF-IDF weights.\n[TFIDF Warning] Row 289 has zero TF-IDF weights.\n[TFIDF Warning] Row 338 has zero TF-IDF weights.\n[TFIDF Warning] Row 374 has zero TF-IDF weights.\n[TFIDF Warning] Row 562 has zero TF-IDF weights.\n[TFIDF Warning] Row 737 has zero TF-IDF weights.\n[TFIDF Warning] Row 85 has zero TF-IDF weights.\n[TFIDF Warning] Row 145 has zero TF-IDF weights.\n[TFIDF Warning] Row 192 has zero TF-IDF weights.\n[TFIDF Warning] Row 345 has zero TF-IDF weights.\n[TFIDF Warning] Row 436 has zero TF-IDF weights.\n[TFIDF Warning] Row 562 has zero TF-IDF weights.\n[TFIDF Warning] Row 702 has zero TF-IDF weights.\n[TFIDF Warning] Row 704 has zero TF-IDF weights.\n[TFIDF Warning] Row 720 has zero TF-IDF weights.\n[TFIDF Warning] Row 741 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_27.pkl\n\nProcessing chunk 29/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 701 has zero TF-IDF weights.\n[TFIDF Warning] Row 34 has zero TF-IDF weights.\n[TFIDF Warning] Row 133 has zero TF-IDF weights.\n[TFIDF Warning] Row 248 has zero TF-IDF weights.\n[TFIDF Warning] Row 292 has zero TF-IDF weights.\n[TFIDF Warning] Row 333 has zero TF-IDF weights.\n[TFIDF Warning] Row 527 has zero TF-IDF weights.\n[TFIDF Warning] Row 531 has zero TF-IDF weights.\n[TFIDF Warning] Row 549 has zero TF-IDF weights.\n[TFIDF Warning] Row 610 has zero TF-IDF weights.\n[TFIDF Warning] Row 57 has zero TF-IDF weights.\n[TFIDF Warning] Row 68 has zero TF-IDF weights.\n[TFIDF Warning] Row 133 has zero TF-IDF weights.\n[TFIDF Warning] Row 162 has zero TF-IDF weights.\n[TFIDF Warning] Row 292 has zero TF-IDF weights.\n[TFIDF Warning] Row 324 has zero TF-IDF weights.\n[TFIDF Warning] Row 333 has zero TF-IDF weights.\n[TFIDF Warning] Row 531 has zero TF-IDF weights.\n[TFIDF Warning] Row 549 has zero TF-IDF weights.\n[TFIDF Warning] Row 584 has zero TF-IDF weights.\n[TFIDF Warning] Row 609 has zero TF-IDF weights.\n[TFIDF Warning] Row 610 has zero TF-IDF weights.\n[TFIDF Warning] Row 633 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_28.pkl\n\nProcessing chunk 30/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 318 has zero TF-IDF weights.\n[TFIDF Warning] Row 347 has zero TF-IDF weights.\n[TFIDF Warning] Row 323 has zero TF-IDF weights.\n[TFIDF Warning] Row 433 has zero TF-IDF weights.\n[TFIDF Warning] Row 537 has zero TF-IDF weights.\n[TFIDF Warning] Row 671 has zero TF-IDF weights.\n[TFIDF Warning] Row 731 has zero TF-IDF weights.\n[TFIDF Warning] Row 43 has zero TF-IDF weights.\n[TFIDF Warning] Row 115 has zero TF-IDF weights.\n[TFIDF Warning] Row 169 has zero TF-IDF weights.\n[TFIDF Warning] Row 433 has zero TF-IDF weights.\n[TFIDF Warning] Row 537 has zero TF-IDF weights.\n[TFIDF Warning] Row 687 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_29.pkl\n\nProcessing chunk 31/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 32 has zero TF-IDF weights.\n[TFIDF Warning] Row 81 has zero TF-IDF weights.\n[TFIDF Warning] Row 306 has zero TF-IDF weights.\n[TFIDF Warning] Row 709 has zero TF-IDF weights.\n[TFIDF Warning] Row 32 has zero TF-IDF weights.\n[TFIDF Warning] Row 103 has zero TF-IDF weights.\n[TFIDF Warning] Row 186 has zero TF-IDF weights.\n[TFIDF Warning] Row 233 has zero TF-IDF weights.\n[TFIDF Warning] Row 329 has zero TF-IDF weights.\n[TFIDF Warning] Row 354 has zero TF-IDF weights.\n[TFIDF Warning] Row 379 has zero TF-IDF weights.\n[TFIDF Warning] Row 517 has zero TF-IDF weights.\n[TFIDF Warning] Row 691 has zero TF-IDF weights.\n[TFIDF Warning] Row 29 has zero TF-IDF weights.\n[TFIDF Warning] Row 103 has zero TF-IDF weights.\n[TFIDF Warning] Row 264 has zero TF-IDF weights.\n[TFIDF Warning] Row 296 has zero TF-IDF weights.\n[TFIDF Warning] Row 354 has zero TF-IDF weights.\n[TFIDF Warning] Row 371 has zero TF-IDF weights.\n[TFIDF Warning] Row 497 has zero TF-IDF weights.\n[TFIDF Warning] Row 517 has zero TF-IDF weights.\n[TFIDF Warning] Row 548 has zero TF-IDF weights.\n[TFIDF Warning] Row 556 has zero TF-IDF weights.\n[TFIDF Warning] Row 691 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_30.pkl\n\nProcessing chunk 32/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 58 has zero TF-IDF weights.\n[TFIDF Warning] Row 295 has zero TF-IDF weights.\n[TFIDF Warning] Row 349 has zero TF-IDF weights.\n[TFIDF Warning] Row 441 has zero TF-IDF weights.\n[TFIDF Warning] Row 521 has zero TF-IDF weights.\n[TFIDF Warning] Row 694 has zero TF-IDF weights.\n[TFIDF Warning] Row 760 has zero TF-IDF weights.\n[TFIDF Warning] Row 18 has zero TF-IDF weights.\n[TFIDF Warning] Row 27 has zero TF-IDF weights.\n[TFIDF Warning] Row 39 has zero TF-IDF weights.\n[TFIDF Warning] Row 40 has zero TF-IDF weights.\n[TFIDF Warning] Row 117 has zero TF-IDF weights.\n[TFIDF Warning] Row 165 has zero TF-IDF weights.\n[TFIDF Warning] Row 169 has zero TF-IDF weights.\n[TFIDF Warning] Row 177 has zero TF-IDF weights.\n[TFIDF Warning] Row 239 has zero TF-IDF weights.\n[TFIDF Warning] Row 317 has zero TF-IDF weights.\n[TFIDF Warning] Row 408 has zero TF-IDF weights.\n[TFIDF Warning] Row 410 has zero TF-IDF weights.\n[TFIDF Warning] Row 435 has zero TF-IDF weights.\n[TFIDF Warning] Row 514 has zero TF-IDF weights.\n[TFIDF Warning] Row 40 has zero TF-IDF weights.\n[TFIDF Warning] Row 117 has zero TF-IDF weights.\n[TFIDF Warning] Row 165 has zero TF-IDF weights.\n[TFIDF Warning] Row 171 has zero TF-IDF weights.\n[TFIDF Warning] Row 177 has zero TF-IDF weights.\n[TFIDF Warning] Row 290 has zero TF-IDF weights.\n[TFIDF Warning] Row 410 has zero TF-IDF weights.\n[TFIDF Warning] Row 490 has zero TF-IDF weights.\n[TFIDF Warning] Row 506 has zero TF-IDF weights.\n[TFIDF Warning] Row 521 has zero TF-IDF weights.\n[TFIDF Warning] Row 678 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_31.pkl\n\nProcessing chunk 33/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 338 has zero TF-IDF weights.\n[TFIDF Warning] Row 432 has zero TF-IDF weights.\n[TFIDF Warning] Row 488 has zero TF-IDF weights.\n[TFIDF Warning] Row 754 has zero TF-IDF weights.\n[TFIDF Warning] Row 760 has zero TF-IDF weights.\n[TFIDF Warning] Row 78 has zero TF-IDF weights.\n[TFIDF Warning] Row 202 has zero TF-IDF weights.\n[TFIDF Warning] Row 251 has zero TF-IDF weights.\n[TFIDF Warning] Row 364 has zero TF-IDF weights.\n[TFIDF Warning] Row 449 has zero TF-IDF weights.\n[TFIDF Warning] Row 487 has zero TF-IDF weights.\n[TFIDF Warning] Row 712 has zero TF-IDF weights.\n[TFIDF Warning] Row 28 has zero TF-IDF weights.\n[TFIDF Warning] Row 78 has zero TF-IDF weights.\n[TFIDF Warning] Row 202 has zero TF-IDF weights.\n[TFIDF Warning] Row 327 has zero TF-IDF weights.\n[TFIDF Warning] Row 348 has zero TF-IDF weights.\n[TFIDF Warning] Row 487 has zero TF-IDF weights.\n[TFIDF Warning] Row 489 has zero TF-IDF weights.\n[TFIDF Warning] Row 739 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_32.pkl\n\nProcessing chunk 34/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 119 has zero TF-IDF weights.\n[TFIDF Warning] Row 406 has zero TF-IDF weights.\n[TFIDF Warning] Row 32 has zero TF-IDF weights.\n[TFIDF Warning] Row 109 has zero TF-IDF weights.\n[TFIDF Warning] Row 116 has zero TF-IDF weights.\n[TFIDF Warning] Row 153 has zero TF-IDF weights.\n[TFIDF Warning] Row 190 has zero TF-IDF weights.\n[TFIDF Warning] Row 195 has zero TF-IDF weights.\n[TFIDF Warning] Row 234 has zero TF-IDF weights.\n[TFIDF Warning] Row 327 has zero TF-IDF weights.\n[TFIDF Warning] Row 340 has zero TF-IDF weights.\n[TFIDF Warning] Row 425 has zero TF-IDF weights.\n[TFIDF Warning] Row 426 has zero TF-IDF weights.\n[TFIDF Warning] Row 512 has zero TF-IDF weights.\n[TFIDF Warning] Row 571 has zero TF-IDF weights.\n[TFIDF Warning] Row 675 has zero TF-IDF weights.\n[TFIDF Warning] Row 32 has zero TF-IDF weights.\n[TFIDF Warning] Row 41 has zero TF-IDF weights.\n[TFIDF Warning] Row 93 has zero TF-IDF weights.\n[TFIDF Warning] Row 103 has zero TF-IDF weights.\n[TFIDF Warning] Row 114 has zero TF-IDF weights.\n[TFIDF Warning] Row 116 has zero TF-IDF weights.\n[TFIDF Warning] Row 119 has zero TF-IDF weights.\n[TFIDF Warning] Row 153 has zero TF-IDF weights.\n[TFIDF Warning] Row 190 has zero TF-IDF weights.\n[TFIDF Warning] Row 195 has zero TF-IDF weights.\n[TFIDF Warning] Row 240 has zero TF-IDF weights.\n[TFIDF Warning] Row 422 has zero TF-IDF weights.\n[TFIDF Warning] Row 512 has zero TF-IDF weights.\n[TFIDF Warning] Row 735 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_33.pkl\n\nProcessing chunk 35/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 388 has zero TF-IDF weights.\n[TFIDF Warning] Row 666 has zero TF-IDF weights.\n[TFIDF Warning] Row 188 has zero TF-IDF weights.\n[TFIDF Warning] Row 191 has zero TF-IDF weights.\n[TFIDF Warning] Row 243 has zero TF-IDF weights.\n[TFIDF Warning] Row 252 has zero TF-IDF weights.\n[TFIDF Warning] Row 470 has zero TF-IDF weights.\n[TFIDF Warning] Row 484 has zero TF-IDF weights.\n[TFIDF Warning] Row 622 has zero TF-IDF weights.\n[TFIDF Warning] Row 690 has zero TF-IDF weights.\n[TFIDF Warning] Row 74 has zero TF-IDF weights.\n[TFIDF Warning] Row 191 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 227 has zero TF-IDF weights.\n[TFIDF Warning] Row 299 has zero TF-IDF weights.\n[TFIDF Warning] Row 470 has zero TF-IDF weights.\n[TFIDF Warning] Row 622 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_34.pkl\n\nProcessing chunk 36/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 221 has zero TF-IDF weights.\n[TFIDF Warning] Row 387 has zero TF-IDF weights.\n[TFIDF Warning] Row 67 has zero TF-IDF weights.\n[TFIDF Warning] Row 105 has zero TF-IDF weights.\n[TFIDF Warning] Row 128 has zero TF-IDF weights.\n[TFIDF Warning] Row 156 has zero TF-IDF weights.\n[TFIDF Warning] Row 204 has zero TF-IDF weights.\n[TFIDF Warning] Row 211 has zero TF-IDF weights.\n[TFIDF Warning] Row 280 has zero TF-IDF weights.\n[TFIDF Warning] Row 528 has zero TF-IDF weights.\n[TFIDF Warning] Row 549 has zero TF-IDF weights.\n[TFIDF Warning] Row 573 has zero TF-IDF weights.\n[TFIDF Warning] Row 592 has zero TF-IDF weights.\n[TFIDF Warning] Row 636 has zero TF-IDF weights.\n[TFIDF Warning] Row 646 has zero TF-IDF weights.\n[TFIDF Warning] Row 665 has zero TF-IDF weights.\n[TFIDF Warning] Row 695 has zero TF-IDF weights.\n[TFIDF Warning] Row 730 has zero TF-IDF weights.\n[TFIDF Warning] Row 739 has zero TF-IDF weights.\n[TFIDF Warning] Row 5 has zero TF-IDF weights.\n[TFIDF Warning] Row 142 has zero TF-IDF weights.\n[TFIDF Warning] Row 156 has zero TF-IDF weights.\n[TFIDF Warning] Row 204 has zero TF-IDF weights.\n[TFIDF Warning] Row 335 has zero TF-IDF weights.\n[TFIDF Warning] Row 373 has zero TF-IDF weights.\n[TFIDF Warning] Row 390 has zero TF-IDF weights.\n[TFIDF Warning] Row 507 has zero TF-IDF weights.\n[TFIDF Warning] Row 528 has zero TF-IDF weights.\n[TFIDF Warning] Row 548 has zero TF-IDF weights.\n[TFIDF Warning] Row 573 has zero TF-IDF weights.\n[TFIDF Warning] Row 636 has zero TF-IDF weights.\n[TFIDF Warning] Row 638 has zero TF-IDF weights.\n[TFIDF Warning] Row 646 has zero TF-IDF weights.\n[TFIDF Warning] Row 739 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_35.pkl\n\nProcessing chunk 37/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 585 has zero TF-IDF weights.\n[TFIDF Warning] Row 621 has zero TF-IDF weights.\n[TFIDF Warning] Row 658 has zero TF-IDF weights.\n[TFIDF Warning] Row 680 has zero TF-IDF weights.\n[TFIDF Warning] Row 45 has zero TF-IDF weights.\n[TFIDF Warning] Row 161 has zero TF-IDF weights.\n[TFIDF Warning] Row 456 has zero TF-IDF weights.\n[TFIDF Warning] Row 605 has zero TF-IDF weights.\n[TFIDF Warning] Row 658 has zero TF-IDF weights.\n[TFIDF Warning] Row 668 has zero TF-IDF weights.\n[TFIDF Warning] Row 161 has zero TF-IDF weights.\n[TFIDF Warning] Row 255 has zero TF-IDF weights.\n[TFIDF Warning] Row 257 has zero TF-IDF weights.\n[TFIDF Warning] Row 275 has zero TF-IDF weights.\n[TFIDF Warning] Row 340 has zero TF-IDF weights.\n[TFIDF Warning] Row 354 has zero TF-IDF weights.\n[TFIDF Warning] Row 428 has zero TF-IDF weights.\n[TFIDF Warning] Row 526 has zero TF-IDF weights.\n[TFIDF Warning] Row 626 has zero TF-IDF weights.\n[TFIDF Warning] Row 664 has zero TF-IDF weights.\n[TFIDF Warning] Row 668 has zero TF-IDF weights.\n[TFIDF Warning] Row 707 has zero TF-IDF weights.\n[TFIDF Warning] Row 722 has zero TF-IDF weights.\n[TFIDF Warning] Row 726 has zero TF-IDF weights.\n[TFIDF Warning] Row 735 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_36.pkl\n\nProcessing chunk 38/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 63 has zero TF-IDF weights.\n[TFIDF Warning] Row 213 has zero TF-IDF weights.\n[TFIDF Warning] Row 360 has zero TF-IDF weights.\n[TFIDF Warning] Row 624 has zero TF-IDF weights.\n[TFIDF Warning] Row 257 has zero TF-IDF weights.\n[TFIDF Warning] Row 264 has zero TF-IDF weights.\n[TFIDF Warning] Row 368 has zero TF-IDF weights.\n[TFIDF Warning] Row 418 has zero TF-IDF weights.\n[TFIDF Warning] Row 433 has zero TF-IDF weights.\n[TFIDF Warning] Row 619 has zero TF-IDF weights.\n[TFIDF Warning] Row 624 has zero TF-IDF weights.\n[TFIDF Warning] Row 167 has zero TF-IDF weights.\n[TFIDF Warning] Row 257 has zero TF-IDF weights.\n[TFIDF Warning] Row 320 has zero TF-IDF weights.\n[TFIDF Warning] Row 528 has zero TF-IDF weights.\n[TFIDF Warning] Row 535 has zero TF-IDF weights.\n[TFIDF Warning] Row 618 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_37.pkl\n\nProcessing chunk 39/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 107 has zero TF-IDF weights.\n[TFIDF Warning] Row 17 has zero TF-IDF weights.\n[TFIDF Warning] Row 101 has zero TF-IDF weights.\n[TFIDF Warning] Row 386 has zero TF-IDF weights.\n[TFIDF Warning] Row 435 has zero TF-IDF weights.\n[TFIDF Warning] Row 502 has zero TF-IDF weights.\n[TFIDF Warning] Row 29 has zero TF-IDF weights.\n[TFIDF Warning] Row 93 has zero TF-IDF weights.\n[TFIDF Warning] Row 167 has zero TF-IDF weights.\n[TFIDF Warning] Row 383 has zero TF-IDF weights.\n[TFIDF Warning] Row 394 has zero TF-IDF weights.\n[TFIDF Warning] Row 667 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_38.pkl\n\nProcessing chunk 40/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 501 has zero TF-IDF weights.\n[TFIDF Warning] Row 656 has zero TF-IDF weights.\n[TFIDF Warning] Row 17 has zero TF-IDF weights.\n[TFIDF Warning] Row 46 has zero TF-IDF weights.\n[TFIDF Warning] Row 54 has zero TF-IDF weights.\n[TFIDF Warning] Row 71 has zero TF-IDF weights.\n[TFIDF Warning] Row 187 has zero TF-IDF weights.\n[TFIDF Warning] Row 286 has zero TF-IDF weights.\n[TFIDF Warning] Row 398 has zero TF-IDF weights.\n[TFIDF Warning] Row 482 has zero TF-IDF weights.\n[TFIDF Warning] Row 544 has zero TF-IDF weights.\n[TFIDF Warning] Row 553 has zero TF-IDF weights.\n[TFIDF Warning] Row 571 has zero TF-IDF weights.\n[TFIDF Warning] Row 579 has zero TF-IDF weights.\n[TFIDF Warning] Row 619 has zero TF-IDF weights.\n[TFIDF Warning] Row 656 has zero TF-IDF weights.\n[TFIDF Warning] Row 688 has zero TF-IDF weights.\n[TFIDF Warning] Row 710 has zero TF-IDF weights.\n[TFIDF Warning] Row 46 has zero TF-IDF weights.\n[TFIDF Warning] Row 54 has zero TF-IDF weights.\n[TFIDF Warning] Row 65 has zero TF-IDF weights.\n[TFIDF Warning] Row 265 has zero TF-IDF weights.\n[TFIDF Warning] Row 286 has zero TF-IDF weights.\n[TFIDF Warning] Row 330 has zero TF-IDF weights.\n[TFIDF Warning] Row 482 has zero TF-IDF weights.\n[TFIDF Warning] Row 580 has zero TF-IDF weights.\n[TFIDF Warning] Row 615 has zero TF-IDF weights.\n[TFIDF Warning] Row 619 has zero TF-IDF weights.\n[TFIDF Warning] Row 710 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_39.pkl\n\nProcessing chunk 41/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 131 has zero TF-IDF weights.\n[TFIDF Warning] Row 566 has zero TF-IDF weights.\n[TFIDF Warning] Row 90 has zero TF-IDF weights.\n[TFIDF Warning] Row 441 has zero TF-IDF weights.\n[TFIDF Warning] Row 524 has zero TF-IDF weights.\n[TFIDF Warning] Row 579 has zero TF-IDF weights.\n[TFIDF Warning] Row 654 has zero TF-IDF weights.\n[TFIDF Warning] Row 672 has zero TF-IDF weights.\n[TFIDF Warning] Row 681 has zero TF-IDF weights.\n[TFIDF Warning] Row 710 has zero TF-IDF weights.\n[TFIDF Warning] Row 716 has zero TF-IDF weights.\n[TFIDF Warning] Row 736 has zero TF-IDF weights.\n[TFIDF Warning] Row 750 has zero TF-IDF weights.\n[TFIDF Warning] Row 765 has zero TF-IDF weights.\n[TFIDF Warning] Row 535 has zero TF-IDF weights.\n[TFIDF Warning] Row 563 has zero TF-IDF weights.\n[TFIDF Warning] Row 672 has zero TF-IDF weights.\n[TFIDF Warning] Row 744 has zero TF-IDF weights.\n[TFIDF Warning] Row 765 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_40.pkl\n\nProcessing chunk 42/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 150 has zero TF-IDF weights.\n[TFIDF Warning] Row 173 has zero TF-IDF weights.\n[TFIDF Warning] Row 756 has zero TF-IDF weights.\n[TFIDF Warning] Row 151 has zero TF-IDF weights.\n[TFIDF Warning] Row 178 has zero TF-IDF weights.\n[TFIDF Warning] Row 235 has zero TF-IDF weights.\n[TFIDF Warning] Row 251 has zero TF-IDF weights.\n[TFIDF Warning] Row 282 has zero TF-IDF weights.\n[TFIDF Warning] Row 317 has zero TF-IDF weights.\n[TFIDF Warning] Row 343 has zero TF-IDF weights.\n[TFIDF Warning] Row 436 has zero TF-IDF weights.\n[TFIDF Warning] Row 501 has zero TF-IDF weights.\n[TFIDF Warning] Row 651 has zero TF-IDF weights.\n[TFIDF Warning] Row 694 has zero TF-IDF weights.\n[TFIDF Warning] Row 744 has zero TF-IDF weights.\n[TFIDF Warning] Row 121 has zero TF-IDF weights.\n[TFIDF Warning] Row 138 has zero TF-IDF weights.\n[TFIDF Warning] Row 151 has zero TF-IDF weights.\n[TFIDF Warning] Row 251 has zero TF-IDF weights.\n[TFIDF Warning] Row 311 has zero TF-IDF weights.\n[TFIDF Warning] Row 388 has zero TF-IDF weights.\n[TFIDF Warning] Row 432 has zero TF-IDF weights.\n[TFIDF Warning] Row 436 has zero TF-IDF weights.\n[TFIDF Warning] Row 543 has zero TF-IDF weights.\n[TFIDF Warning] Row 600 has zero TF-IDF weights.\n[TFIDF Warning] Row 651 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_41.pkl\n\nProcessing chunk 43/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 257 has zero TF-IDF weights.\n[TFIDF Warning] Row 545 has zero TF-IDF weights.\n[TFIDF Warning] Row 681 has zero TF-IDF weights.\n[TFIDF Warning] Row 762 has zero TF-IDF weights.\n[TFIDF Warning] Row 31 has zero TF-IDF weights.\n[TFIDF Warning] Row 212 has zero TF-IDF weights.\n[TFIDF Warning] Row 299 has zero TF-IDF weights.\n[TFIDF Warning] Row 333 has zero TF-IDF weights.\n[TFIDF Warning] Row 422 has zero TF-IDF weights.\n[TFIDF Warning] Row 456 has zero TF-IDF weights.\n[TFIDF Warning] Row 612 has zero TF-IDF weights.\n[TFIDF Warning] Row 623 has zero TF-IDF weights.\n[TFIDF Warning] Row 625 has zero TF-IDF weights.\n[TFIDF Warning] Row 4 has zero TF-IDF weights.\n[TFIDF Warning] Row 36 has zero TF-IDF weights.\n[TFIDF Warning] Row 456 has zero TF-IDF weights.\n[TFIDF Warning] Row 623 has zero TF-IDF weights.\n[TFIDF Warning] Row 625 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_42.pkl\n\nProcessing chunk 44/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 98 has zero TF-IDF weights.\n[TFIDF Warning] Row 434 has zero TF-IDF weights.\n[TFIDF Warning] Row 695 has zero TF-IDF weights.\n[TFIDF Warning] Row 767 has zero TF-IDF weights.\n[TFIDF Warning] Row 69 has zero TF-IDF weights.\n[TFIDF Warning] Row 138 has zero TF-IDF weights.\n[TFIDF Warning] Row 143 has zero TF-IDF weights.\n[TFIDF Warning] Row 308 has zero TF-IDF weights.\n[TFIDF Warning] Row 337 has zero TF-IDF weights.\n[TFIDF Warning] Row 415 has zero TF-IDF weights.\n[TFIDF Warning] Row 429 has zero TF-IDF weights.\n[TFIDF Warning] Row 510 has zero TF-IDF weights.\n[TFIDF Warning] Row 531 has zero TF-IDF weights.\n[TFIDF Warning] Row 541 has zero TF-IDF weights.\n[TFIDF Warning] Row 577 has zero TF-IDF weights.\n[TFIDF Warning] Row 583 has zero TF-IDF weights.\n[TFIDF Warning] Row 593 has zero TF-IDF weights.\n[TFIDF Warning] Row 54 has zero TF-IDF weights.\n[TFIDF Warning] Row 106 has zero TF-IDF weights.\n[TFIDF Warning] Row 415 has zero TF-IDF weights.\n[TFIDF Warning] Row 429 has zero TF-IDF weights.\n[TFIDF Warning] Row 583 has zero TF-IDF weights.\n[TFIDF Warning] Row 593 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_43.pkl\n\nProcessing chunk 45/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 185 has zero TF-IDF weights.\n[TFIDF Warning] Row 502 has zero TF-IDF weights.\n[TFIDF Warning] Row 617 has zero TF-IDF weights.\n[TFIDF Warning] Row 636 has zero TF-IDF weights.\n[TFIDF Warning] Row 639 has zero TF-IDF weights.\n[TFIDF Warning] Row 261 has zero TF-IDF weights.\n[TFIDF Warning] Row 324 has zero TF-IDF weights.\n[TFIDF Warning] Row 397 has zero TF-IDF weights.\n[TFIDF Warning] Row 438 has zero TF-IDF weights.\n[TFIDF Warning] Row 466 has zero TF-IDF weights.\n[TFIDF Warning] Row 500 has zero TF-IDF weights.\n[TFIDF Warning] Row 567 has zero TF-IDF weights.\n[TFIDF Warning] Row 700 has zero TF-IDF weights.\n[TFIDF Warning] Row 742 has zero TF-IDF weights.\n[TFIDF Warning] Row 21 has zero TF-IDF weights.\n[TFIDF Warning] Row 261 has zero TF-IDF weights.\n[TFIDF Warning] Row 416 has zero TF-IDF weights.\n[TFIDF Warning] Row 438 has zero TF-IDF weights.\n[TFIDF Warning] Row 466 has zero TF-IDF weights.\n[TFIDF Warning] Row 500 has zero TF-IDF weights.\n[TFIDF Warning] Row 743 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_44.pkl\n\nProcessing chunk 46/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 82 has zero TF-IDF weights.\n[TFIDF Warning] Row 507 has zero TF-IDF weights.\n[TFIDF Warning] Row 685 has zero TF-IDF weights.\n[TFIDF Warning] Row 105 has zero TF-IDF weights.\n[TFIDF Warning] Row 178 has zero TF-IDF weights.\n[TFIDF Warning] Row 298 has zero TF-IDF weights.\n[TFIDF Warning] Row 394 has zero TF-IDF weights.\n[TFIDF Warning] Row 412 has zero TF-IDF weights.\n[TFIDF Warning] Row 421 has zero TF-IDF weights.\n[TFIDF Warning] Row 497 has zero TF-IDF weights.\n[TFIDF Warning] Row 507 has zero TF-IDF weights.\n[TFIDF Warning] Row 525 has zero TF-IDF weights.\n[TFIDF Warning] Row 537 has zero TF-IDF weights.\n[TFIDF Warning] Row 11 has zero TF-IDF weights.\n[TFIDF Warning] Row 33 has zero TF-IDF weights.\n[TFIDF Warning] Row 43 has zero TF-IDF weights.\n[TFIDF Warning] Row 99 has zero TF-IDF weights.\n[TFIDF Warning] Row 105 has zero TF-IDF weights.\n[TFIDF Warning] Row 129 has zero TF-IDF weights.\n[TFIDF Warning] Row 178 has zero TF-IDF weights.\n[TFIDF Warning] Row 321 has zero TF-IDF weights.\n[TFIDF Warning] Row 412 has zero TF-IDF weights.\n[TFIDF Warning] Row 421 has zero TF-IDF weights.\n[TFIDF Warning] Row 439 has zero TF-IDF weights.\n[TFIDF Warning] Row 507 has zero TF-IDF weights.\n[TFIDF Warning] Row 512 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_45.pkl\n\nProcessing chunk 47/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 237 has zero TF-IDF weights.\n[TFIDF Warning] Row 303 has zero TF-IDF weights.\n[TFIDF Warning] Row 568 has zero TF-IDF weights.\n[TFIDF Warning] Row 572 has zero TF-IDF weights.\n[TFIDF Warning] Row 620 has zero TF-IDF weights.\n[TFIDF Warning] Row 128 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 327 has zero TF-IDF weights.\n[TFIDF Warning] Row 516 has zero TF-IDF weights.\n[TFIDF Warning] Row 545 has zero TF-IDF weights.\n[TFIDF Warning] Row 584 has zero TF-IDF weights.\n[TFIDF Warning] Row 629 has zero TF-IDF weights.\n[TFIDF Warning] Row 12 has zero TF-IDF weights.\n[TFIDF Warning] Row 26 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 572 has zero TF-IDF weights.\n[TFIDF Warning] Row 595 has zero TF-IDF weights.\n[TFIDF Warning] Row 688 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_46.pkl\n\nProcessing chunk 48/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 469 has zero TF-IDF weights.\n[TFIDF Warning] Row 87 has zero TF-IDF weights.\n[TFIDF Warning] Row 214 has zero TF-IDF weights.\n[TFIDF Warning] Row 231 has zero TF-IDF weights.\n[TFIDF Warning] Row 299 has zero TF-IDF weights.\n[TFIDF Warning] Row 359 has zero TF-IDF weights.\n[TFIDF Warning] Row 385 has zero TF-IDF weights.\n[TFIDF Warning] Row 437 has zero TF-IDF weights.\n[TFIDF Warning] Row 484 has zero TF-IDF weights.\n[TFIDF Warning] Row 531 has zero TF-IDF weights.\n[TFIDF Warning] Row 640 has zero TF-IDF weights.\n[TFIDF Warning] Row 684 has zero TF-IDF weights.\n[TFIDF Warning] Row 760 has zero TF-IDF weights.\n[TFIDF Warning] Row 87 has zero TF-IDF weights.\n[TFIDF Warning] Row 214 has zero TF-IDF weights.\n[TFIDF Warning] Row 231 has zero TF-IDF weights.\n[TFIDF Warning] Row 239 has zero TF-IDF weights.\n[TFIDF Warning] Row 299 has zero TF-IDF weights.\n[TFIDF Warning] Row 385 has zero TF-IDF weights.\n[TFIDF Warning] Row 437 has zero TF-IDF weights.\n[TFIDF Warning] Row 490 has zero TF-IDF weights.\n[TFIDF Warning] Row 558 has zero TF-IDF weights.\n[TFIDF Warning] Row 640 has zero TF-IDF weights.\n[TFIDF Warning] Row 699 has zero TF-IDF weights.\n[TFIDF Warning] Row 748 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_47.pkl\n\nProcessing chunk 49/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 345 has zero TF-IDF weights.\n[TFIDF Warning] Row 501 has zero TF-IDF weights.\n[TFIDF Warning] Row 740 has zero TF-IDF weights.\n[TFIDF Warning] Row 67 has zero TF-IDF weights.\n[TFIDF Warning] Row 313 has zero TF-IDF weights.\n[TFIDF Warning] Row 358 has zero TF-IDF weights.\n[TFIDF Warning] Row 610 has zero TF-IDF weights.\n[TFIDF Warning] Row 634 has zero TF-IDF weights.\n[TFIDF Warning] Row 662 has zero TF-IDF weights.\n[TFIDF Warning] Row 726 has zero TF-IDF weights.\n[TFIDF Warning] Row 43 has zero TF-IDF weights.\n[TFIDF Warning] Row 67 has zero TF-IDF weights.\n[TFIDF Warning] Row 107 has zero TF-IDF weights.\n[TFIDF Warning] Row 129 has zero TF-IDF weights.\n[TFIDF Warning] Row 157 has zero TF-IDF weights.\n[TFIDF Warning] Row 191 has zero TF-IDF weights.\n[TFIDF Warning] Row 220 has zero TF-IDF weights.\n[TFIDF Warning] Row 239 has zero TF-IDF weights.\n[TFIDF Warning] Row 306 has zero TF-IDF weights.\n[TFIDF Warning] Row 313 has zero TF-IDF weights.\n[TFIDF Warning] Row 322 has zero TF-IDF weights.\n[TFIDF Warning] Row 351 has zero TF-IDF weights.\n[TFIDF Warning] Row 394 has zero TF-IDF weights.\n[TFIDF Warning] Row 512 has zero TF-IDF weights.\n[TFIDF Warning] Row 611 has zero TF-IDF weights.\n[TFIDF Warning] Row 652 has zero TF-IDF weights.\n[TFIDF Warning] Row 725 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_48.pkl\n\nProcessing chunk 50/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 526 has zero TF-IDF weights.\n[TFIDF Warning] Row 38 has zero TF-IDF weights.\n[TFIDF Warning] Row 95 has zero TF-IDF weights.\n[TFIDF Warning] Row 121 has zero TF-IDF weights.\n[TFIDF Warning] Row 124 has zero TF-IDF weights.\n[TFIDF Warning] Row 198 has zero TF-IDF weights.\n[TFIDF Warning] Row 241 has zero TF-IDF weights.\n[TFIDF Warning] Row 245 has zero TF-IDF weights.\n[TFIDF Warning] Row 305 has zero TF-IDF weights.\n[TFIDF Warning] Row 364 has zero TF-IDF weights.\n[TFIDF Warning] Row 464 has zero TF-IDF weights.\n[TFIDF Warning] Row 498 has zero TF-IDF weights.\n[TFIDF Warning] Row 512 has zero TF-IDF weights.\n[TFIDF Warning] Row 568 has zero TF-IDF weights.\n[TFIDF Warning] Row 674 has zero TF-IDF weights.\n[TFIDF Warning] Row 698 has zero TF-IDF weights.\n[TFIDF Warning] Row 760 has zero TF-IDF weights.\n[TFIDF Warning] Row 38 has zero TF-IDF weights.\n[TFIDF Warning] Row 95 has zero TF-IDF weights.\n[TFIDF Warning] Row 109 has zero TF-IDF weights.\n[TFIDF Warning] Row 114 has zero TF-IDF weights.\n[TFIDF Warning] Row 195 has zero TF-IDF weights.\n[TFIDF Warning] Row 198 has zero TF-IDF weights.\n[TFIDF Warning] Row 245 has zero TF-IDF weights.\n[TFIDF Warning] Row 305 has zero TF-IDF weights.\n[TFIDF Warning] Row 312 has zero TF-IDF weights.\n[TFIDF Warning] Row 364 has zero TF-IDF weights.\n[TFIDF Warning] Row 464 has zero TF-IDF weights.\n[TFIDF Warning] Row 512 has zero TF-IDF weights.\n[TFIDF Warning] Row 568 has zero TF-IDF weights.\n[TFIDF Warning] Row 621 has zero TF-IDF weights.\n[TFIDF Warning] Row 637 has zero TF-IDF weights.\n[TFIDF Warning] Row 674 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_49.pkl\n\nProcessing chunk 51/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 40 has zero TF-IDF weights.\n[TFIDF Warning] Row 49 has zero TF-IDF weights.\n[TFIDF Warning] Row 91 has zero TF-IDF weights.\n[TFIDF Warning] Row 96 has zero TF-IDF weights.\n[TFIDF Warning] Row 201 has zero TF-IDF weights.\n[TFIDF Warning] Row 244 has zero TF-IDF weights.\n[TFIDF Warning] Row 280 has zero TF-IDF weights.\n[TFIDF Warning] Row 297 has zero TF-IDF weights.\n[TFIDF Warning] Row 326 has zero TF-IDF weights.\n[TFIDF Warning] Row 338 has zero TF-IDF weights.\n[TFIDF Warning] Row 350 has zero TF-IDF weights.\n[TFIDF Warning] Row 409 has zero TF-IDF weights.\n[TFIDF Warning] Row 679 has zero TF-IDF weights.\n[TFIDF Warning] Row 728 has zero TF-IDF weights.\n[TFIDF Warning] Row 737 has zero TF-IDF weights.\n[TFIDF Warning] Row 40 has zero TF-IDF weights.\n[TFIDF Warning] Row 64 has zero TF-IDF weights.\n[TFIDF Warning] Row 91 has zero TF-IDF weights.\n[TFIDF Warning] Row 202 has zero TF-IDF weights.\n[TFIDF Warning] Row 219 has zero TF-IDF weights.\n[TFIDF Warning] Row 244 has zero TF-IDF weights.\n[TFIDF Warning] Row 273 has zero TF-IDF weights.\n[TFIDF Warning] Row 279 has zero TF-IDF weights.\n[TFIDF Warning] Row 409 has zero TF-IDF weights.\n[TFIDF Warning] Row 537 has zero TF-IDF weights.\n[TFIDF Warning] Row 587 has zero TF-IDF weights.\n[TFIDF Warning] Row 679 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_50.pkl\n\nProcessing chunk 52/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 45 has zero TF-IDF weights.\n[TFIDF Warning] Row 140 has zero TF-IDF weights.\n[TFIDF Warning] Row 278 has zero TF-IDF weights.\n[TFIDF Warning] Row 638 has zero TF-IDF weights.\n[TFIDF Warning] Row 688 has zero TF-IDF weights.\n[TFIDF Warning] Row 45 has zero TF-IDF weights.\n[TFIDF Warning] Row 157 has zero TF-IDF weights.\n[TFIDF Warning] Row 278 has zero TF-IDF weights.\n[TFIDF Warning] Row 370 has zero TF-IDF weights.\n[TFIDF Warning] Row 470 has zero TF-IDF weights.\n[TFIDF Warning] Row 627 has zero TF-IDF weights.\n[TFIDF Warning] Row 655 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_51.pkl\n\nProcessing chunk 53/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 77 has zero TF-IDF weights.\n[TFIDF Warning] Row 604 has zero TF-IDF weights.\n[TFIDF Warning] Row 37 has zero TF-IDF weights.\n[TFIDF Warning] Row 56 has zero TF-IDF weights.\n[TFIDF Warning] Row 246 has zero TF-IDF weights.\n[TFIDF Warning] Row 357 has zero TF-IDF weights.\n[TFIDF Warning] Row 432 has zero TF-IDF weights.\n[TFIDF Warning] Row 473 has zero TF-IDF weights.\n[TFIDF Warning] Row 691 has zero TF-IDF weights.\n[TFIDF Warning] Row 759 has zero TF-IDF weights.\n[TFIDF Warning] Row 22 has zero TF-IDF weights.\n[TFIDF Warning] Row 37 has zero TF-IDF weights.\n[TFIDF Warning] Row 67 has zero TF-IDF weights.\n[TFIDF Warning] Row 220 has zero TF-IDF weights.\n[TFIDF Warning] Row 278 has zero TF-IDF weights.\n[TFIDF Warning] Row 341 has zero TF-IDF weights.\n[TFIDF Warning] Row 357 has zero TF-IDF weights.\n[TFIDF Warning] Row 459 has zero TF-IDF weights.\n[TFIDF Warning] Row 462 has zero TF-IDF weights.\n[TFIDF Warning] Row 474 has zero TF-IDF weights.\n[TFIDF Warning] Row 594 has zero TF-IDF weights.\n[TFIDF Warning] Row 765 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_52.pkl\n\nProcessing chunk 54/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 147 has zero TF-IDF weights.\n[TFIDF Warning] Row 151 has zero TF-IDF weights.\n[TFIDF Warning] Row 205 has zero TF-IDF weights.\n[TFIDF Warning] Row 260 has zero TF-IDF weights.\n[TFIDF Warning] Row 318 has zero TF-IDF weights.\n[TFIDF Warning] Row 334 has zero TF-IDF weights.\n[TFIDF Warning] Row 715 has zero TF-IDF weights.\n[TFIDF Warning] Row 38 has zero TF-IDF weights.\n[TFIDF Warning] Row 154 has zero TF-IDF weights.\n[TFIDF Warning] Row 192 has zero TF-IDF weights.\n[TFIDF Warning] Row 246 has zero TF-IDF weights.\n[TFIDF Warning] Row 384 has zero TF-IDF weights.\n[TFIDF Warning] Row 405 has zero TF-IDF weights.\n[TFIDF Warning] Row 463 has zero TF-IDF weights.\n[TFIDF Warning] Row 636 has zero TF-IDF weights.\n[TFIDF Warning] Row 717 has zero TF-IDF weights.\n[TFIDF Warning] Row 135 has zero TF-IDF weights.\n[TFIDF Warning] Row 242 has zero TF-IDF weights.\n[TFIDF Warning] Row 246 has zero TF-IDF weights.\n[TFIDF Warning] Row 405 has zero TF-IDF weights.\n[TFIDF Warning] Row 455 has zero TF-IDF weights.\n[TFIDF Warning] Row 654 has zero TF-IDF weights.\n[TFIDF Warning] Row 705 has zero TF-IDF weights.\n[TFIDF Warning] Row 717 has zero TF-IDF weights.\n[TFIDF Warning] Row 718 has zero TF-IDF weights.\n[TFIDF Warning] Row 735 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_53.pkl\n\nProcessing chunk 55/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 34 has zero TF-IDF weights.\n[TFIDF Warning] Row 178 has zero TF-IDF weights.\n[TFIDF Warning] Row 513 has zero TF-IDF weights.\n[TFIDF Warning] Row 547 has zero TF-IDF weights.\n[TFIDF Warning] Row 696 has zero TF-IDF weights.\n[TFIDF Warning] Row 715 has zero TF-IDF weights.\n[TFIDF Warning] Row 745 has zero TF-IDF weights.\n[TFIDF Warning] Row 32 has zero TF-IDF weights.\n[TFIDF Warning] Row 41 has zero TF-IDF weights.\n[TFIDF Warning] Row 101 has zero TF-IDF weights.\n[TFIDF Warning] Row 203 has zero TF-IDF weights.\n[TFIDF Warning] Row 222 has zero TF-IDF weights.\n[TFIDF Warning] Row 277 has zero TF-IDF weights.\n[TFIDF Warning] Row 745 has zero TF-IDF weights.\n[TFIDF Warning] Row 750 has zero TF-IDF weights.\n[TFIDF Warning] Row 32 has zero TF-IDF weights.\n[TFIDF Warning] Row 203 has zero TF-IDF weights.\n[TFIDF Warning] Row 239 has zero TF-IDF weights.\n[TFIDF Warning] Row 585 has zero TF-IDF weights.\n[TFIDF Warning] Row 595 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_54.pkl\n\nProcessing chunk 56/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 105 has zero TF-IDF weights.\n[TFIDF Warning] Row 125 has zero TF-IDF weights.\n[TFIDF Warning] Row 193 has zero TF-IDF weights.\n[TFIDF Warning] Row 217 has zero TF-IDF weights.\n[TFIDF Warning] Row 225 has zero TF-IDF weights.\n[TFIDF Warning] Row 291 has zero TF-IDF weights.\n[TFIDF Warning] Row 391 has zero TF-IDF weights.\n[TFIDF Warning] Row 442 has zero TF-IDF weights.\n[TFIDF Warning] Row 500 has zero TF-IDF weights.\n[TFIDF Warning] Row 638 has zero TF-IDF weights.\n[TFIDF Warning] Row 673 has zero TF-IDF weights.\n[TFIDF Warning] Row 764 has zero TF-IDF weights.\n[TFIDF Warning] Row 391 has zero TF-IDF weights.\n[TFIDF Warning] Row 470 has zero TF-IDF weights.\n[TFIDF Warning] Row 673 has zero TF-IDF weights.\n[TFIDF Warning] Row 703 has zero TF-IDF weights.\n[TFIDF Warning] Row 764 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_55.pkl\n\nProcessing chunk 57/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 95 has zero TF-IDF weights.\n[TFIDF Warning] Row 707 has zero TF-IDF weights.\n[TFIDF Warning] Row 42 has zero TF-IDF weights.\n[TFIDF Warning] Row 127 has zero TF-IDF weights.\n[TFIDF Warning] Row 217 has zero TF-IDF weights.\n[TFIDF Warning] Row 228 has zero TF-IDF weights.\n[TFIDF Warning] Row 245 has zero TF-IDF weights.\n[TFIDF Warning] Row 278 has zero TF-IDF weights.\n[TFIDF Warning] Row 349 has zero TF-IDF weights.\n[TFIDF Warning] Row 604 has zero TF-IDF weights.\n[TFIDF Warning] Row 716 has zero TF-IDF weights.\n[TFIDF Warning] Row 95 has zero TF-IDF weights.\n[TFIDF Warning] Row 127 has zero TF-IDF weights.\n[TFIDF Warning] Row 228 has zero TF-IDF weights.\n[TFIDF Warning] Row 278 has zero TF-IDF weights.\n[TFIDF Warning] Row 349 has zero TF-IDF weights.\n[TFIDF Warning] Row 379 has zero TF-IDF weights.\n[TFIDF Warning] Row 385 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_56.pkl\n\nProcessing chunk 58/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 471 has zero TF-IDF weights.\n[TFIDF Warning] Row 533 has zero TF-IDF weights.\n[TFIDF Warning] Row 560 has zero TF-IDF weights.\n[TFIDF Warning] Row 671 has zero TF-IDF weights.\n[TFIDF Warning] Row 47 has zero TF-IDF weights.\n[TFIDF Warning] Row 175 has zero TF-IDF weights.\n[TFIDF Warning] Row 185 has zero TF-IDF weights.\n[TFIDF Warning] Row 207 has zero TF-IDF weights.\n[TFIDF Warning] Row 551 has zero TF-IDF weights.\n[TFIDF Warning] Row 552 has zero TF-IDF weights.\n[TFIDF Warning] Row 609 has zero TF-IDF weights.\n[TFIDF Warning] Row 630 has zero TF-IDF weights.\n[TFIDF Warning] Row 658 has zero TF-IDF weights.\n[TFIDF Warning] Row 34 has zero TF-IDF weights.\n[TFIDF Warning] Row 159 has zero TF-IDF weights.\n[TFIDF Warning] Row 175 has zero TF-IDF weights.\n[TFIDF Warning] Row 195 has zero TF-IDF weights.\n[TFIDF Warning] Row 203 has zero TF-IDF weights.\n[TFIDF Warning] Row 271 has zero TF-IDF weights.\n[TFIDF Warning] Row 471 has zero TF-IDF weights.\n[TFIDF Warning] Row 552 has zero TF-IDF weights.\n[TFIDF Warning] Row 609 has zero TF-IDF weights.\n[TFIDF Warning] Row 630 has zero TF-IDF weights.\n[TFIDF Warning] Row 631 has zero TF-IDF weights.\n[TFIDF Warning] Row 686 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_57.pkl\n\nProcessing chunk 59/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (768, 1536)\nResponse A stats shape: (768, 1536)\nResponse B stats shape: (768, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (768, 2)\nMean similarity shape: (768, 2)\nMedian similarity shape: (768, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 223 has zero TF-IDF weights.\n[TFIDF Warning] Row 61 has zero TF-IDF weights.\n[TFIDF Warning] Row 67 has zero TF-IDF weights.\n[TFIDF Warning] Row 271 has zero TF-IDF weights.\n[TFIDF Warning] Row 301 has zero TF-IDF weights.\n[TFIDF Warning] Row 313 has zero TF-IDF weights.\n[TFIDF Warning] Row 432 has zero TF-IDF weights.\n[TFIDF Warning] Row 463 has zero TF-IDF weights.\n[TFIDF Warning] Row 503 has zero TF-IDF weights.\n[TFIDF Warning] Row 620 has zero TF-IDF weights.\n[TFIDF Warning] Row 632 has zero TF-IDF weights.\n[TFIDF Warning] Row 70 has zero TF-IDF weights.\n[TFIDF Warning] Row 301 has zero TF-IDF weights.\n[TFIDF Warning] Row 620 has zero TF-IDF weights.\n[TFIDF Warning] Row 632 has zero TF-IDF weights.\nPrompt TF-IDF shape: (768, 1536)\nResponse A TF-IDF shape: (768, 1536)\nResponse B TF-IDF shape: (768, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 768\n\nFeature shapes and types before conversion:\nraw_sim: shape=(768, 2), dtype=float64\nmean_sim: shape=(768, 2), dtype=float64\nmedian_sim: shape=(768, 2), dtype=float64\ntfidf_mean_sim_a: shape=(768, 1), dtype=float64\ntfidf_mean_sim_b: shape=(768, 1), dtype=float64\ntfidf_median_sim_a: shape=(768, 1), dtype=float64\ntfidf_median_sim_b: shape=(768, 1), dtype=float64\nConverted raw_sim: shape=(768, 2), dtype=float32\nConverted mean_sim: shape=(768, 2), dtype=float32\nConverted median_sim: shape=(768, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(768, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(768, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (768, 2)\nFeature 1 shape: (768, 2)\nFeature 2 shape: (768, 2)\nFeature 3 shape: (768, 1)\nFeature 4 shape: (768, 1)\nFeature 5 shape: (768, 1)\nFeature 6 shape: (768, 1)\n\nFinal combined features shape: (768, 10)\nSaved chunk to embedded_features.pkl.chunk_58.pkl\n\nProcessing chunk 60/60\nGenerating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n\nStep 2: Extracting embedding statistics...\nPrompt stats shape: (669, 1536)\nResponse A stats shape: (669, 1536)\nResponse B stats shape: (669, 1536)\n\nStep 3: Calculating similarities...\nRaw similarity shape: (669, 2)\nMean similarity shape: (669, 2)\nMedian similarity shape: (669, 2)\n\nStep 4: Computing TF-IDF attention...\n[TFIDF Warning] Row 516 has zero TF-IDF weights.\n[TFIDF Warning] Row 123 has zero TF-IDF weights.\n[TFIDF Warning] Row 284 has zero TF-IDF weights.\n[TFIDF Warning] Row 288 has zero TF-IDF weights.\n[TFIDF Warning] Row 556 has zero TF-IDF weights.\n[TFIDF Warning] Row 620 has zero TF-IDF weights.\n[TFIDF Warning] Row 638 has zero TF-IDF weights.\n[TFIDF Warning] Row 665 has zero TF-IDF weights.\n[TFIDF Warning] Row 74 has zero TF-IDF weights.\n[TFIDF Warning] Row 211 has zero TF-IDF weights.\n[TFIDF Warning] Row 339 has zero TF-IDF weights.\n[TFIDF Warning] Row 432 has zero TF-IDF weights.\n[TFIDF Warning] Row 556 has zero TF-IDF weights.\n[TFIDF Warning] Row 620 has zero TF-IDF weights.\nPrompt TF-IDF shape: (669, 1536)\nResponse A TF-IDF shape: (669, 1536)\nResponse B TF-IDF shape: (669, 1536)\n\nCalculating one-to-one TF-IDF weighted similarities...\n\nSimilarity shapes:\nNumber of rows: 669\n\nFeature shapes and types before conversion:\nraw_sim: shape=(669, 2), dtype=float64\nmean_sim: shape=(669, 2), dtype=float64\nmedian_sim: shape=(669, 2), dtype=float64\ntfidf_mean_sim_a: shape=(669, 1), dtype=float64\ntfidf_mean_sim_b: shape=(669, 1), dtype=float64\ntfidf_median_sim_a: shape=(669, 1), dtype=float64\ntfidf_median_sim_b: shape=(669, 1), dtype=float64\nConverted raw_sim: shape=(669, 2), dtype=float32\nConverted mean_sim: shape=(669, 2), dtype=float32\nConverted median_sim: shape=(669, 2), dtype=float32\nConverted tfidf_mean_sim_a: shape=(669, 1), dtype=float32\nConverted tfidf_mean_sim_b: shape=(669, 1), dtype=float32\nConverted tfidf_median_sim_a: shape=(669, 1), dtype=float32\nConverted tfidf_median_sim_b: shape=(669, 1), dtype=float32\n\nFeature shapes before stacking:\nFeature 0 shape: (669, 2)\nFeature 1 shape: (669, 2)\nFeature 2 shape: (669, 2)\nFeature 3 shape: (669, 1)\nFeature 4 shape: (669, 1)\nFeature 5 shape: (669, 1)\nFeature 6 shape: (669, 1)\n\nFinal combined features shape: (669, 10)\nSaved chunk to embedded_features.pkl.chunk_59.pkl\n\nProcessing complete!\nLoading and combining chunks...\nConcatenating all chunks...\nFinal DataFrame shape: (45981, 10)\n\nEmbedded DataFrame columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Step 2: Extract statistics from embeddings\nprint(\"\\nStep 2: Extracting embedding statistics...\")\n\n# Process each type of embedding separately\nprompt_stats = EmbeddingStatsExtractor().fit_transform(embedded_df[\"prompt_embed\"])\nresp_a_stats = EmbeddingStatsExtractor().fit_transform(embedded_df[\"resp_a_embed\"])\nresp_b_stats = EmbeddingStatsExtractor().fit_transform(embedded_df[\"resp_b_embed\"])\n\nprint(f\"Prompt stats shape: {prompt_stats.shape}\")\nprint(f\"Response A stats shape: {resp_a_stats.shape}\")\nprint(f\"Response B stats shape: {resp_b_stats.shape}\")","metadata":{"execution":{"execution_failed":"2025-09-19T23:42:13.105Z"}}},{"cell_type":"markdown","source":"embedded_df[\"prompt_embed\"]","metadata":{"execution":{"execution_failed":"2025-09-19T23:42:13.106Z"}}},{"cell_type":"markdown","source":"# Step 3: Calculate similarities\nprint(\"\\nStep 3: Calculating similarities...\")\n\n# Raw embedding similarity\nraw_sim = RawEmbeddingSimilarity().fit_transform(embedded_df)\nprint(f\"Raw similarity shape: {raw_sim.shape}\")\n\n# Mean and median similarities\nmean_sim = AggregatedEmbeddingSimilarity(agg_type=\"mean\").fit_transform(embedded_df)\nmedian_sim = AggregatedEmbeddingSimilarity(agg_type=\"median\").fit_transform(embedded_df)\n\nprint(f\"Mean similarity shape: {mean_sim.shape}\")\nprint(f\"Median similarity shape: {median_sim.shape}\")","metadata":{"execution":{"execution_failed":"2025-09-19T23:42:13.106Z"}}},{"cell_type":"markdown","source":"# Step 4: TF-IDF attention\nprint(\"\\nStep 4: Computing TF-IDF attention...\")\n\n# Create and fit TF-IDF embedders\ntfidf_prompt = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\ntfidf_resp_a = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\ntfidf_resp_b = TFIDFAttentionEmbedder(tokenizer=shared_embedder.tokenizer)\n\n# Transform each column\nprompt_tfidf = tfidf_prompt.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in embedded_df['prompt_embed']])\nresp_a_tfidf = tfidf_resp_a.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in embedded_df['resp_a_embed']])\nresp_b_tfidf = tfidf_resp_b.fit_transform([{'text': x['text'], 'raw': x['raw']} for x in embedded_df['resp_b_embed']])\n\nprint(f\"Prompt TF-IDF shape: {prompt_tfidf.shape}\")\nprint(f\"Response A TF-IDF shape: {resp_a_tfidf.shape}\")\nprint(f\"Response B TF-IDF shape: {resp_b_tfidf.shape}\")","metadata":{"execution":{"execution_failed":"2025-09-19T23:42:13.107Z"}}},{"cell_type":"markdown","source":"# Calculate TF-IDF weighted similarities (one-to-one)\nprint(\"\\nCalculating one-to-one TF-IDF weighted similarities...\")\nhalf_dim=768\nprompt_mean = prompt_tfidf[:, :half_dim]\nprompt_median = prompt_tfidf[:, half_dim:]\nresp_a_mean = resp_a_tfidf[:, :half_dim]\nresp_a_median = resp_a_tfidf[:, half_dim:]\nresp_b_mean = resp_b_tfidf[:, :half_dim]\nresp_b_median = resp_b_tfidf[:, half_dim:]\n\n# Calculate similarities between corresponding pairs\nmean_sim_a = np.array([\n    cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0]\n    for p, a in zip(prompt_mean, resp_a_mean)\n])\nmean_sim_b = np.array([\n    cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0]\n    for p, b in zip(prompt_mean, resp_b_mean)\n])\n\nmedian_sim_a = np.array([\n    cosine_similarity(p.reshape(1, -1), a.reshape(1, -1))[0, 0]\n    for p, a in zip(prompt_median, resp_a_median)\n])\nmedian_sim_b = np.array([\n    cosine_similarity(p.reshape(1, -1), b.reshape(1, -1))[0, 0]\n    for p, b in zip(prompt_median, resp_b_median)\n])\n\n# Create DataFrame with one-to-one similarities\nsimilarities_df = pd.DataFrame({\n    'tfidf_mean_sim_a': mean_sim_a,\n    'tfidf_mean_sim_b': mean_sim_b,\n    'tfidf_median_sim_a': median_sim_a,\n    'tfidf_median_sim_b': median_sim_b\n})\n\nprint(\"\\nSimilarity shapes:\")\nprint(f\"Number of rows: {len(similarities_df)}\")\nprint(\"\\nFirst few rows of similarities:\")\nprint(similarities_df.head())","metadata":{"execution":{"execution_failed":"2025-09-19T23:42:13.107Z"}}},{"cell_type":"code","source":"# Step 5: Common words features\nprint(\"\\nStep 5: Computing common words features...\")\n\ncommon_words_features = common_words_transformer.fit_transform(X_train)\nprint(f\"Common words features shape: {common_words_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T10:43:05.421098Z","iopub.execute_input":"2025-09-20T10:43:05.421395Z","iopub.status.idle":"2025-09-20T10:43:58.875701Z","shell.execute_reply.started":"2025-09-20T10:43:05.421374Z","shell.execute_reply":"2025-09-20T10:43:58.875080Z"}},"outputs":[{"name":"stdout","text":"\nStep 5: Computing common words features...\nCommon words features shape: (45981, 2)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"common_words_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T10:50:51.491202Z","iopub.execute_input":"2025-09-20T10:50:51.491782Z","iopub.status.idle":"2025-09-20T10:50:51.496474Z","shell.execute_reply.started":"2025-09-20T10:50:51.491758Z","shell.execute_reply":"2025-09-20T10:50:51.495841Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"array([[ 5,  4],\n       [10, 11],\n       [ 8,  5],\n       ...,\n       [ 2,  2],\n       [ 4,  4],\n       [ 0,  0]])"},"metadata":{}}],"execution_count":46},{"cell_type":"markdown","source":"# Step 6: Combine all features\nprint(\"\\nStep 6: Combining all features...\")\n\n# Convert numpy arrays to sparse matrices if needed\nfrom scipy.sparse import csr_matrix, hstack\n\ndef ensure_sparse(x, dtype=np.float32):\n    if x is None:\n        return None\n    # Convert to numpy array first with consistent dtype\n    if not isinstance(x, (csr_matrix, np.ndarray)):\n        x = np.array(x, dtype=dtype)\n    elif isinstance(x, np.ndarray):\n        x = x.astype(dtype)\n    \n    # If already sparse, ensure correct dtype\n    if isinstance(x, csr_matrix):\n        return x.astype(dtype)\n    \n    # Convert to sparse\n    return csr_matrix(x)\n\n# Print shapes and dtypes before conversion\nprint(\"\\nFeature shapes and types before conversion:\")\nfeatures = [\n    #(\"prompt_stats\", prompt_stats),\n    #(\"resp_a_stats\", resp_a_stats),\n    #(\"resp_b_stats\", resp_b_stats),\n    (\"raw_sim\", raw_sim),\n    (\"mean_sim\", mean_sim),\n    (\"median_sim\", median_sim),\n    (\"tfidf_mean_sim_a\", similarities_df['tfidf_mean_sim_a'].values.reshape(-1,1)),\n    (\"tfidf_mean_sim_b\", similarities_df['tfidf_mean_sim_b'].values.reshape(-1,1)),\n    (\"tfidf_median_sim_a\", similarities_df['tfidf_median_sim_a'].values.reshape(-1,1)),\n    (\"tfidf_median_sim_b\",similarities_df['tfidf_median_sim_b'].values.reshape(-1,1)),\n    (\"common_words\", common_words_features)\n]\n\nfor name, feat in features:\n    if feat is not None:\n        print(f\"{name}: shape={feat.shape}, dtype={feat.dtype}\")\n\n# Convert each feature set to sparse format with consistent dtype\nfeatures_to_stack = []\nfor name, feat in features:\n    if feat is not None:\n        sparse_feat = ensure_sparse(feat)\n        features_to_stack.append(sparse_feat)\n        print(f\"Converted {name}: shape={sparse_feat.shape}, dtype={sparse_feat.dtype}\")\n\n# Print shapes before stacking\nprint(\"\\nFeature shapes before stacking:\")\nfor i, feat in enumerate(features_to_stack):\n    print(f\"Feature {i} shape: {feat.shape}\")\n\n# Combine all the feature matrices\nfinal_features = hstack(features_to_stack)\n\nprint(f\"\\nFinal combined features shape: {final_features.shape}\")\n\n# Convert to DataFrame and save\nX_final = pd.DataFrame(final_features.toarray())\n#X_final.to_csv(\"X_final_step_by_step.csv\", index=False)\n#print(\"\\nFeatures saved to 'X_final_step_by_step.csv'\")","metadata":{"execution":{"iopub.status.busy":"2025-09-20T10:32:30.460240Z","iopub.execute_input":"2025-09-20T10:32:30.460473Z","iopub.status.idle":"2025-09-20T10:33:24.040551Z","shell.execute_reply.started":"2025-09-20T10:32:30.460456Z","shell.execute_reply":"2025-09-20T10:33:24.039659Z"},"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"embedded_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T10:45:49.697903Z","iopub.execute_input":"2025-09-20T10:45:49.698477Z","iopub.status.idle":"2025-09-20T10:45:49.712186Z","shell.execute_reply.started":"2025-09-20T10:45:49.698456Z","shell.execute_reply":"2025-09-20T10:45:49.711568Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"              0         1         2         3         4         5         6  \\\n0      0.048206  0.266984  0.096257  0.553622 -0.054856  0.424662  0.864312   \n1      0.092558  0.088836  0.232275  0.206772  0.644696  0.622387  0.928233   \n2      0.362552  0.159841  0.652014  0.297614  0.853396 -0.019965  0.786856   \n3      0.230009  0.077728  0.387369  0.124695  0.104626  0.016593  0.781424   \n4      0.231216  0.257020  0.628362  0.624361  0.028580 -0.014859  0.900432   \n...         ...       ...       ...       ...       ...       ...       ...   \n45976  0.125274  0.411201  0.220533  0.791142 -0.085762  0.975237  0.750785   \n45977  0.002032 -0.000362 -0.025511 -0.028797 -0.149126 -0.146241  0.000000   \n45978  0.252332  0.242113  0.477212  0.443996  0.305100  0.303021  0.827993   \n45979  0.696520  0.173814  0.974114  0.285210  0.997904 -0.013173  0.966053   \n45980  0.063313  0.165001  0.090970  0.261274 -0.046661  0.197984  0.838114   \n\n              7         8         9  \n0      0.919458  0.829818  0.901671  \n1      0.867967  0.909265  0.827335  \n2      0.750248  0.743564  0.703505  \n3      0.804966  0.743567  0.748063  \n4      0.906686  0.895983  0.896844  \n...         ...       ...       ...  \n45976  0.620295  0.711580  0.536891  \n45977  0.958179  0.000000  0.934212  \n45978  0.850761  0.818516  0.865889  \n45979  0.887665  0.946798  0.872915  \n45980  0.803018  0.805370  0.756684  \n\n[45981 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.048206</td>\n      <td>0.266984</td>\n      <td>0.096257</td>\n      <td>0.553622</td>\n      <td>-0.054856</td>\n      <td>0.424662</td>\n      <td>0.864312</td>\n      <td>0.919458</td>\n      <td>0.829818</td>\n      <td>0.901671</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.092558</td>\n      <td>0.088836</td>\n      <td>0.232275</td>\n      <td>0.206772</td>\n      <td>0.644696</td>\n      <td>0.622387</td>\n      <td>0.928233</td>\n      <td>0.867967</td>\n      <td>0.909265</td>\n      <td>0.827335</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.362552</td>\n      <td>0.159841</td>\n      <td>0.652014</td>\n      <td>0.297614</td>\n      <td>0.853396</td>\n      <td>-0.019965</td>\n      <td>0.786856</td>\n      <td>0.750248</td>\n      <td>0.743564</td>\n      <td>0.703505</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.230009</td>\n      <td>0.077728</td>\n      <td>0.387369</td>\n      <td>0.124695</td>\n      <td>0.104626</td>\n      <td>0.016593</td>\n      <td>0.781424</td>\n      <td>0.804966</td>\n      <td>0.743567</td>\n      <td>0.748063</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.231216</td>\n      <td>0.257020</td>\n      <td>0.628362</td>\n      <td>0.624361</td>\n      <td>0.028580</td>\n      <td>-0.014859</td>\n      <td>0.900432</td>\n      <td>0.906686</td>\n      <td>0.895983</td>\n      <td>0.896844</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>45976</th>\n      <td>0.125274</td>\n      <td>0.411201</td>\n      <td>0.220533</td>\n      <td>0.791142</td>\n      <td>-0.085762</td>\n      <td>0.975237</td>\n      <td>0.750785</td>\n      <td>0.620295</td>\n      <td>0.711580</td>\n      <td>0.536891</td>\n    </tr>\n    <tr>\n      <th>45977</th>\n      <td>0.002032</td>\n      <td>-0.000362</td>\n      <td>-0.025511</td>\n      <td>-0.028797</td>\n      <td>-0.149126</td>\n      <td>-0.146241</td>\n      <td>0.000000</td>\n      <td>0.958179</td>\n      <td>0.000000</td>\n      <td>0.934212</td>\n    </tr>\n    <tr>\n      <th>45978</th>\n      <td>0.252332</td>\n      <td>0.242113</td>\n      <td>0.477212</td>\n      <td>0.443996</td>\n      <td>0.305100</td>\n      <td>0.303021</td>\n      <td>0.827993</td>\n      <td>0.850761</td>\n      <td>0.818516</td>\n      <td>0.865889</td>\n    </tr>\n    <tr>\n      <th>45979</th>\n      <td>0.696520</td>\n      <td>0.173814</td>\n      <td>0.974114</td>\n      <td>0.285210</td>\n      <td>0.997904</td>\n      <td>-0.013173</td>\n      <td>0.966053</td>\n      <td>0.887665</td>\n      <td>0.946798</td>\n      <td>0.872915</td>\n    </tr>\n    <tr>\n      <th>45980</th>\n      <td>0.063313</td>\n      <td>0.165001</td>\n      <td>0.090970</td>\n      <td>0.261274</td>\n      <td>-0.046661</td>\n      <td>0.197984</td>\n      <td>0.838114</td>\n      <td>0.803018</td>\n      <td>0.805370</td>\n      <td>0.756684</td>\n    </tr>\n  </tbody>\n</table>\n<p>45981 rows × 10 columns</p>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"#feature_selection = SelectKBest(score_func = chi2, k=6)\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfeature_selection = SelectKBest(score_func=f_classif, k=6)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T10:33:24.041033Z","iopub.status.idle":"2025-09-20T10:33:24.041256Z","shell.execute_reply.started":"2025-09-20T10:33:24.041149Z","shell.execute_reply":"2025-09-20T10:33:24.041159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedded_df.to_csv(\"X_final_step_by_step.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T10:46:40.820359Z","iopub.execute_input":"2025-09-20T10:46:40.820904Z","iopub.status.idle":"2025-09-20T10:46:41.291783Z","shell.execute_reply.started":"2025-09-20T10:46:40.820879Z","shell.execute_reply":"2025-09-20T10:46:41.291181Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"model = XGBClassifier(\n    objective=\"multi:softprob\",  \n    num_class=3,                  \n    eval_metric=\"mlogloss\",       \n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T10:46:47.305301Z","iopub.execute_input":"2025-09-20T10:46:47.305866Z","iopub.status.idle":"2025-09-20T10:46:47.309488Z","shell.execute_reply.started":"2025-09-20T10:46:47.305842Z","shell.execute_reply":"2025-09-20T10:46:47.308773Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"X_final=pd.read_csv(\"X_final_step_by_step.csv\")\nX_final.columns = [\"drop_col\",\n    \"raw_sim_a\", \"raw_sim_b\",\n    \"mean_sim_a\", \"mean_sim_b\",\n    \"median_sim_a\", \"median_sim_b\",\n    \"tfidf_mean_sim_a\", \"tfidf_mean_sim_b\",\n    \"tfidf_median_sim_a\", \"tfidf_median_sim_b\"\n]\n\ny_final=y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:09:34.574707Z","iopub.execute_input":"2025-09-20T11:09:34.575580Z","iopub.status.idle":"2025-09-20T11:09:34.632204Z","shell.execute_reply.started":"2025-09-20T11:09:34.575546Z","shell.execute_reply":"2025-09-20T11:09:34.631638Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"X_final.drop(columns=[\"drop_col\"],inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:09:45.324526Z","iopub.execute_input":"2025-09-20T11:09:45.324778Z","iopub.status.idle":"2025-09-20T11:09:45.329551Z","shell.execute_reply.started":"2025-09-20T11:09:45.324760Z","shell.execute_reply":"2025-09-20T11:09:45.328837Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"X_final[['commword a','commword b']]=common_words_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:09:47.988273Z","iopub.execute_input":"2025-09-20T11:09:47.988558Z","iopub.status.idle":"2025-09-20T11:09:47.993469Z","shell.execute_reply.started":"2025-09-20T11:09:47.988536Z","shell.execute_reply":"2025-09-20T11:09:47.992829Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"X_final[X_train.columns]=X_train.values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:09:52.488836Z","iopub.execute_input":"2025-09-20T11:09:52.489109Z","iopub.status.idle":"2025-09-20T11:09:52.513622Z","shell.execute_reply.started":"2025-09-20T11:09:52.489088Z","shell.execute_reply":"2025-09-20T11:09:52.513054Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"X_final['y-train']=y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:09:54.267850Z","iopub.execute_input":"2025-09-20T11:09:54.268093Z","iopub.status.idle":"2025-09-20T11:09:54.272192Z","shell.execute_reply.started":"2025-09-20T11:09:54.268076Z","shell.execute_reply":"2025-09-20T11:09:54.271583Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"X_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:10:01.614202Z","iopub.execute_input":"2025-09-20T11:10:01.614866Z","iopub.status.idle":"2025-09-20T11:10:01.630924Z","shell.execute_reply.started":"2025-09-20T11:10:01.614842Z","shell.execute_reply":"2025-09-20T11:10:01.630234Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"       raw_sim_a  raw_sim_b  mean_sim_a  mean_sim_b  median_sim_a  \\\n0       0.048206   0.266984    0.096257    0.553622     -0.054856   \n1       0.092558   0.088836    0.232275    0.206772      0.644696   \n2       0.362552   0.159841    0.652014    0.297614      0.853396   \n3       0.230009   0.077728    0.387369    0.124695      0.104626   \n4       0.231216   0.257020    0.628362    0.624361      0.028580   \n...          ...        ...         ...         ...           ...   \n45976   0.125274   0.411201    0.220533    0.791142     -0.085762   \n45977   0.002032  -0.000362   -0.025511   -0.028797     -0.149126   \n45978   0.252332   0.242113    0.477212    0.443996      0.305100   \n45979   0.696520   0.173814    0.974114    0.285210      0.997904   \n45980   0.063313   0.165001    0.090970    0.261274     -0.046661   \n\n       median_sim_b  tfidf_mean_sim_a  tfidf_mean_sim_b  tfidf_median_sim_a  \\\n0          0.424662          0.864312          0.919458            0.829818   \n1          0.622386          0.928233          0.867967            0.909265   \n2         -0.019965          0.786856          0.750248            0.743564   \n3          0.016593          0.781424          0.804966            0.743567   \n4         -0.014859          0.900432          0.906686            0.895983   \n...             ...               ...               ...                 ...   \n45976      0.975237          0.750785          0.620295            0.711580   \n45977     -0.146241          0.000000          0.958179            0.000000   \n45978      0.303021          0.827993          0.850761            0.818516   \n45979     -0.013173          0.966053          0.887665            0.946798   \n45980      0.197984          0.838114          0.803018            0.805370   \n\n       tfidf_median_sim_b  commword a  commword b          id  \\\n0                0.901671           5           4  2245506736   \n1                0.827335          10          11   656268525   \n2                0.703505           8           5  2266396135   \n3                0.748063           6           6  2497468695   \n4                0.896844          40          23  1679992057   \n...                   ...         ...         ...         ...   \n45976            0.536891           4           4  4061186081   \n45977            0.934212           0           2  2840606458   \n45978            0.865889           2           2    62750940   \n45979            0.872915           4           4  1180878567   \n45980            0.756684           0           0  4218036675   \n\n                                                  prompt  \\\n0      [\"can you give me some escape room themes with...   \n1      [\"Why did digital electronics switch from 5 vo...   \n2      [\"as an acclaimed and visionary author punishe...   \n3      [\"\\n\\nWrite a micro editor plugin that shows t...   \n4      [\"1. 19 year old Emese who is thin (165cm 48kg...   \n...                                                  ...   \n45976         [\"mega-sena brazilian lottery statistics\"]   \n45977                     [\"Draw a dog using ASCII art\"]   \n45978                          [\"How to wear a bikini?\"]   \n45979  [\"Can you tell me what was going on with all t...   \n45980                          [\"SHOW ME WHAT YOU GOT!\"]   \n\n                                              response_a  \\\n0      [\"Certainly! Here are several escape room them...   \n1      [\"There are a few key reasons why digital elec...   \n2      [\"As an acclaimed and visionary author, Elieze...   \n3      [\"Here is Python code for a basic micro editor...   \n4      [\"Certainly, let's address each of your points...   \n...                                                  ...   \n45976  [\"I do not have access to real-time mega-sena ...   \n45977                        [\"\\/_\\/\\\\  \\n( o.o )\\n> ^\"]   \n45978  [\"Wearing a bikini can be a fun and comfortabl...   \n45979  [\"The direwolves, which are also known as dire...   \n45980  [\"Of course! As an AI, I can assist with a var...   \n\n                                              response_b  \\\n0      [\"Here are some fun and immersive escape room ...   \n1      [\"The shift from 5 volt logic to 3 volt logic ...   \n2      [\"Eliezer Yudkowsky is often referred to as th...   \n3      [\"Micro is a modern and intuitive terminal-bas...   \n4      [\"1. From the description you provided, it see...   \n...                                                  ...   \n45976  [\"Mega-Sena is a Brazilian lottery game that o...   \n45977  [\"Here is a dog drawn with ASCII art:\\n\\n     ...   \n45978  [\"Here are the basic steps to wear a bikini:\\n...   \n45979  [\"Sure! On the night of the Red Wedding, all o...   \n45980  [\"I'm a large language model, so I don't have ...   \n\n                                            prompt_clean  \\\n0      [\"can you give me some escape room themes with...   \n1      [\"Why did digital electronics switch from 5 vo...   \n2      [\"as an acclaimed and visionary author punishe...   \n3      [\"\\n\\nWrite a micro editor plugin that shows t...   \n4      [\"1. 19 year old Emese who is thin (165cm 48kg...   \n...                                                  ...   \n45976         [\"mega-sena brazilian lottery statistics\"]   \n45977                     [\"Draw a dog using ASCII art\"]   \n45978                          [\"How to wear a bikini?\"]   \n45979  [\"Can you tell me what was going on with all t...   \n45980                          [\"SHOW ME WHAT YOU GOT!\"]   \n\n                                        response_a_clean  \\\n0      [\"Certainly! Here are several escape room them...   \n1      [\"There are a few key reasons why digital elec...   \n2      [\"As an acclaimed and visionary author, Elieze...   \n3      [\"Here is Python code for a basic micro editor...   \n4      [\"Certainly, let's address each of your points...   \n...                                                  ...   \n45976  [\"I do not have access to real-time mega-sena ...   \n45977                        [\"\\/_\\/\\\\  \\n( o.o )\\n> ^\"]   \n45978  [\"Wearing a bikini can be a fun and comfortabl...   \n45979  [\"The direwolves, which are also known as dire...   \n45980  [\"Of course! As an AI, I can assist with a var...   \n\n                                        response_b_clean  y-train  \n0      [\"Here are some fun and immersive escape room ...        2  \n1      [\"The shift from 5 volt logic to 3 volt logic ...        1  \n2      [\"Eliezer Yudkowsky is often referred to as th...        1  \n3      [\"Micro is a modern and intuitive terminal-bas...        2  \n4      [\"1. From the description you provided, it see...        1  \n...                                                  ...      ...  \n45976  [\"Mega-Sena is a Brazilian lottery game that o...        2  \n45977  [\"Here is a dog drawn with ASCII art:\\n\\n     ...        2  \n45978  [\"Here are the basic steps to wear a bikini:\\n...        1  \n45979  [\"Sure! On the night of the Red Wedding, all o...        1  \n45980  [\"I'm a large language model, so I don't have ...        0  \n\n[45981 rows x 20 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw_sim_a</th>\n      <th>raw_sim_b</th>\n      <th>mean_sim_a</th>\n      <th>mean_sim_b</th>\n      <th>median_sim_a</th>\n      <th>median_sim_b</th>\n      <th>tfidf_mean_sim_a</th>\n      <th>tfidf_mean_sim_b</th>\n      <th>tfidf_median_sim_a</th>\n      <th>tfidf_median_sim_b</th>\n      <th>commword a</th>\n      <th>commword b</th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>prompt_clean</th>\n      <th>response_a_clean</th>\n      <th>response_b_clean</th>\n      <th>y-train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.048206</td>\n      <td>0.266984</td>\n      <td>0.096257</td>\n      <td>0.553622</td>\n      <td>-0.054856</td>\n      <td>0.424662</td>\n      <td>0.864312</td>\n      <td>0.919458</td>\n      <td>0.829818</td>\n      <td>0.901671</td>\n      <td>5</td>\n      <td>4</td>\n      <td>2245506736</td>\n      <td>[\"can you give me some escape room themes with...</td>\n      <td>[\"Certainly! Here are several escape room them...</td>\n      <td>[\"Here are some fun and immersive escape room ...</td>\n      <td>[\"can you give me some escape room themes with...</td>\n      <td>[\"Certainly! Here are several escape room them...</td>\n      <td>[\"Here are some fun and immersive escape room ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.092558</td>\n      <td>0.088836</td>\n      <td>0.232275</td>\n      <td>0.206772</td>\n      <td>0.644696</td>\n      <td>0.622386</td>\n      <td>0.928233</td>\n      <td>0.867967</td>\n      <td>0.909265</td>\n      <td>0.827335</td>\n      <td>10</td>\n      <td>11</td>\n      <td>656268525</td>\n      <td>[\"Why did digital electronics switch from 5 vo...</td>\n      <td>[\"There are a few key reasons why digital elec...</td>\n      <td>[\"The shift from 5 volt logic to 3 volt logic ...</td>\n      <td>[\"Why did digital electronics switch from 5 vo...</td>\n      <td>[\"There are a few key reasons why digital elec...</td>\n      <td>[\"The shift from 5 volt logic to 3 volt logic ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.362552</td>\n      <td>0.159841</td>\n      <td>0.652014</td>\n      <td>0.297614</td>\n      <td>0.853396</td>\n      <td>-0.019965</td>\n      <td>0.786856</td>\n      <td>0.750248</td>\n      <td>0.743564</td>\n      <td>0.703505</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2266396135</td>\n      <td>[\"as an acclaimed and visionary author punishe...</td>\n      <td>[\"As an acclaimed and visionary author, Elieze...</td>\n      <td>[\"Eliezer Yudkowsky is often referred to as th...</td>\n      <td>[\"as an acclaimed and visionary author punishe...</td>\n      <td>[\"As an acclaimed and visionary author, Elieze...</td>\n      <td>[\"Eliezer Yudkowsky is often referred to as th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.230009</td>\n      <td>0.077728</td>\n      <td>0.387369</td>\n      <td>0.124695</td>\n      <td>0.104626</td>\n      <td>0.016593</td>\n      <td>0.781424</td>\n      <td>0.804966</td>\n      <td>0.743567</td>\n      <td>0.748063</td>\n      <td>6</td>\n      <td>6</td>\n      <td>2497468695</td>\n      <td>[\"\\n\\nWrite a micro editor plugin that shows t...</td>\n      <td>[\"Here is Python code for a basic micro editor...</td>\n      <td>[\"Micro is a modern and intuitive terminal-bas...</td>\n      <td>[\"\\n\\nWrite a micro editor plugin that shows t...</td>\n      <td>[\"Here is Python code for a basic micro editor...</td>\n      <td>[\"Micro is a modern and intuitive terminal-bas...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.231216</td>\n      <td>0.257020</td>\n      <td>0.628362</td>\n      <td>0.624361</td>\n      <td>0.028580</td>\n      <td>-0.014859</td>\n      <td>0.900432</td>\n      <td>0.906686</td>\n      <td>0.895983</td>\n      <td>0.896844</td>\n      <td>40</td>\n      <td>23</td>\n      <td>1679992057</td>\n      <td>[\"1. 19 year old Emese who is thin (165cm 48kg...</td>\n      <td>[\"Certainly, let's address each of your points...</td>\n      <td>[\"1. From the description you provided, it see...</td>\n      <td>[\"1. 19 year old Emese who is thin (165cm 48kg...</td>\n      <td>[\"Certainly, let's address each of your points...</td>\n      <td>[\"1. From the description you provided, it see...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>45976</th>\n      <td>0.125274</td>\n      <td>0.411201</td>\n      <td>0.220533</td>\n      <td>0.791142</td>\n      <td>-0.085762</td>\n      <td>0.975237</td>\n      <td>0.750785</td>\n      <td>0.620295</td>\n      <td>0.711580</td>\n      <td>0.536891</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4061186081</td>\n      <td>[\"mega-sena brazilian lottery statistics\"]</td>\n      <td>[\"I do not have access to real-time mega-sena ...</td>\n      <td>[\"Mega-Sena is a Brazilian lottery game that o...</td>\n      <td>[\"mega-sena brazilian lottery statistics\"]</td>\n      <td>[\"I do not have access to real-time mega-sena ...</td>\n      <td>[\"Mega-Sena is a Brazilian lottery game that o...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>45977</th>\n      <td>0.002032</td>\n      <td>-0.000362</td>\n      <td>-0.025511</td>\n      <td>-0.028797</td>\n      <td>-0.149126</td>\n      <td>-0.146241</td>\n      <td>0.000000</td>\n      <td>0.958179</td>\n      <td>0.000000</td>\n      <td>0.934212</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2840606458</td>\n      <td>[\"Draw a dog using ASCII art\"]</td>\n      <td>[\"\\/_\\/\\\\  \\n( o.o )\\n&gt; ^\"]</td>\n      <td>[\"Here is a dog drawn with ASCII art:\\n\\n     ...</td>\n      <td>[\"Draw a dog using ASCII art\"]</td>\n      <td>[\"\\/_\\/\\\\  \\n( o.o )\\n&gt; ^\"]</td>\n      <td>[\"Here is a dog drawn with ASCII art:\\n\\n     ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>45978</th>\n      <td>0.252332</td>\n      <td>0.242113</td>\n      <td>0.477212</td>\n      <td>0.443996</td>\n      <td>0.305100</td>\n      <td>0.303021</td>\n      <td>0.827993</td>\n      <td>0.850761</td>\n      <td>0.818516</td>\n      <td>0.865889</td>\n      <td>2</td>\n      <td>2</td>\n      <td>62750940</td>\n      <td>[\"How to wear a bikini?\"]</td>\n      <td>[\"Wearing a bikini can be a fun and comfortabl...</td>\n      <td>[\"Here are the basic steps to wear a bikini:\\n...</td>\n      <td>[\"How to wear a bikini?\"]</td>\n      <td>[\"Wearing a bikini can be a fun and comfortabl...</td>\n      <td>[\"Here are the basic steps to wear a bikini:\\n...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>45979</th>\n      <td>0.696520</td>\n      <td>0.173814</td>\n      <td>0.974114</td>\n      <td>0.285210</td>\n      <td>0.997904</td>\n      <td>-0.013173</td>\n      <td>0.966053</td>\n      <td>0.887665</td>\n      <td>0.946798</td>\n      <td>0.872915</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1180878567</td>\n      <td>[\"Can you tell me what was going on with all t...</td>\n      <td>[\"The direwolves, which are also known as dire...</td>\n      <td>[\"Sure! On the night of the Red Wedding, all o...</td>\n      <td>[\"Can you tell me what was going on with all t...</td>\n      <td>[\"The direwolves, which are also known as dire...</td>\n      <td>[\"Sure! On the night of the Red Wedding, all o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>45980</th>\n      <td>0.063313</td>\n      <td>0.165001</td>\n      <td>0.090970</td>\n      <td>0.261274</td>\n      <td>-0.046661</td>\n      <td>0.197984</td>\n      <td>0.838114</td>\n      <td>0.803018</td>\n      <td>0.805370</td>\n      <td>0.756684</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4218036675</td>\n      <td>[\"SHOW ME WHAT YOU GOT!\"]</td>\n      <td>[\"Of course! As an AI, I can assist with a var...</td>\n      <td>[\"I'm a large language model, so I don't have ...</td>\n      <td>[\"SHOW ME WHAT YOU GOT!\"]</td>\n      <td>[\"Of course! As an AI, I can assist with a var...</td>\n      <td>[\"I'm a large language model, so I don't have ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>45981 rows × 20 columns</p>\n</div>"},"metadata":{}}],"execution_count":65},{"cell_type":"markdown","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Load the embedded features DataFrame\nembedded_df = pd.read_pickle(\"embedded_features.pkl.chunk_0.pkl\")\n\n# Extract the text from the prompt embeddings\ntexts = [x['text'] for x in embedded_df['prompt_embed']]\n\n# Fit TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit(texts)\n\n# Identify rows with all-zero TF-IDF weights\nzero_weight_rows = []\nfor i, text in enumerate(texts):\n    tfidf_vector = vectorizer.transform([text]).toarray()[0]\n    if np.sum(tfidf_vector) == 0:\n        tokens = vectorizer.build_tokenizer()(text.lower())\n        missing_tokens = [t for t in tokens if t not in vectorizer.vocabulary_]\n        zero_weight_rows.append({\n            \"index\": i,\n            \"text\": text,\n            \"tokens\": tokens,\n            \"missing_tokens\": missing_tokens\n        })\n\n# Print results\nprint(f\"Found {len(zero_weight_rows)} rows with all zero TF-IDF weights.\\n\")\nfor row in zero_weight_rows[:5]:  # Show only first 5 for brevity\n    print(f\"Row {row['index']}\\nText: {row['text']}\\nTokens: {row['tokens']}\\nMissing from vocab: {row['missing_tokens']}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2025-09-20T10:46:47.396503Z","iopub.execute_input":"2025-09-20T10:46:47.396668Z","iopub.status.idle":"2025-09-20T10:46:47.431356Z","shell.execute_reply.started":"2025-09-20T10:46:47.396655Z","shell.execute_reply":"2025-09-20T10:46:47.430360Z"},"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"\n# Load the original training data (first 10,000 rows as used in the notebook)\ndf_train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\").head(10000)\n\n# Select TF-IDF weighted embedding columns (columns 6 to 9)\ntfidf_cols = X_final.iloc[:, 6:10]\n\n# Identify rows where all TF-IDF columns are zero\nzero_mask = (tfidf_cols == 0).all(axis=1)\nzero_indices = zero_mask[zero_mask].index\n\n# Show corresponding prompt text from df_train\nzero_prompts = df_train.loc[zero_indices, \"prompt\"]\n\n# Report\nprint(f\"Found {len(zero_prompts)} rows where all TF-IDF weighted embedding columns are zero.\\n\")\nprint(\"First few prompt examples:\")\nprint(zero_prompts.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T10:46:47.431727Z","iopub.status.idle":"2025-09-20T10:46:47.431952Z","shell.execute_reply.started":"2025-09-20T10:46:47.431846Z","shell.execute_reply":"2025-09-20T10:46:47.431856Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nX_final_run = X_final[[\n    \"raw_sim_a\", \"raw_sim_b\",\n    \"mean_sim_a\", \"mean_sim_b\",\n    \"median_sim_a\", \"median_sim_b\",\n    \"tfidf_mean_sim_a\", \"tfidf_mean_sim_b\",\n    \"tfidf_median_sim_a\", \"tfidf_median_sim_b\",\n    \"commword a\", \"commword b\"\n]]\n\nscores = cross_val_score(model, X_final_run, y_final, cv=3, scoring=\"accuracy\")\nprint(\"Cross-validated Accuracy:\", scores.mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:11:34.980364Z","iopub.execute_input":"2025-09-20T11:11:34.981069Z","iopub.status.idle":"2025-09-20T11:11:42.710843Z","shell.execute_reply.started":"2025-09-20T11:11:34.981042Z","shell.execute_reply":"2025-09-20T11:11:42.710047Z"}},"outputs":[{"name":"stdout","text":"Cross-validated Accuracy: 0.4130401687653596\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"model.fit(X_final_run, y_final)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:12:15.779123Z","iopub.execute_input":"2025-09-20T11:12:15.779822Z","iopub.status.idle":"2025-09-20T11:12:18.846083Z","shell.execute_reply.started":"2025-09-20T11:12:15.779799Z","shell.execute_reply":"2025-09-20T11:12:18.845369Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=300,\n              n_jobs=None, num_class=3, num_parallel_tree=None, ...)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=300,\n              n_jobs=None, num_class=3, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=300,\n              n_jobs=None, num_class=3, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"df_test[\"prompt_clean\"] = df_test[\"prompt\"].apply(clean_text_for_common_words)\ndf_test[\"response_a_clean\"] = df_test[\"response_a\"].apply(clean_text_for_common_words)\ndf_test[\"response_b_clean\"] = df_test[\"response_b\"].apply(clean_text_for_common_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T10:47:32.968109Z","iopub.execute_input":"2025-09-20T10:47:32.968856Z","iopub.status.idle":"2025-09-20T10:47:32.977182Z","shell.execute_reply.started":"2025-09-20T10:47:32.968829Z","shell.execute_reply":"2025-09-20T10:47:32.976475Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}