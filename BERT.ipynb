{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f2fff661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2018-152156   11/8/2018  11/11/2018    Second Class    CG-12520   \n",
      "1       2  CA-2018-152156   11/8/2018  11/11/2018    Second Class    CG-12520   \n",
      "2       3  CA-2018-138688   6/12/2018   6/16/2018    Second Class    DV-13045   \n",
      "3       4  US-2017-108966  10/11/2017  10/18/2017  Standard Class    SO-20335   \n",
      "4       5  US-2017-108966  10/11/2017  10/18/2017  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment Country/Region             City  ...  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "1      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "\n",
      "  Postal Code  Region       Product ID         Category Sub-Category  \\\n",
      "0     42420.0   South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1     42420.0   South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2     90036.0    West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3     33311.0   South  FUR-TA-10000577        Furniture       Tables   \n",
      "4     33311.0   South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales  Quantity  \\\n",
      "0                  Bush Somerset Collection Bookcase  261.9600         2   \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n",
      "\n",
      "   Discount    Profit  \n",
      "0      0.00   41.9136  \n",
      "1      0.00  219.5820  \n",
      "2      0.00    6.8714  \n",
      "3      0.45 -383.0310  \n",
      "4      0.20    2.5164  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "import pandas as pd\n",
    "# Detect the encoding of the file\n",
    "with open(\"orders.csv\", \"rb\") as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Read the CSV file with the detected encoding\n",
    "order_data = pd.read_csv(\"orders.csv\", encoding=result['encoding'])\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(order_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ec0dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " Row ID             0\n",
      "Order ID           0\n",
      "Order Date         0\n",
      "Ship Date          0\n",
      "Ship Mode          0\n",
      "Customer ID        0\n",
      "Customer Name      0\n",
      "Segment            0\n",
      "Country/Region     0\n",
      "City               0\n",
      "State              0\n",
      "Postal Code       11\n",
      "Region             0\n",
      "Product ID         0\n",
      "Category           0\n",
      "Sub-Category       0\n",
      "Product Name       0\n",
      "Sales              0\n",
      "Quantity           0\n",
      "Discount           0\n",
      "Profit             0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 0\n",
      "Data types of each column:\n",
      " Row ID              int64\n",
      "Order ID           object\n",
      "Order Date         object\n",
      "Ship Date          object\n",
      "Ship Mode          object\n",
      "Customer ID        object\n",
      "Customer Name      object\n",
      "Segment            object\n",
      "Country/Region     object\n",
      "City               object\n",
      "State              object\n",
      "Postal Code       float64\n",
      "Region             object\n",
      "Product ID         object\n",
      "Category           object\n",
      "Sub-Category       object\n",
      "Product Name       object\n",
      "Sales             float64\n",
      "Quantity            int64\n",
      "Discount          float64\n",
      "Profit            float64\n",
      "dtype: object\n",
      "Basic statistics of the dataset:\n",
      "             Row ID   Postal Code         Sales     Quantity     Discount  \\\n",
      "count  9994.000000   9983.000000   9994.000000  9994.000000  9994.000000   \n",
      "mean   4997.500000  55245.233297    229.858001     3.789574     0.156203   \n",
      "std    2885.163629  32038.715955    623.245101     2.225110     0.206452   \n",
      "min       1.000000   1040.000000      0.444000     1.000000     0.000000   \n",
      "25%    2499.250000  23223.000000     17.280000     2.000000     0.000000   \n",
      "50%    4997.500000  57103.000000     54.490000     3.000000     0.200000   \n",
      "75%    7495.750000  90008.000000    209.940000     5.000000     0.200000   \n",
      "max    9994.000000  99301.000000  22638.480000    14.000000     0.800000   \n",
      "\n",
      "            Profit  \n",
      "count  9994.000000  \n",
      "mean     28.656896  \n",
      "std     234.260108  \n",
      "min   -6599.978000  \n",
      "25%       1.728750  \n",
      "50%       8.666500  \n",
      "75%      29.364000  \n",
      "max    8399.976000  \n",
      "Current length of the dataset: 9994\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = order_data.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = order_data.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicate_rows)\n",
    "\n",
    "# Optionally, remove duplicate rows\n",
    "order_data = order_data.drop_duplicates()\n",
    "\n",
    "# Check data types\n",
    "data_types = order_data.dtypes\n",
    "print(\"Data types of each column:\\n\", data_types)\n",
    "\n",
    "# Generate basic statistics\n",
    "statistics = order_data.describe()\n",
    "print(\"Basic statistics of the dataset:\\n\", statistics)\n",
    "\n",
    "current_length = len(order_data)\n",
    "print(f\"Current length of the dataset: {current_length}\")\n",
    "\n",
    "order_data=order_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7f79a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current length of the dataset: 9983\n"
     ]
    }
   ],
   "source": [
    "current_length = len(order_data)\n",
    "print(f\"Current length of the dataset: {current_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c2bbbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_data=order_data[['Segment','Category','Sub-Category','Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db3cc628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>261.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>731.9400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Corporate</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>14.6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>957.5775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>22.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>25.2480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>91.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Phones</td>\n",
       "      <td>258.5760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Paper</td>\n",
       "      <td>29.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>243.1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9983 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Segment         Category Sub-Category     Sales\n",
       "0      Consumer        Furniture    Bookcases  261.9600\n",
       "1      Consumer        Furniture       Chairs  731.9400\n",
       "2     Corporate  Office Supplies       Labels   14.6200\n",
       "3      Consumer        Furniture       Tables  957.5775\n",
       "4      Consumer  Office Supplies      Storage   22.3680\n",
       "...         ...              ...          ...       ...\n",
       "9989   Consumer        Furniture  Furnishings   25.2480\n",
       "9990   Consumer        Furniture  Furnishings   91.9600\n",
       "9991   Consumer       Technology       Phones  258.5760\n",
       "9992   Consumer  Office Supplies        Paper   29.6000\n",
       "9993   Consumer  Office Supplies   Appliances  243.1600\n",
       "\n",
       "[9983 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "48339e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./BERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model from the local directory\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./BERT\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./BERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7a4983d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u013709\\AppData\\Local\\Temp\\ipykernel_4164\\2264562095.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  order_data['text'] = order_data.apply(table_to_text, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Sales</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>261.9600</td>\n",
       "      <td>Segment: Consumer | Category: Furniture | Sub-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>731.9400</td>\n",
       "      <td>Segment: Consumer | Category: Furniture | Sub-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Corporate</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>14.6200</td>\n",
       "      <td>Segment: Corporate | Category: Office Supplies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>957.5775</td>\n",
       "      <td>Segment: Consumer | Category: Furniture | Sub-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>22.3680</td>\n",
       "      <td>Segment: Consumer | Category: Office Supplies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Envelopes</td>\n",
       "      <td>105.4200</td>\n",
       "      <td>Segment: Consumer | Category: Office Supplies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>119.6160</td>\n",
       "      <td>Segment: Consumer | Category: Office Supplies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>255.7600</td>\n",
       "      <td>Segment: Consumer | Category: Furniture | Sub-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>241.5680</td>\n",
       "      <td>Segment: Consumer | Category: Furniture | Sub-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>Segment: Consumer | Category: Furniture | Sub-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Segment         Category Sub-Category     Sales  \\\n",
       "0     Consumer        Furniture    Bookcases  261.9600   \n",
       "1     Consumer        Furniture       Chairs  731.9400   \n",
       "2    Corporate  Office Supplies       Labels   14.6200   \n",
       "3     Consumer        Furniture       Tables  957.5775   \n",
       "4     Consumer  Office Supplies      Storage   22.3680   \n",
       "..         ...              ...          ...       ...   \n",
       "495   Consumer  Office Supplies    Envelopes  105.4200   \n",
       "496   Consumer  Office Supplies      Binders  119.6160   \n",
       "497   Consumer        Furniture  Furnishings  255.7600   \n",
       "498   Consumer        Furniture       Chairs  241.5680   \n",
       "499   Consumer        Furniture  Furnishings   69.3000   \n",
       "\n",
       "                                                  text  \n",
       "0    Segment: Consumer | Category: Furniture | Sub-...  \n",
       "1    Segment: Consumer | Category: Furniture | Sub-...  \n",
       "2    Segment: Corporate | Category: Office Supplies...  \n",
       "3    Segment: Consumer | Category: Furniture | Sub-...  \n",
       "4    Segment: Consumer | Category: Office Supplies ...  \n",
       "..                                                 ...  \n",
       "495  Segment: Consumer | Category: Office Supplies ...  \n",
       "496  Segment: Consumer | Category: Office Supplies ...  \n",
       "497  Segment: Consumer | Category: Furniture | Sub-...  \n",
       "498  Segment: Consumer | Category: Furniture | Sub-...  \n",
       "499  Segment: Consumer | Category: Furniture | Sub-...  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_data = order_data.head(500)\n",
    "\n",
    "# Convert your table data to text format\n",
    "def table_to_text(row):\n",
    "    return \" | \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
    "\n",
    "order_data['text'] = order_data.apply(table_to_text, axis=1)\n",
    "order_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a6a2afe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165    Consumer\n",
       "Name: Segment, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_data[order_data['Sales']==max(order_data['Sales'])]['Segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5168f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicted class: 1\n",
      "Final predicted label: Corporate\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question, context):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "def process_table_in_chunks(question, dataframe, chunk_size=50):\n",
    "    results = []\n",
    "    for start in range(0, len(dataframe), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = dataframe.iloc[start:end]\n",
    "        context = \" \".join(chunk['text'].tolist())\n",
    "        predicted_class = ask_question(question, context)\n",
    "        results.append(predicted_class)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "question = \"What Segment has the highest sum of sales?\"\n",
    "results = process_table_in_chunks(question, order_data, chunk_size=50)\n",
    "\n",
    "# Aggregate results (this is a simple example, adjust based on your needs)\n",
    "final_prediction = max(set(results), key=results.count)\n",
    "print(f\"Final predicted class: {final_prediction}\")\n",
    "\n",
    "# Mapping the predicted class to a label\n",
    "label_map = {0: \"Consumer\", 1: \"Corporate\", 2: \"Home Office\"}  # Adjust this based on your actual labels\n",
    "predicted_label = label_map.get(final_prediction, \"Unknown\")\n",
    "print(f\"Final predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4101297a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer</td>\n",
       "      <td>69921.5396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Corporate</td>\n",
       "      <td>32323.8468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home Office</td>\n",
       "      <td>27180.9297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Segment       Sales\n",
       "0     Consumer  69921.5396\n",
       "1    Corporate  32323.8468\n",
       "2  Home Office  27180.9297"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4721c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "Predicted label: Corporate\n"
     ]
    }
   ],
   "source": [
    "summary = order_data.groupby('Segment')['Sales'].sum().reset_index()\n",
    "summary_text = \" | \".join([f\"{row['Segment']}: {row['Sales']}\" for _, row in summary.iterrows()])\n",
    "\n",
    "# Function to ask a question using the model\n",
    "def ask_question(question, context):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "question = \"What Segment has the lowest sales?\"\n",
    "predicted_class = ask_question(question, summary_text)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "# Mapping the predicted class to a label\n",
    "label_map = {0: \"Consumer\", 1: \"Corporate\", 2: \"Home Office\"}  # Adjust this based on your actual labels\n",
    "predicted_label = label_map.get(predicted_class, \"Unknown\")\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8c1cd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Segment: Consumer | Category: Furniture | Sub-Category: Bookcases | Sales: 261.96 | text: Segment: Consumer | Category: Furniture | Sub-Category: Bookcases | Sales: 261.96 | text: Segment: Consumer | Category: Furniture | Sub-Category: Bookcases | Sales: 261.96 | text: Segment: Consumer | Category: Furniture | Sub-Category: Bookcases | Sales: 261.96 | text: Segment: Consumer | Category: Furniture | Sub-Category: Bookcases | Sales: 261.96'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5b7b59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5649, -5.4607]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cb5a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "question = \"What segment has the highest sum of sales?\"\n",
    "context = order_data.iloc[0]['text']\n",
    "predicted_class = ask_question(question, context)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59f63600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'BERT',\n",
       " 'BERT.ipynb',\n",
       " 'fine-tuned-tapas',\n",
       " 'fine_tuned_tabert',\n",
       " 'fraudTest.csv',\n",
       " 'fraudTrain.csv',\n",
       " 'Fraudulent transactions-Bkp02Sept2024.ipynb',\n",
       " 'Fraudulent transactions-Copy1.ipynb',\n",
       " 'Fraudulent transactions.ipynb',\n",
       " 'fraud_detection_nn.pth',\n",
       " 'fraud_detection_nn_es.pth',\n",
       " 'LIME_implementation',\n",
       " 'LIME_implementation.zip',\n",
       " 'logs',\n",
       " 'orders.csv',\n",
       " 'orders_updated.csv',\n",
       " 'proj2',\n",
       " 'results',\n",
       " 'Tapas',\n",
       " 'Tapas.ipynb',\n",
       " 'tokenized_data.pt',\n",
       " 'train_df.csv',\n",
       " 'train_df_new.csv',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1c36e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./BERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 82\u001b[0m\n\u001b[0;32m     74\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     75\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     76\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     77\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     78\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[0;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine-tuned-tabert\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1815\u001b[0m ):\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   2656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2657\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2678\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2679\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:976\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 976\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    977\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Download and save the model locally\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "# Convert your table data to text format\n",
    "def table_to_text(row):\n",
    "    return \" | \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
    "\n",
    "order_data['text'] = order_data.apply(table_to_text, axis=1)\n",
    "\n",
    "# Prepare the dataset for TaBERT\n",
    "class TableDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, questions, answers):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        input_text = f\"question: {question} context: {row['text']}\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        labels = self.tokenizer(answer, return_tensors=\"pt\", padding=\"max_length\", truncation=True).input_ids\n",
    "        inputs['labels'] = labels\n",
    "        return inputs\n",
    "\n",
    "def adjust_list_length(dataframe, questions, answers):\n",
    "    # Calculate the difference in length\n",
    "    diff = len(dataframe) - len(questions)\n",
    "    \n",
    "    # If questions list is shorter, extend it with the last question\n",
    "    if diff > 0:\n",
    "        questions.extend([questions[-1]] * diff)\n",
    "        answers.extend([answers[-1]] * diff)\n",
    "    \n",
    "    return questions, answers\n",
    "\n",
    "# Example questions and answers\n",
    "questions = [\"What segment has the highest sales?\"]\n",
    "answers = [\"Consumer\"]\n",
    "\n",
    "# Adjust the length of questions and answers lists\n",
    "questions, answers = adjust_list_length(order_data, questions, answers)\n",
    "\n",
    "\n",
    "# Create instances of the custom dataset\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./BERT\")\n",
    "train_dataset = TableDataset(order_data, tokenizer, questions, answers)\n",
    "eval_dataset = TableDataset(order_data, tokenizer, questions, answers)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "model = BertForSequenceClassification.from_pretrained(\"./BERT\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-tabert\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-tabert\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine-tuned-tabert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./fine-tuned-tabert\")\n",
    "\n",
    "# Ask Questions\n",
    "input_text = \"question: What segment has the highest sales? context: \" + order_data.iloc[0]['text']\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "outputs = model(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37df6874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./BERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 7:19:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "order_data=order_data.head(500)\n",
    "# Convert your table data to text format\n",
    "def table_to_text(row):\n",
    "    return \" | \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
    "\n",
    "# Ensure order_data is defined and loaded\n",
    "\n",
    "order_data['text'] = order_data.apply(table_to_text, axis=1)\n",
    "\n",
    "# Prepare the dataset for TaBERT\n",
    "class TableDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, questions, answers):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        input_text = f\"question: {question} context: {row['text']}\"\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "\n",
    "        # Convert the answer to an integer label\n",
    "        label = torch.tensor(0 if answer == \"Consumer\" else 1)  # Adjust this logic based on your actual labels\n",
    "\n",
    "        # Return a dictionary with the expected keys\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'token_type_ids': inputs.get('token_type_ids', None).squeeze() if 'token_type_ids' in inputs else None,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "def adjust_list_length(dataframe, questions, answers):\n",
    "    diff = len(dataframe) - len(questions)\n",
    "    if diff > 0:\n",
    "        questions.extend([questions[-1]] * diff)\n",
    "        answers.extend([answers[-1]] * diff)\n",
    "    return questions, answers\n",
    "\n",
    "# Example questions and answers\n",
    "questions = [\"What segment has the highest sales?\"]\n",
    "answers = [\"Consumer\"]\n",
    "\n",
    "# Adjust the length of questions and answers lists\n",
    "questions, answers = adjust_list_length(order_data, questions, answers)\n",
    "\n",
    "# Create instances of the custom dataset\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./BERT\")\n",
    "train_dataset = TableDataset(order_data, tokenizer, questions, answers)\n",
    "eval_dataset = TableDataset(order_data, tokenizer, questions, answers)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "model = BertForSequenceClassification.from_pretrained(\"./BERT\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(\"./fine_tuned_tabert\")\n",
    "    tokenizer.save_pretrained(\"./fine_tuned_tabert\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_tabert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./fine_tuned_tabert\")\n",
    "\n",
    "# Ask Questions\n",
    "input_text = \"question: What segment has the highest sales? context: \" + order_data.iloc[0]['text']\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33c0dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n",
      "Predicted label: Consumer\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_tabert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./fine_tuned_tabert\")\n",
    "\n",
    "# Ask Questions\n",
    "input_text = \"question: What segment has the highest sales? context: \" + order_data.iloc[0]['text']\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "# Map the predicted class to the label\n",
    "label_map = {0: \"Consumer\", 1: \"Other\"}  # Adjust this based on your actual labels\n",
    "predicted_label = label_map[predicted_class]\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0585f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 7995\n",
      "Evaluation dataset size: 1999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b636c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
