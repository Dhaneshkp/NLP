{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":205017,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":4684,"modelId":2820}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n#!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\n\n#model = SentenceTransformer(\"all-mpnet-base-v2\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#model.save(\"/kaggle/working/all-mpnet-base-v2\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/qwen-llm\")\n#model = AutoModel.from_pretrained(\"/kaggle/input/qwen-llm\")\n\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\nsubmission_test","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ndf_train","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf_test","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train=df_train.head(20000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_train.drop(['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie'], axis = 1)\nX","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n\ny = np.argmax(y, axis=1)\ny","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catagorical_feature = [col for col in X.columns if X[col].dtype == 'object']\ncatagorical_feature","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_name=\"all-mpnet-base-v2\"):\n        self.model_name = model_name  \n        self.model = SentenceTransformer(model_name)\n        if torch.cuda.is_available():\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self.model.encode(X.tolist(), show_progress_bar=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Example usage in ColumnTransformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", HFEmbedder(), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"execution":{"iopub.status.busy":"2025-08-23T12:09:38.196345Z","iopub.execute_input":"2025-08-23T12:09:38.197094Z","iopub.status.idle":"2025-08-23T12:09:43.999261Z","shell.execute_reply.started":"2025-08-23T12:09:38.197069Z","shell.execute_reply":"2025-08-23T12:09:43.998604Z"},"editable":false}},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\",use_auth_token=False , local_files_only=True,batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.model = SentenceTransformer(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            embeddings = self.model.encode(\n                batch,\n                show_progress_bar=False,\n                device=\"cuda\" if self.use_cuda else \"cpu\"\n            )\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-24T00:06:01.592096Z","iopub.execute_input":"2025-08-24T00:06:01.592776Z","iopub.status.idle":"2025-08-24T00:06:01.599761Z","shell.execute_reply.started":"2025-08-24T00:06:01.592751Z","shell.execute_reply":"2025-08-24T00:06:01.59901Z"},"editable":false}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_deberta_v3_model(config):\n    # Extract config values\n    vocab_size = config[\"vocabulary_size\"]\n    num_layers = config[\"num_layers\"]\n    num_heads = config[\"num_heads\"]\n    hidden_dim = config[\"hidden_dim\"]\n    intermediate_dim = config[\"intermediate_dim\"]\n    dropout_rate = config[\"dropout\"]\n    max_seq_len = config[\"max_sequence_length\"]\n\n    # Input layers\n    input_ids = layers.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = layers.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"attention_mask\")\n\n    # Embedding layer\n    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=hidden_dim)(input_ids)\n\n    # Positional encoding (simplified)\n    position_embeddings = layers.Embedding(input_dim=max_seq_len, output_dim=hidden_dim)(tf.range(start=0, limit=max_seq_len, delta=1))\n    position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n    x = embedding_layer + position_embeddings\n\n    # Transformer blocks\n    for _ in range(num_layers):\n        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)(x, x, attention_mask=attention_mask)\n        attention_output = layers.Dropout(dropout_rate)(attention_output)\n        attention_output = layers.LayerNormalization()(x + attention_output)\n\n        ffn_output = layers.Dense(intermediate_dim, activation='gelu')(attention_output)\n        ffn_output = layers.Dense(hidden_dim)(ffn_output)\n        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n        x = layers.LayerNormalization()(attention_output + ffn_output)\n\n    # Output (for embedding use, you might just return x)\n    pooled_output = layers.GlobalAveragePooling1D()(x)\n\n    model = models.Model(inputs=[input_ids, attention_mask], outputs=pooled_output)\n    return model\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import DebertaV2Tokenizer\nfrom tensorflow.keras.models import load_model\nimport torch\nimport numpy as np\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\nconfig = {\n    \"vocabulary_size\": 128100,\n    \"num_layers\": 12,\n    \"num_heads\": 6,\n    \"hidden_dim\": 384,\n    \"intermediate_dim\": 1536,\n    \"dropout\": 0.1,\n    \"max_sequence_length\": 512,\n    \"bucket_size\": 256\n}\n\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/assets/tokenizer/vocabulary.spm\")\n        #self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        #self.model = AutoModel.from_pretrained(self.model_path)\n        self.model =  DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n  # Define your model architecture here\n        self.model.load_weights(\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/model.weights.h5\")\n        print([input.name for input in self.model.inputs])\n\n        if self.use_cuda:\n            self.model = self.model#.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            \n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            \n            # Rename keys to match model input expectations\n            model_inputs = {\n                \"padding_mask\": inputs[\"attention_mask\"],\n                \"token_ids\": inputs[\"input_ids\"]\n            }\n            \n            if self.use_cuda:\n                model_inputs = {k: v.to(\"cuda\") for k, v in model_inputs.items()}\n            \n            with torch.no_grad():\n                outputs = self.model(model_inputs) \n\n            # Mean pooling over token embeddings\n            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4):\n        self.model_path = model_path\n        self.batch_size = batch_size\n\n        # Load tokenizer\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n\n        # Load model\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"‚úÖ Model input names:\", [input.name for input in self.model.inputs])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize using PyTorch tokenizer\n            tokens = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n\n            # Convert PyTorch tensors to NumPy arrays\n            padding_mask_np = tokens[\"attention_mask\"].cpu().numpy()\n            token_ids_np = tokens[\"input_ids\"].cpu().numpy()\n\n            # Convert to TensorFlow tensors\n            model_inputs = {\n                \"padding_mask\": tf.convert_to_tensor(padding_mask_np),\n                \"token_ids\": tf.convert_to_tensor(token_ids_np)\n            }\n\n            # Forward pass\n            outputs = self.model(model_inputs)  # returns a tensor directly\n\n            # Mean pooling and convert to NumPy\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n# ...existing code...\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4, max_length=512):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        # enforce and cap max token length to backbone capacity\n        self.max_length = min(int(max_length), 512)\n\n        # Load tokenizer and set model max length\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"‚úÖ Model input names:\", [input.name for input in self.model.inputs])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize with fixed max length so TF model receives fixed-size inputs\n            tokens = self.tokenizer(\n                batch,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\",\n            )\n\n            # Convert PyTorch tensors to NumPy arrays\n            padding_mask_np = tokens[\"attention_mask\"].cpu().numpy()\n            token_ids_np = tokens[\"input_ids\"].cpu().numpy()\n\n            # Convert to TensorFlow tensors for the Keras backbone\n            model_inputs = {\n                \"padding_mask\": tf.convert_to_tensor(padding_mask_np),\n                \"token_ids\": tf.convert_to_tensor(token_ids_np)\n            }\n\n            # Forward pass (returns tensor [batch, seq_len, hidden])\n            outputs = self.model(model_inputs)\n\n            # Mean pooling across sequence length, convert to NumPy\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n\n# ...existing code...\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4, max_length=512):\n        import tensorflow as tf\n        import numpy as np\n\n        self.model_path = model_path\n        self.batch_size = batch_size\n        # enforce and cap max token length to backbone capacity\n        self.max_length = min(int(max_length), 512)\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # Load tokenizer and set model max length\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"‚úÖ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        all_embeddings = []\n        hidden_dim = None\n        try:\n            # Try to infer hidden dim from model output_shape (Keras style)\n            hidden_dim = int(self.model.output_shape[-1])\n        except Exception:\n            hidden_dim = None\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize directly to TensorFlow tensors to avoid PyTorch tensors and extra copies\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",   # <-- use \"tf\" to get TF tensors directly\n                )\n            except Exception:\n                # fallback: tokenize each item separately and return numpy then convert to tf\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                # convert np arrays to tf tensors\n                tokens = {k: tf.convert_to_tensor(v, dtype=tf.int32) for k, v in tokens.items()}\n\n            # Ensure integer dtype expected by TF/Keras\n            token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n            attention_mask_tf = tf.cast(tokens[\"attention_mask\"], tf.int32)\n\n            model_inputs = {\n                \"padding_mask\": attention_mask_tf,\n                \"token_ids\": token_ids_tf\n            }\n\n            # Forward pass (Keras/TF will place ops on GPU if available)\n            outputs = self.model(model_inputs)  # [batch, seq_len, hidden]\n\n            # Mean pooling across sequence length, convert to NumPy for downstream sklearn pipelines\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n            # update hidden_dim if unknown\n            if hidden_dim is None:\n                try:\n                    hidden_dim = int(embeddings.shape[1])\n                except Exception:\n                    hidden_dim = None\n\n        if not all_embeddings:\n            # return an empty 2D array with known hidden dim if possible\n            if hidden_dim is not None:\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            return np.empty((0,))\n\n        return np.vstack(all_embeddings)\n# ...existing\n\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport os # Import os module\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path, batch_size=8, max_length=512, use_fast_tokenizer=True, enable_mixed_precision=True):\n        import tensorflow as tf\n        import numpy as np\n        import os\n        import multiprocessing\n\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision # Add this line to store the parameter\n        self._cpu_count = multiprocessing.cpu_count()\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # optionally use mixed precision on GPUs (speeds up fp16 capable GPUs)\n        try:\n            if self.enable_mixed_precision: # Use self.enable_mixed_precision\n                from tensorflow.keras import mixed_precision\n                mixed_precision.set_global_policy(\"mixed_float16\")\n        except Exception:\n            pass\n\n        # Load tokenizer (prefer fast tokenizer if available)\n        try:\n            if self.use_fast_tokenizer:\n                from transformers import DebertaV2TokenizerFast as _TokFast\n                self.tokenizer = _TokFast(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            else:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n        except Exception:\n            # fallback to original import name/location\n            try:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            except Exception:\n                raise\n\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(os.path.join(self.model_path, \"model.weights.h5\")) # Use os.path.join\n\n        print(\"‚úÖ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            # no data -> return empty with inferred dim if possible\n            try:\n                hidden_dim = int(self.model.output_shape[-1])\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            except Exception:\n                return np.empty((0,))\n\n        # If tokenizer is \"fast\" we can tokenize the whole dataset in one call (fast, Rust-backed)\n        try:\n            if self.use_fast_tokenizer:\n                # Vectorized tokenization to numpy arrays (fast)\n                tokens_np = self.tokenizer(\n                    texts,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                # Build a tf.data.Dataset to feed the model efficiently (reduces Python overhead)\n                input_ids = tf.cast(tokens_np[\"input_ids\"], tf.int32)\n                attention_mask = tf.cast(tokens_np[\"attention_mask\"], tf.int32)\n\n                ds = tf.data.Dataset.from_tensor_slices(\n                    {\"token_ids\": input_ids, \"padding_mask\": attention_mask}\n                )\n                ds = ds.batch(max(1, self.batch_size)).prefetch(tf.data.AUTOTUNE)\n\n                # Use model.predict on the dataset so Keras can optimize execution and improve GPU utilization\n                outputs = self.model.predict(ds, verbose=0)\n                embeddings = np.mean(outputs, axis=1)\n                return embeddings\n        except Exception:\n            # fall back to batching loop below if anything fails\n            pass\n\n        # Fallback: batch-tokenize and call model per-batch (keeps fewer copies)\n        all_embeddings = []\n        hidden_dim = None\n        try:\n            hidden_dim = int(self.model.output_shape[-1])\n        except Exception:\n            hidden_dim = None\n\n        for i in range(0, n, self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            # Tokenize directly to TensorFlow tensors where possible (reduces copies)\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",\n                )\n                token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n                attention_mask_tf = tf.cast(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            except Exception:\n                # safest fallback: numpy tokenization then tf convert\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                token_ids_tf = tf.convert_to_tensor(tokens[\"input_ids\"], dtype=tf.int32)\n                attention_mask_tf = tf.convert_to_tensor(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n\n            all_embeddings.append(embeddings)\n            if hidden_dim is None:\n                try:\n                    hidden_dim = int(embeddings.shape[1])\n                except Exception:\n                    hidden_dim = None\n\n        if not all_embeddings:\n            if hidden_dim is not None:\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            return np.empty((0,))\n\n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\"))\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text_for_common_words(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    tokens = text.split()\n    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n\n\nclass CommonWordsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        common_meaningful_words_a = []\n        common_meaningful_words_b = []\n\n        for index, row in X.iterrows():\n            prompt_tokens = clean_text_for_common_words(row['prompt'])\n            response_a_tokens = clean_text_for_common_words(row['response_a'])\n            response_b_tokens = clean_text_for_common_words(row['response_b'])\n\n            common_meaningful_a = len(set(prompt_tokens) & set(response_a_tokens))\n            common_meaningful_b = len(set(prompt_tokens) & set(response_b_tokens))\n\n            common_meaningful_words_a.append(common_meaningful_a)\n            common_meaningful_words_b.append(common_meaningful_b)\n\n        return np.array([common_meaningful_words_a, common_meaningful_words_b]).T","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.listdir(\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\"))\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedder = HFEmbedder(model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\")\nX_train[\"prompt_clean\"] = X_train[\"prompt\"].apply(clean_text_for_common_words)\nX_train[\"response_a_clean\"] = X_train[\"response_a\"].apply(clean_text_for_common_words)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"].apply(clean_text_for_common_words)\n\ncommon_words_transformer = CommonWordsTransformer()\n# Updated preprocessing transformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", embedder, \"prompt_clean\"),\n    (\"resp_a_embed\", embedder, \"response_a_clean\"),\n    (\"resp_b_embed\", embedder, \"response_a_clean\"),\n    (\"common_words\", common_words_transformer, [\"prompt\", \"response_a\", \"response_b\"]),\n    #(\"num\", \"passthrough\", [\"id\"])\n])\n\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessing","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#feature_selection = SelectKBest(score_func = chi2, k=6)\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfeature_selection = SelectKBest(score_func=f_classif, k=6)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBClassifier(\n    objective=\"multi:softprob\",  \n    num_class=3,                  \n    eval_metric=\"mlogloss\",       \n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42\n)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_pipeline = Pipeline([\n    (\"preprocessing\", preprocessing),\n    #(\"feature_selection\", feature_selection),\n    (\"model\", model)\n])\nmy_pipeline","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = cross_val_score(my_pipeline, X_train, y_train, cv=3, scoring=\"accuracy\")\nprint(\"Accuracy:\", scores.mean())","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_pipeline.fit(X_train, y_train)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"prompt_clean\"] = df_test[\"prompt\"].apply(clean_text_for_common_words)\ndf_test[\"response_a_clean\"] = df_test[\"response_a\"].apply(clean_text_for_common_words)\ndf_test[\"response_b_clean\"] = df_test[\"response_b\"].apply(clean_text_for_common_words)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs = my_pipeline.predict_proba(df_test)\n\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"winner_model_a\": probs[:,0],\n    \"winner_model_b\": probs[:,1],\n    \"winner_tie\": probs[:,2],\n})\nprint(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DebertaV2Tokenizer\nimport torch\nimport numpy as np\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport tensorflow as tf\n\n# Configuration\nmodel_path = \"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\"\nvocab_file = f\"{model_path}/assets/tokenizer/vocabulary.spm\"\nweights_file = f\"{model_path}/model.weights.h5\"\n\n# Initialize tokenizer\ntokenizer = DebertaV2Tokenizer(vocab_file=vocab_file)\nprint(\"‚úÖ Tokenizer initialized:\", type(tokenizer))\n\n# Initialize model\nmodel = DebertaV3Backbone(\n    vocabulary_size=128100,\n    num_layers=12,\n    num_heads=6,\n    hidden_dim=384,\n    intermediate_dim=1536,\n    dropout=0.1,\n    max_sequence_length=512,\n    bucket_size=256\n)\nmodel.load_weights(weights_file)\nprint(\"‚úÖ Model loaded and weights applied.\")\nprint(\"Model input names:\", [input.name for input in model.inputs])\n\n# Example input text\ntext = \"DeBERTa is a powerful transformer model.\"\n\n# Tokenize the input\ntokens = tokenizer(text, return_tensors=\"pt\")\nprint(\"‚úÖ Tokenizer output keys:\", tokens.keys())\nprint(\"Input IDs:\", tokens[\"input_ids\"])\nprint(\"Attention Mask:\", tokens[\"attention_mask\"])\n\n# Convert token IDs to token strings\ninput_ids = tokens[\"input_ids\"][0]\ntoken_names = tokenizer.convert_ids_to_tokens(input_ids)\nprint(\"‚úÖ Token names:\", token_names)\n\n# Convert PyTorch tensors to NumPy arrays (on CPU)\npadding_mask_np = tokens[\"attention_mask\"].cpu().numpy()\ntoken_ids_np = tokens[\"input_ids\"].cpu().numpy()\n\n# Convert to TensorFlow tensors\nmodel_inputs = {\n    \"padding_mask\": tf.convert_to_tensor(padding_mask_np),\n    \"token_ids\": tf.convert_to_tensor(token_ids_np)\n}\n\n# Run forward pass\nprint(\"üöÄ Running model forward pass...\")\noutputs = model(model_inputs)\n\n# Inspect model output\nprint(\"‚úÖ Model output type:\", type(outputs))\nif isinstance(outputs, dict):\n    print(\"Model output keys:\", outputs.keys())\n    if \"last_hidden_state\" in outputs:\n        hidden_state = outputs[\"last_hidden_state\"]\n        print(\"Last hidden state shape:\", hidden_state.shape)\n        # Mean pooling and convert to NumPy\n        embeddings = tf.reduce_mean(hidden_state, axis=1).numpy()\n        print(\"‚úÖ Mean pooled embeddings shape:\", embeddings.shape)\n    else:\n        print(\"‚ö†Ô∏è Warning: 'last_hidden_state' not found in model output.\")\nelse:\n    print(\"‚ö†Ô∏è Model returned a non-dict output:\", outputs)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}