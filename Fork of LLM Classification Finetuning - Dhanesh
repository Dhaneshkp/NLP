{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":205017,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":4684,"modelId":2820}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-08-31T10:13:05.037315Z","iopub.execute_input":"2025-08-31T10:13:05.037592Z","iopub.status.idle":"2025-08-31T10:13:07.188218Z","shell.execute_reply.started":"2025-08-31T10:13:05.037569Z","shell.execute_reply":"2025-08-31T10:13:07.187568Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/assets/tokenizer/vocabulary.spm\n/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n\n#!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\n\n#model = SentenceTransformer(\"all-mpnet-base-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:07.189510Z","iopub.execute_input":"2025-08-31T10:13:07.189885Z","iopub.status.idle":"2025-08-31T10:13:22.837415Z","shell.execute_reply.started":"2025-08-31T10:13:07.189857Z","shell.execute_reply":"2025-08-31T10:13:22.836582Z"}},"outputs":[{"name":"stderr","text":"2025-08-31 10:13:16.470196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756635196.637088     306 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756635196.689592     306 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n#model.save(\"/kaggle/working/all-mpnet-base-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:22.838427Z","iopub.execute_input":"2025-08-31T10:13:22.839193Z","iopub.status.idle":"2025-08-31T10:13:22.842691Z","shell.execute_reply.started":"2025-08-31T10:13:22.839164Z","shell.execute_reply":"2025-08-31T10:13:22.841968Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/qwen-llm\")\n#model = AutoModel.from_pretrained(\"/kaggle/input/qwen-llm\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:22.844136Z","iopub.execute_input":"2025-08-31T10:13:22.844362Z","iopub.status.idle":"2025-08-31T10:13:22.858684Z","shell.execute_reply.started":"2025-08-31T10:13:22.844344Z","shell.execute_reply":"2025-08-31T10:13:22.858116Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"submission_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\nsubmission_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:22.859355Z","iopub.execute_input":"2025-08-31T10:13:22.859584Z","iopub.status.idle":"2025-08-31T10:13:22.887920Z","shell.execute_reply.started":"2025-08-31T10:13:22.859559Z","shell.execute_reply":"2025-08-31T10:13:22.887382Z"},"editable":false},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.333333        0.333333    0.333333\n1   211333        0.333333        0.333333    0.333333\n2  1233961        0.333333        0.333333    0.333333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ndf_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:22.888641Z","iopub.execute_input":"2025-08-31T10:13:22.888846Z","iopub.status.idle":"2025-08-31T10:13:26.137126Z","shell.execute_reply.started":"2025-08-31T10:13:22.888830Z","shell.execute_reply":"2025-08-31T10:13:26.136436Z"},"editable":false},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.137895Z","iopub.execute_input":"2025-08-31T10:13:26.138208Z","iopub.status.idle":"2025-08-31T10:13:26.149748Z","shell.execute_reply.started":"2025-08-31T10:13:26.138175Z","shell.execute_reply":"2025-08-31T10:13:26.148892Z"},"editable":false},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>[\"I have three oranges today, I ate an orange ...</td>\n      <td>[\"You have two oranges today.\"]</td>\n      <td>[\"You still have three oranges. Eating an oran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>[\"You are a mediator in a heated political deb...</td>\n      <td>[\"Thank you for sharing the details of the sit...</td>\n      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>[\"How to initialize the classification head wh...</td>\n      <td>[\"When you want to initialize the classificati...</td>\n      <td>[\"To initialize the classification head when p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.150602Z","iopub.execute_input":"2025-08-31T10:13:26.150871Z","iopub.status.idle":"2025-08-31T10:13:26.187666Z","shell.execute_reply.started":"2025-08-31T10:13:26.150852Z","shell.execute_reply":"2025-08-31T10:13:26.187115Z"},"editable":false},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              57477 non-null  int64 \n 1   model_a         57477 non-null  object\n 2   model_b         57477 non-null  object\n 3   prompt          57477 non-null  object\n 4   response_a      57477 non-null  object\n 5   response_b      57477 non-null  object\n 6   winner_model_a  57477 non-null  int64 \n 7   winner_model_b  57477 non-null  int64 \n 8   winner_tie      57477 non-null  int64 \ndtypes: int64(4), object(5)\nmemory usage: 3.9+ MB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df_train=df_train.head(57477)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.188278Z","iopub.execute_input":"2025-08-31T10:13:26.188459Z","iopub.status.idle":"2025-08-31T10:13:26.192069Z","shell.execute_reply.started":"2025-08-31T10:13:26.188444Z","shell.execute_reply":"2025-08-31T10:13:26.191376Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.194558Z","iopub.execute_input":"2025-08-31T10:13:26.195134Z","iopub.status.idle":"2025-08-31T10:13:26.555779Z","shell.execute_reply.started":"2025-08-31T10:13:26.195115Z","shell.execute_reply":"2025-08-31T10:13:26.555025Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.556635Z","iopub.execute_input":"2025-08-31T10:13:26.556906Z","iopub.status.idle":"2025-08-31T10:13:26.597347Z","shell.execute_reply.started":"2025-08-31T10:13:26.556882Z","shell.execute_reply":"2025-08-31T10:13:26.596751Z"},"editable":false},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"id                0\nmodel_a           0\nmodel_b           0\nprompt            0\nresponse_a        0\nresponse_b        0\nwinner_model_a    0\nwinner_model_b    0\nwinner_tie        0\ndtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"X = df_train.drop(['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie'], axis = 1)\nX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.597897Z","iopub.execute_input":"2025-08-31T10:13:26.598093Z","iopub.status.idle":"2025-08-31T10:13:26.619774Z","shell.execute_reply.started":"2025-08-31T10:13:26.598060Z","shell.execute_reply":"2025-08-31T10:13:26.619245Z"},"editable":false},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n0           30192  [\"Is it morally right to try to have a certain...   \n1           53567  [\"What is the difference between marriage lice...   \n2           65089  [\"explain function calling. how would you call...   \n3           96401  [\"How can I create a test set for a very rare ...   \n4          198779  [\"What is the best way to travel from Tel-Aviv...   \n...           ...                                                ...   \n57472  4294656694  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  4294692063  [\"In python, implement a naive Bayes with gaus...   \n57474  4294710549  [\"is it unethical to work on building weapons?...   \n57475  4294899228  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  4294947231  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  \n0      [\"As an AI, I don't have personal beliefs or o...  \n1      [\"A marriage license and a marriage certificat...  \n2      [\"Function calling is the process of invoking ...  \n3      [\"When building a classifier for a very rare c...  \n4      [\"The best way to travel from Tel-Aviv to Jeru...  \n...                                                  ...  \n57472  [\"Here is how that mnemonic represents the dig...  \n57473  [\"Sure! Here's an implementation of a naive Ba...  \n57474  [\"It depends on the context. Weapons can be us...  \n57475  [\"As an AI language model, I do not promote or...  \n57476  [\"If three kids eat three apples in three days...  \n\n[57477 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"y = df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n\ny = np.argmax(y, axis=1)\ny","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.620405Z","iopub.execute_input":"2025-08-31T10:13:26.620593Z","iopub.status.idle":"2025-08-31T10:13:26.627544Z","shell.execute_reply.started":"2025-08-31T10:13:26.620578Z","shell.execute_reply":"2025-08-31T10:13:26.626894Z"},"editable":false},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([0, 1, 2, ..., 0, 1, 0])"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.628256Z","iopub.execute_input":"2025-08-31T10:13:26.628474Z","iopub.status.idle":"2025-08-31T10:13:26.645963Z","shell.execute_reply.started":"2025-08-31T10:13:26.628459Z","shell.execute_reply":"2025-08-31T10:13:26.645259Z"},"editable":false},"outputs":[],"execution_count":14},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.646917Z","iopub.execute_input":"2025-08-31T10:13:26.647192Z","iopub.status.idle":"2025-08-31T10:13:26.655701Z","shell.execute_reply.started":"2025-08-31T10:13:26.647169Z","shell.execute_reply":"2025-08-31T10:13:26.654940Z"},"editable":false},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n30205  2245506736  [\"can you give me some escape room themes with...   \n8763    656268525  [\"Why did digital electronics switch from 5 vo...   \n30501  2266396135  [\"as an acclaimed and visionary author punishe...   \n33619  2497468695  [\"\\n\\nWrite a micro editor plugin that shows t...   \n22573  1679992057  [\"1. 19 year old Emese who is thin (165cm 48kg...   \n...           ...                                                ...   \n54343  4061186081         [\"mega-sena brazilian lottery statistics\"]   \n38158  2840606458                     [\"Draw a dog using ASCII art\"]   \n860      62750940                          [\"How to wear a bikini?\"]   \n15795  1180878567  [\"Can you tell me what was going on with all t...   \n56422  4218036675                          [\"SHOW ME WHAT YOU GOT!\"]   \n\n                                              response_a  \\\n30205  [\"Certainly! Here are several escape room them...   \n8763   [\"There are a few key reasons why digital elec...   \n30501  [\"As an acclaimed and visionary author, Elieze...   \n33619  [\"Here is Python code for a basic micro editor...   \n22573  [\"Certainly, let's address each of your points...   \n...                                                  ...   \n54343  [\"I do not have access to real-time mega-sena ...   \n38158                        [\"\\/_\\/\\\\  \\n( o.o )\\n> ^\"]   \n860    [\"Wearing a bikini can be a fun and comfortabl...   \n15795  [\"The direwolves, which are also known as dire...   \n56422  [\"Of course! As an AI, I can assist with a var...   \n\n                                              response_b  \n30205  [\"Here are some fun and immersive escape room ...  \n8763   [\"The shift from 5 volt logic to 3 volt logic ...  \n30501  [\"Eliezer Yudkowsky is often referred to as th...  \n33619  [\"Micro is a modern and intuitive terminal-bas...  \n22573  [\"1. From the description you provided, it see...  \n...                                                  ...  \n54343  [\"Mega-Sena is a Brazilian lottery game that o...  \n38158  [\"Here is a dog drawn with ASCII art:\\n\\n     ...  \n860    [\"Here are the basic steps to wear a bikini:\\n...  \n15795  [\"Sure! On the night of the Red Wedding, all o...  \n56422  [\"I'm a large language model, so I don't have ...  \n\n[45981 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30205</th>\n      <td>2245506736</td>\n      <td>[\"can you give me some escape room themes with...</td>\n      <td>[\"Certainly! Here are several escape room them...</td>\n      <td>[\"Here are some fun and immersive escape room ...</td>\n    </tr>\n    <tr>\n      <th>8763</th>\n      <td>656268525</td>\n      <td>[\"Why did digital electronics switch from 5 vo...</td>\n      <td>[\"There are a few key reasons why digital elec...</td>\n      <td>[\"The shift from 5 volt logic to 3 volt logic ...</td>\n    </tr>\n    <tr>\n      <th>30501</th>\n      <td>2266396135</td>\n      <td>[\"as an acclaimed and visionary author punishe...</td>\n      <td>[\"As an acclaimed and visionary author, Elieze...</td>\n      <td>[\"Eliezer Yudkowsky is often referred to as th...</td>\n    </tr>\n    <tr>\n      <th>33619</th>\n      <td>2497468695</td>\n      <td>[\"\\n\\nWrite a micro editor plugin that shows t...</td>\n      <td>[\"Here is Python code for a basic micro editor...</td>\n      <td>[\"Micro is a modern and intuitive terminal-bas...</td>\n    </tr>\n    <tr>\n      <th>22573</th>\n      <td>1679992057</td>\n      <td>[\"1. 19 year old Emese who is thin (165cm 48kg...</td>\n      <td>[\"Certainly, let's address each of your points...</td>\n      <td>[\"1. From the description you provided, it see...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54343</th>\n      <td>4061186081</td>\n      <td>[\"mega-sena brazilian lottery statistics\"]</td>\n      <td>[\"I do not have access to real-time mega-sena ...</td>\n      <td>[\"Mega-Sena is a Brazilian lottery game that o...</td>\n    </tr>\n    <tr>\n      <th>38158</th>\n      <td>2840606458</td>\n      <td>[\"Draw a dog using ASCII art\"]</td>\n      <td>[\"\\/_\\/\\\\  \\n( o.o )\\n&gt; ^\"]</td>\n      <td>[\"Here is a dog drawn with ASCII art:\\n\\n     ...</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>62750940</td>\n      <td>[\"How to wear a bikini?\"]</td>\n      <td>[\"Wearing a bikini can be a fun and comfortabl...</td>\n      <td>[\"Here are the basic steps to wear a bikini:\\n...</td>\n    </tr>\n    <tr>\n      <th>15795</th>\n      <td>1180878567</td>\n      <td>[\"Can you tell me what was going on with all t...</td>\n      <td>[\"The direwolves, which are also known as dire...</td>\n      <td>[\"Sure! On the night of the Red Wedding, all o...</td>\n    </tr>\n    <tr>\n      <th>56422</th>\n      <td>4218036675</td>\n      <td>[\"SHOW ME WHAT YOU GOT!\"]</td>\n      <td>[\"Of course! As an AI, I can assist with a var...</td>\n      <td>[\"I'm a large language model, so I don't have ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>45981 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.656415Z","iopub.execute_input":"2025-08-31T10:13:26.656641Z","iopub.status.idle":"2025-08-31T10:13:26.671530Z","shell.execute_reply.started":"2025-08-31T10:13:26.656626Z","shell.execute_reply":"2025-08-31T10:13:26.670846Z"},"editable":false},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"               id                                             prompt  \\\n37379  2785062085                     [\"what does hello world mean\"]   \n644      48259531  [\"I ran a marathon in 3:12:00 weighting 84kg. ...   \n48496  3622731894  [\"Below is an instruction that describes a tas...   \n12603   933663766  [\"How do I run static analysis with gcc in QT ...   \n16697  1246768370  [\"What did the music producer \\u00d8fdream die...   \n...           ...                                                ...   \n3151    240261313        [\"How to save a web page into a jpg file?\"]   \n30983  2300512815  [\"provide a short story about a pirate that le...   \n31183  2314277410  [\"Name 3 features of Alain Badiou's ontology t...   \n50634  3781372084  [\"Please give me a list of concise bullet poin...   \n23146  1722610582                      [\"What is funnier than 420?\"]   \n\n                                              response_a  \\\n37379  [\"\\\"Hello, World!\\\" is a phrase used in comput...   \n644    [\"It's difficult to provide an exact answer to...   \n48496  [\"\\u8fd9\\u6b3e\\u7eaf\\u8272\\u4e94\\u5206\\u88e4\\u...   \n12603  [\"To run static analysis with gcc in QT Creato...   \n16697  [\"I am not able to provide information on the ...   \n...                                                  ...   \n3151   [\"Here are a few ways to save a web page as a ...   \n30983  [\"Here is a short story about a pirate who lea...   \n31183  [\"Alain Badiou and Plato have distinct philoso...   \n50634  [\"- Musician who plays guitar and enjoys jammi...   \n23146  [\"The question \\\"What is funnier than 420?\\\" i...   \n\n                                              response_b  \n37379  [\"\\\"Hello, World!\\\" is a common phrase used to...  \n644    [\"To accurately estimate how much faster you w...  \n48496  [\"\\u6839\\u636e\\u63d0\\u4f9b\\u7684\\u4ea7\\u54c1\\u...  \n12603  [\"To run static analysis with gcc in QT Creato...  \n16697  [\"I'm really sorry, but I don't have the abili...  \n...                                                  ...  \n3151   [\"Here are a few ways to save a web page as a ...  \n30983  [\"Once upon a time, in the golden age of pirac...  \n31183  [\"Here are 3 key differences between Alain Bad...  \n50634  [\"* Music: plays guitar, drums, and bass\\n* Sp...  \n23146  [\"Humor is subjective and can vary from person...  \n\n[11496 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37379</th>\n      <td>2785062085</td>\n      <td>[\"what does hello world mean\"]</td>\n      <td>[\"\\\"Hello, World!\\\" is a phrase used in comput...</td>\n      <td>[\"\\\"Hello, World!\\\" is a common phrase used to...</td>\n    </tr>\n    <tr>\n      <th>644</th>\n      <td>48259531</td>\n      <td>[\"I ran a marathon in 3:12:00 weighting 84kg. ...</td>\n      <td>[\"It's difficult to provide an exact answer to...</td>\n      <td>[\"To accurately estimate how much faster you w...</td>\n    </tr>\n    <tr>\n      <th>48496</th>\n      <td>3622731894</td>\n      <td>[\"Below is an instruction that describes a tas...</td>\n      <td>[\"\\u8fd9\\u6b3e\\u7eaf\\u8272\\u4e94\\u5206\\u88e4\\u...</td>\n      <td>[\"\\u6839\\u636e\\u63d0\\u4f9b\\u7684\\u4ea7\\u54c1\\u...</td>\n    </tr>\n    <tr>\n      <th>12603</th>\n      <td>933663766</td>\n      <td>[\"How do I run static analysis with gcc in QT ...</td>\n      <td>[\"To run static analysis with gcc in QT Creato...</td>\n      <td>[\"To run static analysis with gcc in QT Creato...</td>\n    </tr>\n    <tr>\n      <th>16697</th>\n      <td>1246768370</td>\n      <td>[\"What did the music producer \\u00d8fdream die...</td>\n      <td>[\"I am not able to provide information on the ...</td>\n      <td>[\"I'm really sorry, but I don't have the abili...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3151</th>\n      <td>240261313</td>\n      <td>[\"How to save a web page into a jpg file?\"]</td>\n      <td>[\"Here are a few ways to save a web page as a ...</td>\n      <td>[\"Here are a few ways to save a web page as a ...</td>\n    </tr>\n    <tr>\n      <th>30983</th>\n      <td>2300512815</td>\n      <td>[\"provide a short story about a pirate that le...</td>\n      <td>[\"Here is a short story about a pirate who lea...</td>\n      <td>[\"Once upon a time, in the golden age of pirac...</td>\n    </tr>\n    <tr>\n      <th>31183</th>\n      <td>2314277410</td>\n      <td>[\"Name 3 features of Alain Badiou's ontology t...</td>\n      <td>[\"Alain Badiou and Plato have distinct philoso...</td>\n      <td>[\"Here are 3 key differences between Alain Bad...</td>\n    </tr>\n    <tr>\n      <th>50634</th>\n      <td>3781372084</td>\n      <td>[\"Please give me a list of concise bullet poin...</td>\n      <td>[\"- Musician who plays guitar and enjoys jammi...</td>\n      <td>[\"* Music: plays guitar, drums, and bass\\n* Sp...</td>\n    </tr>\n    <tr>\n      <th>23146</th>\n      <td>1722610582</td>\n      <td>[\"What is funnier than 420?\"]</td>\n      <td>[\"The question \\\"What is funnier than 420?\\\" i...</td>\n      <td>[\"Humor is subjective and can vary from person...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11496 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.672272Z","iopub.execute_input":"2025-08-31T10:13:26.672574Z","iopub.status.idle":"2025-08-31T10:13:26.685351Z","shell.execute_reply.started":"2025-08-31T10:13:26.672538Z","shell.execute_reply":"2025-08-31T10:13:26.684752Z"},"editable":false},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([2, 1, 1, ..., 1, 1, 0])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"catagorical_feature = [col for col in X.columns if X[col].dtype == 'object']\ncatagorical_feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.686321Z","iopub.execute_input":"2025-08-31T10:13:26.686580Z","iopub.status.idle":"2025-08-31T10:13:26.698969Z","shell.execute_reply.started":"2025-08-31T10:13:26.686556Z","shell.execute_reply":"2025-08-31T10:13:26.698409Z"},"editable":false},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['prompt', 'response_a', 'response_b']"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.699675Z","iopub.execute_input":"2025-08-31T10:13:26.699875Z","iopub.status.idle":"2025-08-31T10:13:26.712452Z","shell.execute_reply.started":"2025-08-31T10:13:26.699860Z","shell.execute_reply":"2025-08-31T10:13:26.711891Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_name=\"all-mpnet-base-v2\"):\n        self.model_name = model_name  \n        self.model = SentenceTransformer(model_name)\n        if torch.cuda.is_available():\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self.model.encode(X.tolist(), show_progress_bar=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Example usage in ColumnTransformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", HFEmbedder(), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"execution":{"iopub.status.busy":"2025-08-23T12:09:38.196345Z","iopub.execute_input":"2025-08-23T12:09:38.197094Z","iopub.status.idle":"2025-08-23T12:09:43.999261Z","shell.execute_reply.started":"2025-08-23T12:09:38.197069Z","shell.execute_reply":"2025-08-23T12:09:43.998604Z"},"editable":false}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\",use_auth_token=False , local_files_only=True,batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.model = SentenceTransformer(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            embeddings = self.model.encode(\n                batch,\n                show_progress_bar=False,\n                device=\"cuda\" if self.use_cuda else \"cpu\"\n            )\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-24T00:06:01.592096Z","iopub.execute_input":"2025-08-24T00:06:01.592776Z","iopub.status.idle":"2025-08-24T00:06:01.599761Z","shell.execute_reply.started":"2025-08-24T00:06:01.592751Z","shell.execute_reply":"2025-08-24T00:06:01.59901Z"}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_deberta_v3_model(config):\n    # Extract config values\n    vocab_size = config[\"vocabulary_size\"]\n    num_layers = config[\"num_layers\"]\n    num_heads = config[\"num_heads\"]\n    hidden_dim = config[\"hidden_dim\"]\n    intermediate_dim = config[\"intermediate_dim\"]\n    dropout_rate = config[\"dropout\"]\n    max_seq_len = config[\"max_sequence_length\"]\n\n    # Input layers\n    input_ids = layers.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = layers.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"attention_mask\")\n\n    # Embedding layer\n    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=hidden_dim)(input_ids)\n\n    # Positional encoding (simplified)\n    position_embeddings = layers.Embedding(input_dim=max_seq_len, output_dim=hidden_dim)(tf.range(start=0, limit=max_seq_len, delta=1))\n    position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n    x = embedding_layer + position_embeddings\n\n    # Transformer blocks\n    for _ in range(num_layers):\n        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)(x, x, attention_mask=attention_mask)\n        attention_output = layers.Dropout(dropout_rate)(attention_output)\n        attention_output = layers.LayerNormalization()(x + attention_output)\n\n        ffn_output = layers.Dense(intermediate_dim, activation='gelu')(attention_output)\n        ffn_output = layers.Dense(hidden_dim)(ffn_output)\n        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n        x = layers.LayerNormalization()(attention_output + ffn_output)\n\n    # Output (for embedding use, you might just return x)\n    pooled_output = layers.GlobalAveragePooling1D()(x)\n\n    model = models.Model(inputs=[input_ids, attention_mask], outputs=pooled_output)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.713402Z","iopub.execute_input":"2025-08-31T10:13:26.713685Z","iopub.status.idle":"2025-08-31T10:13:26.751866Z","shell.execute_reply.started":"2025-08-31T10:13:26.713663Z","shell.execute_reply":"2025-08-31T10:13:26.751393Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import DebertaV2Tokenizer\nfrom tensorflow.keras.models import load_model\nimport torch\nimport numpy as np\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\nconfig = {\n    \"vocabulary_size\": 128100,\n    \"num_layers\": 12,\n    \"num_heads\": 6,\n    \"hidden_dim\": 384,\n    \"intermediate_dim\": 1536,\n    \"dropout\": 0.1,\n    \"max_sequence_length\": 512,\n    \"bucket_size\": 256\n}\n\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/assets/tokenizer/vocabulary.spm\")\n        #self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        #self.model = AutoModel.from_pretrained(self.model_path)\n        self.model =  DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n  # Define your model architecture here\n        self.model.load_weights(\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/model.weights.h5\")\n        print([input.name for input in self.model.inputs])\n\n        if self.use_cuda:\n            self.model = self.model#.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            \n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            \n            # Rename keys to match model input expectations\n            model_inputs = {\n                \"padding_mask\": inputs[\"attention_mask\"],\n                \"token_ids\": inputs[\"input_ids\"]\n            }\n            \n            if self.use_cuda:\n                model_inputs = {k: v.to(\"cuda\") for k, v in model_inputs.items()}\n            \n            with torch.no_grad():\n                outputs = self.model(model_inputs) \n\n            # Mean pooling over token embeddings\n            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:26.752501Z","iopub.execute_input":"2025-08-31T10:13:26.752688Z","iopub.status.idle":"2025-08-31T10:13:27.134962Z","shell.execute_reply.started":"2025-08-31T10:13:26.752674Z","shell.execute_reply":"2025-08-31T10:13:27.134394Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4):\n        self.model_path = model_path\n        self.batch_size = batch_size\n\n        # Load tokenizer\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n\n        # Load model\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize using PyTorch tokenizer\n            tokens = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n\n            # Convert PyTorch tensors to NumPy arrays\n            padding_mask_np = tokens[\"attention_mask\"].cpu().numpy()\n            token_ids_np = tokens[\"input_ids\"].cpu().numpy()\n\n            # Convert to TensorFlow tensors\n            model_inputs = {\n                \"padding_mask\": tf.convert_to_tensor(padding_mask_np),\n                \"token_ids\": tf.convert_to_tensor(token_ids_np)\n            }\n\n            # Forward pass\n            outputs = self.model(model_inputs)  # returns a tensor directly\n\n            # Mean pooling and convert to NumPy\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n# ...existing code...\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4, max_length=512):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        # enforce and cap max token length to backbone capacity\n        self.max_length = min(int(max_length), 512)\n\n        # Load tokenizer and set model max length\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize with fixed max length so TF model receives fixed-size inputs\n            tokens = self.tokenizer(\n                batch,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\",\n            )\n\n            # Convert PyTorch tensors to NumPy arrays\n            padding_mask_np = tokens[\"attention_mask\"].cpu().numpy()\n            token_ids_np = tokens[\"input_ids\"].cpu().numpy()\n\n            # Convert to TensorFlow tensors for the Keras backbone\n            model_inputs = {\n                \"padding_mask\": tf.convert_to_tensor(padding_mask_np),\n                \"token_ids\": tf.convert_to_tensor(token_ids_np)\n            }\n\n            # Forward pass (returns tensor [batch, seq_len, hidden])\n            outputs = self.model(model_inputs)\n\n            # Mean pooling across sequence length, convert to NumPy\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n\n# ...existing code...\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4, max_length=512):\n        import tensorflow as tf\n        import numpy as np\n\n        self.model_path = model_path\n        self.batch_size = batch_size\n        # enforce and cap max token length to backbone capacity\n        self.max_length = min(int(max_length), 512)\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # Load tokenizer and set model max length\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        all_embeddings = []\n        hidden_dim = None\n        try:\n            # Try to infer hidden dim from model output_shape (Keras style)\n            hidden_dim = int(self.model.output_shape[-1])\n        except Exception:\n            hidden_dim = None\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize directly to TensorFlow tensors to avoid PyTorch tensors and extra copies\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",   # <-- use \"tf\" to get TF tensors directly\n                )\n            except Exception:\n                # fallback: tokenize each item separately and return numpy then convert to tf\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                # convert np arrays to tf tensors\n                tokens = {k: tf.convert_to_tensor(v, dtype=tf.int32) for k, v in tokens.items()}\n\n            # Ensure integer dtype expected by TF/Keras\n            token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n            attention_mask_tf = tf.cast(tokens[\"attention_mask\"], tf.int32)\n\n            model_inputs = {\n                \"padding_mask\": attention_mask_tf,\n                \"token_ids\": token_ids_tf\n            }\n\n            # Forward pass (Keras/TF will place ops on GPU if available)\n            outputs = self.model(model_inputs)  # [batch, seq_len, hidden]\n\n            # Mean pooling across sequence length, convert to NumPy for downstream sklearn pipelines\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n            # update hidden_dim if unknown\n            if hidden_dim is None:\n                try:\n                    hidden_dim = int(embeddings.shape[1])\n                except Exception:\n                    hidden_dim = None\n\n        if not all_embeddings:\n            # return an empty 2D array with known hidden dim if possible\n            if hidden_dim is not None:\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            return np.empty((0,))\n\n        return np.vstack(all_embeddings)\n# ...existing\n\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport os # Import os module\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path, batch_size=32, max_length=512, use_fast_tokenizer=True, enable_mixed_precision=True):\n        import tensorflow as tf\n        import numpy as np\n        import os\n        import multiprocessing\n\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision # Add this line to store the parameter\n        self._cpu_count = multiprocessing.cpu_count()\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # optionally use mixed precision on GPUs (speeds up fp16 capable GPUs)\n        try:\n            if self.enable_mixed_precision: # Use self.enable_mixed_precision\n                from tensorflow.keras import mixed_precision\n                mixed_precision.set_global_policy(\"mixed_float16\")\n        except Exception:\n            pass\n\n        # Load tokenizer (prefer fast tokenizer if available)\n        try:\n            if self.use_fast_tokenizer:\n                from transformers import DebertaV2TokenizerFast as _TokFast\n                self.tokenizer = _TokFast(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            else:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n        except Exception:\n            # fallback to original import name/location\n            try:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            except Exception:\n                raise\n\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(os.path.join(self.model_path, \"model.weights.h5\")) # Use os.path.join\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            # no data -> return empty with inferred dim if possible\n            try:\n                hidden_dim = int(self.model.output_shape[-1])\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            except Exception:\n                return np.empty((0,))\n\n        # If tokenizer is \"fast\" we can tokenize the whole dataset in one call (fast, Rust-backed)\n        try:\n            if self.use_fast_tokenizer:\n                # Vectorized tokenization to numpy arrays (fast)\n                tokens_np = self.tokenizer(\n                    texts,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                # Build a tf.data.Dataset to feed the model efficiently (reduces Python overhead)\n                input_ids = tf.cast(tokens_np[\"input_ids\"], tf.int32)\n                attention_mask = tf.cast(tokens_np[\"attention_mask\"], tf.int32)\n\n                ds = tf.data.Dataset.from_tensor_slices(\n                    {\"token_ids\": input_ids, \"padding_mask\": attention_mask}\n                )\n                ds = ds.batch(max(1, self.batch_size)).prefetch(tf.data.AUTOTUNE)\n\n                # Use model.predict on the dataset so Keras can optimize execution and improve GPU utilization\n                outputs = self.model.predict(ds, verbose=0)\n                embeddings = np.mean(outputs, axis=1)\n                return embeddings\n        except Exception:\n            # fall back to batching loop below if anything fails\n            pass\n\n        # Fallback: batch-tokenize and call model per-batch (keeps fewer copies)\n        all_embeddings = []\n        hidden_dim = None\n        try:\n            hidden_dim = int(self.model.output_shape[-1])\n        except Exception:\n            hidden_dim = None\n\n        for i in range(0, n, self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            # Tokenize directly to TensorFlow tensors where possible (reduces copies)\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",\n                )\n                token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n                attention_mask_tf = tf.cast(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            except Exception:\n                # safest fallback: numpy tokenization then tf convert\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                token_ids_tf = tf.convert_to_tensor(tokens[\"input_ids\"], dtype=tf.int32)\n                attention_mask_tf = tf.convert_to_tensor(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n\n            all_embeddings.append(embeddings)\n            if hidden_dim is None:\n                try:\n                    hidden_dim = int(embeddings.shape[1])\n                except Exception:\n                    hidden_dim = None\n\n        if not all_embeddings:\n            if hidden_dim is not None:\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            return np.empty((0,))\n\n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:27.135745Z","iopub.execute_input":"2025-08-31T10:13:27.135930Z","iopub.status.idle":"2025-08-31T10:13:27.174685Z","shell.execute_reply.started":"2025-08-31T10:13:27.135916Z","shell.execute_reply":"2025-08-31T10:13:27.173949Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:27.175516Z","iopub.execute_input":"2025-08-31T10:13:27.175710Z","iopub.status.idle":"2025-08-31T10:13:27.193115Z","shell.execute_reply.started":"2025-08-31T10:13:27.175694Z","shell.execute_reply":"2025-08-31T10:13:27.192427Z"}},"outputs":[{"name":"stdout","text":"['config.json', 'assets', 'tokenizer.json', 'metadata.json', 'model.weights.h5']\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text_for_common_words(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    tokens = text.split()\n    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n\n\nclass CommonWordsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        common_meaningful_words_a = []\n        common_meaningful_words_b = []\n\n        for index, row in X.iterrows():\n            prompt_tokens = clean_text_for_common_words(row['prompt'])\n            response_a_tokens = clean_text_for_common_words(row['response_a'])\n            response_b_tokens = clean_text_for_common_words(row['response_b'])\n\n            common_meaningful_a = len(set(prompt_tokens) & set(response_a_tokens))\n            common_meaningful_b = len(set(prompt_tokens) & set(response_b_tokens))\n\n            common_meaningful_words_a.append(common_meaningful_a)\n            common_meaningful_words_b.append(common_meaningful_b)\n\n        return np.array([common_meaningful_words_a, common_meaningful_words_b]).T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:27.193834Z","iopub.execute_input":"2025-08-31T10:13:27.194103Z","iopub.status.idle":"2025-08-31T10:13:27.690798Z","shell.execute_reply.started":"2025-08-31T10:13:27.194054Z","shell.execute_reply":"2025-08-31T10:13:27.690136Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(os.listdir(\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:27.691479Z","iopub.execute_input":"2025-08-31T10:13:27.691679Z","iopub.status.idle":"2025-08-31T10:13:27.696063Z","shell.execute_reply.started":"2025-08-31T10:13:27.691663Z","shell.execute_reply":"2025-08-31T10:13:27.695507Z"}},"outputs":[{"name":"stdout","text":"['config.json', 'assets', 'tokenizer.json', 'metadata.json', 'model.weights.h5']\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"embedder = HFEmbedder(model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\")\nX_train[\"prompt_clean\"] = X_train[\"prompt\"].apply(clean_text_for_common_words)\nX_train[\"response_a_clean\"] = X_train[\"response_a\"].apply(clean_text_for_common_words)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"].apply(clean_text_for_common_words)\n\ncommon_words_transformer = CommonWordsTransformer()\n# Updated preprocessing transformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", embedder, \"prompt_clean\"),\n    (\"resp_a_embed\", embedder, \"response_a_clean\"),\n    (\"resp_b_embed\", embedder, \"response_a_clean\"),\n    (\"common_words\", common_words_transformer, [\"prompt\", \"response_a\", \"response_b\"]),\n    #(\"num\", \"passthrough\", [\"id\"])\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:13:27.696784Z","iopub.execute_input":"2025-08-31T10:13:27.697023Z","iopub.status.idle":"2025-08-31T10:14:27.453876Z","shell.execute_reply.started":"2025-08-31T10:13:27.696995Z","shell.execute_reply":"2025-08-31T10:14:27.452993Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nI0000 00:00:1756635210.100071     306 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"preprocessing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:14:27.454839Z","iopub.execute_input":"2025-08-31T10:14:27.455908Z","iopub.status.idle":"2025-08-31T10:14:27.468778Z","shell.execute_reply.started":"2025-08-31T10:14:27.455877Z","shell.execute_reply":"2025-08-31T10:14:27.468208Z"},"editable":false},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"ColumnTransformer(transformers=[('prompt_embed',\n                                 HFEmbedder(model_path='/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3'),\n                                 'prompt_clean'),\n                                ('resp_a_embed',\n                                 HFEmbedder(model_path='/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3'),\n                                 'response_a_clean'),\n                                ('resp_b_embed',\n                                 HFEmbedder(model_path='/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3'),\n                                 'response_a_clean'),\n                                ('common_words', CommonWordsTransformer(),\n                                 ['prompt', 'response_a', 'response_b'])])","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;prompt_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;prompt_clean&#x27;),\n                                (&#x27;resp_a_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;response_a_clean&#x27;),\n                                (&#x27;resp_b_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;response_a_clean&#x27;),\n                                (&#x27;common_words&#x27;, CommonWordsTransformer(),\n                                 [&#x27;prompt&#x27;, &#x27;response_a&#x27;, &#x27;response_b&#x27;])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;prompt_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;prompt_clean&#x27;),\n                                (&#x27;resp_a_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;response_a_clean&#x27;),\n                                (&#x27;resp_b_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;response_a_clean&#x27;),\n                                (&#x27;common_words&#x27;, CommonWordsTransformer(),\n                                 [&#x27;prompt&#x27;, &#x27;response_a&#x27;, &#x27;response_b&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">prompt_embed</label><div class=\"sk-toggleable__content\"><pre>prompt_clean</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HFEmbedder</label><div class=\"sk-toggleable__content\"><pre>HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">resp_a_embed</label><div class=\"sk-toggleable__content\"><pre>response_a_clean</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HFEmbedder</label><div class=\"sk-toggleable__content\"><pre>HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">resp_b_embed</label><div class=\"sk-toggleable__content\"><pre>response_a_clean</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HFEmbedder</label><div class=\"sk-toggleable__content\"><pre>HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">common_words</label><div class=\"sk-toggleable__content\"><pre>[&#x27;prompt&#x27;, &#x27;response_a&#x27;, &#x27;response_b&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CommonWordsTransformer</label><div class=\"sk-toggleable__content\"><pre>CommonWordsTransformer()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"#feature_selection = SelectKBest(score_func = chi2, k=6)\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfeature_selection = SelectKBest(score_func=f_classif, k=6)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:14:27.472142Z","iopub.execute_input":"2025-08-31T10:14:27.472358Z","iopub.status.idle":"2025-08-31T10:14:27.475922Z","shell.execute_reply.started":"2025-08-31T10:14:27.472341Z","shell.execute_reply":"2025-08-31T10:14:27.475406Z"},"editable":false},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model = XGBClassifier(\n    objective=\"multi:softprob\",  \n    num_class=3,                  \n    eval_metric=\"mlogloss\",       \n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:14:27.476640Z","iopub.execute_input":"2025-08-31T10:14:27.476874Z","iopub.status.idle":"2025-08-31T10:14:27.491707Z","shell.execute_reply.started":"2025-08-31T10:14:27.476852Z","shell.execute_reply":"2025-08-31T10:14:27.490966Z"},"editable":false},"outputs":[],"execution_count":29},{"cell_type":"code","source":"my_pipeline = Pipeline([\n    (\"preprocessing\", preprocessing),\n    #(\"feature_selection\", feature_selection),\n    (\"model\", model)\n])\nmy_pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:14:27.492467Z","iopub.execute_input":"2025-08-31T10:14:27.492766Z","iopub.status.idle":"2025-08-31T10:14:27.529993Z","shell.execute_reply.started":"2025-08-31T10:14:27.492736Z","shell.execute_reply":"2025-08-31T10:14:27.529277Z"},"editable":false},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Pipeline(steps=[('preprocessing',\n                 ColumnTransformer(transformers=[('prompt_embed',\n                                                  HFEmbedder(model_path='/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3'),\n                                                  'prompt_clean'),\n                                                 ('resp_a_embed',\n                                                  HFEmbedder(model_path='/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3'),\n                                                  'response_a_clean'),\n                                                 ('resp_b_embed',\n                                                  HFEmbedder(model_path='/kaggle/in...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=0.1,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=6, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=300, n_jobs=None, num_class=3,\n                               num_parallel_tree=None, ...))])","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;prompt_embed&#x27;,\n                                                  HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                                  &#x27;prompt_clean&#x27;),\n                                                 (&#x27;resp_a_embed&#x27;,\n                                                  HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                                  &#x27;response_a_clean&#x27;),\n                                                 (&#x27;resp_b_embed&#x27;,\n                                                  HFEmbedder(model_path=&#x27;/kaggle/in...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=0.1,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=6, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=300, n_jobs=None, num_class=3,\n                               num_parallel_tree=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;prompt_embed&#x27;,\n                                                  HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                                  &#x27;prompt_clean&#x27;),\n                                                 (&#x27;resp_a_embed&#x27;,\n                                                  HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                                  &#x27;response_a_clean&#x27;),\n                                                 (&#x27;resp_b_embed&#x27;,\n                                                  HFEmbedder(model_path=&#x27;/kaggle/in...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=0.1,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=6, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=300, n_jobs=None, num_class=3,\n                               num_parallel_tree=None, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;prompt_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;prompt_clean&#x27;),\n                                (&#x27;resp_a_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;response_a_clean&#x27;),\n                                (&#x27;resp_b_embed&#x27;,\n                                 HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;),\n                                 &#x27;response_a_clean&#x27;),\n                                (&#x27;common_words&#x27;, CommonWordsTransformer(),\n                                 [&#x27;prompt&#x27;, &#x27;response_a&#x27;, &#x27;response_b&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">prompt_embed</label><div class=\"sk-toggleable__content\"><pre>prompt_clean</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HFEmbedder</label><div class=\"sk-toggleable__content\"><pre>HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">resp_a_embed</label><div class=\"sk-toggleable__content\"><pre>response_a_clean</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HFEmbedder</label><div class=\"sk-toggleable__content\"><pre>HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">resp_b_embed</label><div class=\"sk-toggleable__content\"><pre>response_a_clean</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HFEmbedder</label><div class=\"sk-toggleable__content\"><pre>HFEmbedder(model_path=&#x27;/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">common_words</label><div class=\"sk-toggleable__content\"><pre>[&#x27;prompt&#x27;, &#x27;response_a&#x27;, &#x27;response_b&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CommonWordsTransformer</label><div class=\"sk-toggleable__content\"><pre>CommonWordsTransformer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=300,\n              n_jobs=None, num_class=3, num_parallel_tree=None, ...)</pre></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"scores = cross_val_score(my_pipeline, X_train, y_train, cv=3, scoring=\"accuracy\")\nprint(\"Accuracy:\", scores.mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:14:27.530756Z","iopub.execute_input":"2025-08-31T10:14:27.531102Z","execution_failed":"2025-08-31T11:49:03.656Z"},"editable":false},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1756635302.058617     342 service.cc:148] XLA service 0x7cd4dc001510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1756635302.059668     342 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1756635303.097520     342 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1756635305.135775     342 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n✅ Model input names: ['padding_mask', 'token_ids']\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"my_pipeline.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-31T11:49:03.657Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"prompt_clean\"] = df_test[\"prompt\"].apply(clean_text_for_common_words)\ndf_test[\"response_a_clean\"] = df_test[\"response_a\"].apply(clean_text_for_common_words)\ndf_test[\"response_b_clean\"] = df_test[\"response_b\"].apply(clean_text_for_common_words)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-31T11:49:03.658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs = my_pipeline.predict_proba(df_test)\n\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"winner_model_a\": probs[:,0],\n    \"winner_model_b\": probs[:,1],\n    \"winner_tie\": probs[:,2],\n})\nprint(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-31T11:49:03.659Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DebertaV2Tokenizer\nimport torch\nimport numpy as np\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport tensorflow as tf\n\n# Configuration\nmodel_path = \"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\"\nvocab_file = f\"{model_path}/assets/tokenizer/vocabulary.spm\"\nweights_file = f\"{model_path}/model.weights.h5\"\n\n# Initialize tokenizer\ntokenizer = DebertaV2Tokenizer(vocab_file=vocab_file)\nprint(\"✅ Tokenizer initialized:\", type(tokenizer))\n\n# Initialize model\nmodel = DebertaV3Backbone(\n    vocabulary_size=128100,\n    num_layers=12,\n    num_heads=6,\n    hidden_dim=384,\n    intermediate_dim=1536,\n    dropout=0.1,\n    max_sequence_length=512,\n    bucket_size=256\n)\nmodel.load_weights(weights_file)\nprint(\"✅ Model loaded and weights applied.\")\nprint(\"Model input names:\", [input.name for input in model.inputs])\n\n# Example input text\ntext = \"DeBERTa is a powerful transformer model.\"\n\n# Tokenize the input\ntokens = tokenizer(text, return_tensors=\"pt\")\nprint(\"✅ Tokenizer output keys:\", tokens.keys())\nprint(\"Input IDs:\", tokens[\"input_ids\"])\nprint(\"Attention Mask:\", tokens[\"attention_mask\"])\n\n# Convert token IDs to token strings\ninput_ids = tokens[\"input_ids\"][0]\ntoken_names = tokenizer.convert_ids_to_tokens(input_ids)\nprint(\"✅ Token names:\", token_names)\n\n# Convert PyTorch tensors to NumPy arrays (on CPU)\npadding_mask_np = tokens[\"attention_mask\"].cpu().numpy()\ntoken_ids_np = tokens[\"input_ids\"].cpu().numpy()\n\n# Convert to TensorFlow tensors\nmodel_inputs = {\n    \"padding_mask\": tf.convert_to_tensor(padding_mask_np),\n    \"token_ids\": tf.convert_to_tensor(token_ids_np)\n}\n\n# Run forward pass\nprint(\"🚀 Running model forward pass...\")\noutputs = model(model_inputs)\n\n# Inspect model output\nprint(\"✅ Model output type:\", type(outputs))\nif isinstance(outputs, dict):\n    print(\"Model output keys:\", outputs.keys())\n    if \"last_hidden_state\" in outputs:\n        hidden_state = outputs[\"last_hidden_state\"]\n        print(\"Last hidden state shape:\", hidden_state.shape)\n        # Mean pooling and convert to NumPy\n        embeddings = tf.reduce_mean(hidden_state, axis=1).numpy()\n        print(\"✅ Mean pooled embeddings shape:\", embeddings.shape)\n    else:\n        print(\"⚠️ Warning: 'last_hidden_state' not found in model output.\")\nelse:\n    print(\"⚠️ Model returned a non-dict output:\", outputs)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-31T11:49:03.659Z"}},"outputs":[],"execution_count":null}]}