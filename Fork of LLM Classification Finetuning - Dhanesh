{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":205020,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":4685,"modelId":2820}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:05.203235Z","iopub.execute_input":"2025-09-04T10:40:05.203550Z","iopub.status.idle":"2025-09-04T10:40:05.219313Z","shell.execute_reply.started":"2025-09-04T10:40:05.203532Z","shell.execute_reply":"2025-09-04T10:40:05.218544Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3/assets/tokenizer/vocabulary.spm\n/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n","output_type":"stream"}],"execution_count":126},{"cell_type":"code","source":"\n\n#!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\n\n#model = SentenceTransformer(\"all-mpnet-base-v2\")\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:05.220688Z","iopub.execute_input":"2025-09-04T10:40:05.220882Z","iopub.status.idle":"2025-09-04T10:40:05.224699Z","shell.execute_reply.started":"2025-09-04T10:40:05.220865Z","shell.execute_reply":"2025-09-04T10:40:05.223807Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"\n#model.save(\"/kaggle/working/all-mpnet-base-v2\")\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:05.225592Z","iopub.execute_input":"2025-09-04T10:40:05.226178Z","iopub.status.idle":"2025-09-04T10:40:05.240024Z","shell.execute_reply.started":"2025-09-04T10:40:05.226154Z","shell.execute_reply":"2025-09-04T10:40:05.239150Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/qwen-llm\")\n#model = AutoModel.from_pretrained(\"/kaggle/input/qwen-llm\")\n\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:05.241492Z","iopub.execute_input":"2025-09-04T10:40:05.241701Z","iopub.status.idle":"2025-09-04T10:40:05.253760Z","shell.execute_reply.started":"2025-09-04T10:40:05.241669Z","shell.execute_reply":"2025-09-04T10:40:05.253033Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"submission_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\nsubmission_test","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:05.254562Z","iopub.execute_input":"2025-09-04T10:40:05.255041Z","iopub.status.idle":"2025-09-04T10:40:05.277892Z","shell.execute_reply.started":"2025-09-04T10:40:05.255017Z","shell.execute_reply":"2025-09-04T10:40:05.277354Z"}},"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.333333        0.333333    0.333333\n1   211333        0.333333        0.333333    0.333333\n2  1233961        0.333333        0.333333    0.333333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":130},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ndf_train","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:05.278599Z","iopub.execute_input":"2025-09-04T10:40:05.278814Z","iopub.status.idle":"2025-09-04T10:40:07.053326Z","shell.execute_reply.started":"2025-09-04T10:40:05.278796Z","shell.execute_reply":"2025-09-04T10:40:07.052610Z"}},"outputs":[{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":131},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf_test","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.054014Z","iopub.execute_input":"2025-09-04T10:40:07.054270Z","iopub.status.idle":"2025-09-04T10:40:07.064280Z","shell.execute_reply.started":"2025-09-04T10:40:07.054252Z","shell.execute_reply":"2025-09-04T10:40:07.063599Z"}},"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>[\"I have three oranges today, I ate an orange ...</td>\n      <td>[\"You have two oranges today.\"]</td>\n      <td>[\"You still have three oranges. Eating an oran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>[\"You are a mediator in a heated political deb...</td>\n      <td>[\"Thank you for sharing the details of the sit...</td>\n      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>[\"How to initialize the classification head wh...</td>\n      <td>[\"When you want to initialize the classificati...</td>\n      <td>[\"To initialize the classification head when p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":132},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.065164Z","iopub.execute_input":"2025-09-04T10:40:07.065450Z","iopub.status.idle":"2025-09-04T10:40:07.105073Z","shell.execute_reply.started":"2025-09-04T10:40:07.065425Z","shell.execute_reply":"2025-09-04T10:40:07.104251Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              57477 non-null  int64 \n 1   model_a         57477 non-null  object\n 2   model_b         57477 non-null  object\n 3   prompt          57477 non-null  object\n 4   response_a      57477 non-null  object\n 5   response_b      57477 non-null  object\n 6   winner_model_a  57477 non-null  int64 \n 7   winner_model_b  57477 non-null  int64 \n 8   winner_tie      57477 non-null  int64 \ndtypes: int64(4), object(5)\nmemory usage: 3.9+ MB\n","output_type":"stream"}],"execution_count":133},{"cell_type":"code","source":"df_train=df_train.head(1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.106140Z","iopub.execute_input":"2025-09-04T10:40:07.106866Z","iopub.status.idle":"2025-09-04T10:40:07.110416Z","shell.execute_reply.started":"2025-09-04T10:40:07.106839Z","shell.execute_reply":"2025-09-04T10:40:07.109723Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.112916Z","iopub.execute_input":"2025-09-04T10:40:07.113527Z","iopub.status.idle":"2025-09-04T10:40:07.130237Z","shell.execute_reply.started":"2025-09-04T10:40:07.113500Z","shell.execute_reply":"2025-09-04T10:40:07.129686Z"}},"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"           id             model_a              model_b  \\\n0       30192  gpt-4-1106-preview           gpt-4-0613   \n1       53567           koala-13b           gpt-4-0613   \n2       65089  gpt-3.5-turbo-0613       mistral-medium   \n3       96401    llama-2-13b-chat  mistral-7b-instruct   \n4      198779           koala-13b   gpt-3.5-turbo-0314   \n..        ...                 ...                  ...   \n995  74274070  gpt-4-1106-preview           claude-2.1   \n996  74321866         chatglm3-6b   gpt-3.5-turbo-1106   \n997  74458627  gpt-4-1106-preview           claude-2.0   \n998  74492564          chatglm-6b            vicuna-7b   \n999  74521474          claude-2.0   gpt-4-1106-preview   \n\n                                                prompt  \\\n0    [\"Is it morally right to try to have a certain...   \n1    [\"What is the difference between marriage lice...   \n2    [\"explain function calling. how would you call...   \n3    [\"How can I create a test set for a very rare ...   \n4    [\"What is the best way to travel from Tel-Aviv...   \n..                                                 ...   \n995  [\"Compare modern concept of gender identity to...   \n996                                 [\"how do you do?\"]   \n997  [\"Implement a h264 video decoder in pure Rust....   \n998  [\"Write these subsections in a master's thesis...   \n999  [\"Implement the Karatsuba algorithm with comma...   \n\n                                            response_a  \\\n0    [\"The question of whether it is morally right ...   \n1    [\"A marriage license is a legal document that ...   \n2    [\"Function calling is the process of invoking ...   \n3    [\"Creating a test set for a very rare category...   \n4    [\"The best way to travel from Tel Aviv to Jeru...   \n..                                                 ...   \n995  [\"Comparing the modern concept of gender ident...   \n996  [\"Hello! As an AI language model, I don't have...   \n997  [\"Creating a full H.264 video decoder from scr...   \n998  [\"Title: Evaluation of Machine Translation Met...   \n999  [\"Here is an implementation of the Karatsuba a...   \n\n                                            response_b  winner_model_a  \\\n0    [\"As an AI, I don't have personal beliefs or o...               1   \n1    [\"A marriage license and a marriage certificat...               0   \n2    [\"Function calling is the process of invoking ...               0   \n3    [\"When building a classifier for a very rare c...               1   \n4    [\"The best way to travel from Tel-Aviv to Jeru...               0   \n..                                                 ...             ...   \n995  [\"I do not feel comfortable making definitive ...               1   \n996  [\"I'm just a computer program, so I don't have...               0   \n997  [\"Here is an outline of how to implement a h26...               1   \n998  [\"Sure, here are the revised subsections in a ...               0   \n999  [\"Certainly! Below is a C++ implementation of ...               0   \n\n     winner_model_b  winner_tie  \n0                 0           0  \n1                 1           0  \n2                 0           1  \n3                 0           0  \n4                 1           0  \n..              ...         ...  \n995               0           0  \n996               0           1  \n997               0           0  \n998               0           1  \n999               1           0  \n\n[1000 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>74274070</td>\n      <td>gpt-4-1106-preview</td>\n      <td>claude-2.1</td>\n      <td>[\"Compare modern concept of gender identity to...</td>\n      <td>[\"Comparing the modern concept of gender ident...</td>\n      <td>[\"I do not feel comfortable making definitive ...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>74321866</td>\n      <td>chatglm3-6b</td>\n      <td>gpt-3.5-turbo-1106</td>\n      <td>[\"how do you do?\"]</td>\n      <td>[\"Hello! As an AI language model, I don't have...</td>\n      <td>[\"I'm just a computer program, so I don't have...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>74458627</td>\n      <td>gpt-4-1106-preview</td>\n      <td>claude-2.0</td>\n      <td>[\"Implement a h264 video decoder in pure Rust....</td>\n      <td>[\"Creating a full H.264 video decoder from scr...</td>\n      <td>[\"Here is an outline of how to implement a h26...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>74492564</td>\n      <td>chatglm-6b</td>\n      <td>vicuna-7b</td>\n      <td>[\"Write these subsections in a master's thesis...</td>\n      <td>[\"Title: Evaluation of Machine Translation Met...</td>\n      <td>[\"Sure, here are the revised subsections in a ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>74521474</td>\n      <td>claude-2.0</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"Implement the Karatsuba algorithm with comma...</td>\n      <td>[\"Here is an implementation of the Karatsuba a...</td>\n      <td>[\"Certainly! Below is a C++ implementation of ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":135},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.130906Z","iopub.execute_input":"2025-09-04T10:40:07.131091Z","iopub.status.idle":"2025-09-04T10:40:07.147034Z","shell.execute_reply.started":"2025-09-04T10:40:07.131075Z","shell.execute_reply":"2025-09-04T10:40:07.146231Z"}},"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"id                0\nmodel_a           0\nmodel_b           0\nprompt            0\nresponse_a        0\nresponse_b        0\nwinner_model_a    0\nwinner_model_b    0\nwinner_tie        0\ndtype: int64"},"metadata":{}}],"execution_count":136},{"cell_type":"code","source":"X = df_train.drop(['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie'], axis = 1)\nX","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.147784Z","iopub.execute_input":"2025-09-04T10:40:07.147968Z","iopub.status.idle":"2025-09-04T10:40:07.167034Z","shell.execute_reply.started":"2025-09-04T10:40:07.147952Z","shell.execute_reply":"2025-09-04T10:40:07.166386Z"}},"outputs":[{"execution_count":137,"output_type":"execute_result","data":{"text/plain":"           id                                             prompt  \\\n0       30192  [\"Is it morally right to try to have a certain...   \n1       53567  [\"What is the difference between marriage lice...   \n2       65089  [\"explain function calling. how would you call...   \n3       96401  [\"How can I create a test set for a very rare ...   \n4      198779  [\"What is the best way to travel from Tel-Aviv...   \n..        ...                                                ...   \n995  74274070  [\"Compare modern concept of gender identity to...   \n996  74321866                                 [\"how do you do?\"]   \n997  74458627  [\"Implement a h264 video decoder in pure Rust....   \n998  74492564  [\"Write these subsections in a master's thesis...   \n999  74521474  [\"Implement the Karatsuba algorithm with comma...   \n\n                                            response_a  \\\n0    [\"The question of whether it is morally right ...   \n1    [\"A marriage license is a legal document that ...   \n2    [\"Function calling is the process of invoking ...   \n3    [\"Creating a test set for a very rare category...   \n4    [\"The best way to travel from Tel Aviv to Jeru...   \n..                                                 ...   \n995  [\"Comparing the modern concept of gender ident...   \n996  [\"Hello! As an AI language model, I don't have...   \n997  [\"Creating a full H.264 video decoder from scr...   \n998  [\"Title: Evaluation of Machine Translation Met...   \n999  [\"Here is an implementation of the Karatsuba a...   \n\n                                            response_b  \n0    [\"As an AI, I don't have personal beliefs or o...  \n1    [\"A marriage license and a marriage certificat...  \n2    [\"Function calling is the process of invoking ...  \n3    [\"When building a classifier for a very rare c...  \n4    [\"The best way to travel from Tel-Aviv to Jeru...  \n..                                                 ...  \n995  [\"I do not feel comfortable making definitive ...  \n996  [\"I'm just a computer program, so I don't have...  \n997  [\"Here is an outline of how to implement a h26...  \n998  [\"Sure, here are the revised subsections in a ...  \n999  [\"Certainly! Below is a C++ implementation of ...  \n\n[1000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>74274070</td>\n      <td>[\"Compare modern concept of gender identity to...</td>\n      <td>[\"Comparing the modern concept of gender ident...</td>\n      <td>[\"I do not feel comfortable making definitive ...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>74321866</td>\n      <td>[\"how do you do?\"]</td>\n      <td>[\"Hello! As an AI language model, I don't have...</td>\n      <td>[\"I'm just a computer program, so I don't have...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>74458627</td>\n      <td>[\"Implement a h264 video decoder in pure Rust....</td>\n      <td>[\"Creating a full H.264 video decoder from scr...</td>\n      <td>[\"Here is an outline of how to implement a h26...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>74492564</td>\n      <td>[\"Write these subsections in a master's thesis...</td>\n      <td>[\"Title: Evaluation of Machine Translation Met...</td>\n      <td>[\"Sure, here are the revised subsections in a ...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>74521474</td>\n      <td>[\"Implement the Karatsuba algorithm with comma...</td>\n      <td>[\"Here is an implementation of the Karatsuba a...</td>\n      <td>[\"Certainly! Below is a C++ implementation of ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":137},{"cell_type":"code","source":"y = df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n\ny = np.argmax(y, axis=1)\ny","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.167806Z","iopub.execute_input":"2025-09-04T10:40:07.168743Z","iopub.status.idle":"2025-09-04T10:40:07.187225Z","shell.execute_reply.started":"2025-09-04T10:40:07.168723Z","shell.execute_reply":"2025-09-04T10:40:07.186647Z"}},"outputs":[{"execution_count":138,"output_type":"execute_result","data":{"text/plain":"array([0, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 0, 1, 2, 1, 2, 1,\n       2, 2, 2, 2, 2, 0, 1, 2, 0, 2, 1, 0, 0, 1, 0, 1, 2, 2, 0, 2, 0, 1,\n       1, 0, 1, 2, 2, 1, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 1, 0, 0, 2, 2, 1,\n       1, 2, 2, 0, 0, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 2, 0, 1, 1, 2, 0, 1,\n       0, 1, 2, 0, 2, 1, 0, 0, 1, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1,\n       0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 2, 2, 0, 2, 1, 1, 1, 0, 2,\n       0, 0, 1, 1, 1, 2, 0, 1, 1, 2, 0, 0, 2, 0, 1, 1, 1, 1, 0, 0, 2, 0,\n       0, 1, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 2, 1, 0, 1, 1, 0, 0, 2, 1,\n       2, 1, 0, 2, 0, 1, 1, 2, 0, 0, 1, 2, 2, 2, 2, 2, 1, 0, 0, 2, 0, 0,\n       2, 0, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 2, 2, 1, 0, 1, 0, 1, 2, 2, 1,\n       1, 0, 1, 1, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 1, 2, 1, 0, 1, 0, 0, 1,\n       1, 1, 2, 0, 0, 0, 2, 0, 1, 0, 1, 2, 2, 0, 2, 0, 2, 0, 1, 1, 0, 0,\n       0, 1, 0, 2, 1, 0, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 0, 0, 1,\n       0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 2,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 1, 1, 0, 2,\n       1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2,\n       0, 2, 1, 1, 2, 1, 0, 2, 1, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 0, 2, 2,\n       2, 1, 0, 1, 0, 0, 1, 2, 1, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 2,\n       1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 1, 0, 0, 2,\n       2, 0, 1, 2, 0, 0, 1, 0, 2, 2, 0, 1, 1, 0, 0, 0, 1, 2, 2, 0, 1, 1,\n       0, 1, 2, 0, 1, 1, 2, 1, 1, 2, 0, 1, 0, 2, 0, 0, 2, 0, 1, 1, 0, 0,\n       0, 2, 2, 0, 0, 2, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2, 0, 0, 0, 2, 2, 0,\n       0, 0, 2, 0, 1, 2, 2, 0, 0, 1, 0, 2, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0,\n       2, 1, 0, 1, 0, 1, 2, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 1, 1,\n       2, 2, 0, 2, 2, 1, 1, 1, 1, 1, 0, 2, 0, 2, 0, 2, 0, 1, 2, 1, 1, 0,\n       1, 0, 2, 0, 2, 2, 1, 0, 1, 0, 0, 0, 2, 1, 1, 2, 1, 1, 0, 0, 1, 1,\n       0, 1, 2, 0, 2, 2, 1, 1, 2, 0, 2, 0, 2, 1, 2, 2, 0, 1, 2, 1, 1, 1,\n       0, 2, 0, 2, 0, 1, 2, 2, 1, 1, 2, 0, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1,\n       2, 1, 1, 2, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 2, 1,\n       0, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 0, 1,\n       2, 1, 2, 2, 0, 1, 1, 2, 2, 1, 2, 0, 2, 2, 0, 1, 2, 2, 0, 0, 1, 0,\n       0, 1, 1, 1, 1, 1, 1, 0, 2, 0, 0, 2, 2, 0, 1, 2, 0, 1, 2, 2, 0, 2,\n       0, 1, 2, 2, 2, 1, 1, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0,\n       0, 2, 2, 0, 1, 1, 1, 1, 2, 1, 0, 1, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0,\n       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 1, 2, 2, 0, 1, 2, 1,\n       1, 2, 1, 0, 1, 2, 0, 1, 0, 1, 2, 0, 0, 2, 2, 2, 0, 0, 1, 0, 0, 0,\n       1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 1, 1, 0, 2, 1, 2, 0, 0, 0, 1,\n       0, 0, 2, 2, 2, 1, 2, 0, 2, 1, 0, 0, 2, 0, 1, 0, 1, 0, 2, 2, 2, 2,\n       0, 0, 1, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0, 2, 2, 0, 0, 0, 1, 2,\n       1, 0, 1, 2, 2, 1, 1, 2, 1, 2, 0, 2, 2, 1, 2, 1, 0, 2, 1, 2, 2, 1,\n       0, 1, 1, 0, 2, 0, 0, 2, 2, 1, 0, 2, 1, 2, 0, 2, 1, 0, 1, 1, 2, 0,\n       2, 0, 0, 1, 2, 2, 1, 1, 1, 0, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2,\n       2, 0, 2, 1, 2, 2, 2, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 2, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 2,\n       2, 1, 0, 0, 1, 0, 2, 0, 2, 1])"},"metadata":{}}],"execution_count":138},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42)","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.188164Z","iopub.execute_input":"2025-09-04T10:40:07.188425Z","iopub.status.idle":"2025-09-04T10:40:07.201880Z","shell.execute_reply.started":"2025-09-04T10:40:07.188396Z","shell.execute_reply":"2025-09-04T10:40:07.201216Z"}},"outputs":[],"execution_count":139},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.202573Z","iopub.execute_input":"2025-09-04T10:40:07.202821Z","iopub.status.idle":"2025-09-04T10:40:07.219049Z","shell.execute_reply.started":"2025-09-04T10:40:07.202798Z","shell.execute_reply":"2025-09-04T10:40:07.218358Z"}},"outputs":[{"execution_count":140,"output_type":"execute_result","data":{"text/plain":"           id                                             prompt  \\\n29    1827787  [\"Given a passage and some supplementary infor...   \n535  40014519  [\"act as a professional dog expert and content...   \n695  51618708  [\"write a paragraph that would invites anyone ...   \n557  41123370  [\"\\nimport random\\n\\ndef generate_random_edges...   \n836  61327490  [\"Provide a list of scifi, fantasy or mystery ...   \n..        ...                                                ...   \n106   7739903  [\"\\n\\nok, all you have to do is give me an SDG...   \n270  20960602  [\"This answers my question on the previous pag...   \n860  62750940                          [\"How to wear a bikini?\"]   \n435  32992211              [\"Would a smartphone work in space?\"]   \n102   7417163  [\"Give  10 useful suggestions how to best prep...   \n\n                                            response_a  \\\n29   [\"Refined passage: \\nThe person in the photo i...   \n535  [\"Here is a 150 word introduction for a video ...   \n695  [\"Attention all photography enthusiasts! Are y...   \n557  [\"To generate a fully connected graph with 289...   \n836  [\"If you enjoyed the works of Terry Pratchett,...   \n..                                                 ...   \n106  [\"Based on the project description provided, h...   \n270  [\"Yes, grouping variables can still be conside...   \n860  [\"Wearing a bikini can be a fun and comfortabl...   \n435  [\"Here are a few key points about whether smar...   \n102  [\"1. **Embrace lifelong learning**: Maintain a...   \n\n                                            response_b  \n29   [\"Refined passage: \\nThe person in the photo i...  \n535  [\"Welcome to our latest video on the fascinati...  \n695  [\"If you're someone who loves the thrill of th...  \n557  [\"Sure, I can help you modify the program to g...  \n836  [\"Here are some book recommendations based on ...  \n..                                                 ...  \n106  [\"Based on the description provided, here are ...  \n270  [\"In the context of creating a codebook for da...  \n860  [\"Here are the basic steps to wear a bikini:\\n...  \n435  [\"No, a standard smartphone would not work in ...  \n102  [\"1. Stay Informed: Keep up to date with the l...  \n\n[800 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>29</th>\n      <td>1827787</td>\n      <td>[\"Given a passage and some supplementary infor...</td>\n      <td>[\"Refined passage: \\nThe person in the photo i...</td>\n      <td>[\"Refined passage: \\nThe person in the photo i...</td>\n    </tr>\n    <tr>\n      <th>535</th>\n      <td>40014519</td>\n      <td>[\"act as a professional dog expert and content...</td>\n      <td>[\"Here is a 150 word introduction for a video ...</td>\n      <td>[\"Welcome to our latest video on the fascinati...</td>\n    </tr>\n    <tr>\n      <th>695</th>\n      <td>51618708</td>\n      <td>[\"write a paragraph that would invites anyone ...</td>\n      <td>[\"Attention all photography enthusiasts! Are y...</td>\n      <td>[\"If you're someone who loves the thrill of th...</td>\n    </tr>\n    <tr>\n      <th>557</th>\n      <td>41123370</td>\n      <td>[\"\\nimport random\\n\\ndef generate_random_edges...</td>\n      <td>[\"To generate a fully connected graph with 289...</td>\n      <td>[\"Sure, I can help you modify the program to g...</td>\n    </tr>\n    <tr>\n      <th>836</th>\n      <td>61327490</td>\n      <td>[\"Provide a list of scifi, fantasy or mystery ...</td>\n      <td>[\"If you enjoyed the works of Terry Pratchett,...</td>\n      <td>[\"Here are some book recommendations based on ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>7739903</td>\n      <td>[\"\\n\\nok, all you have to do is give me an SDG...</td>\n      <td>[\"Based on the project description provided, h...</td>\n      <td>[\"Based on the description provided, here are ...</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>20960602</td>\n      <td>[\"This answers my question on the previous pag...</td>\n      <td>[\"Yes, grouping variables can still be conside...</td>\n      <td>[\"In the context of creating a codebook for da...</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>62750940</td>\n      <td>[\"How to wear a bikini?\"]</td>\n      <td>[\"Wearing a bikini can be a fun and comfortabl...</td>\n      <td>[\"Here are the basic steps to wear a bikini:\\n...</td>\n    </tr>\n    <tr>\n      <th>435</th>\n      <td>32992211</td>\n      <td>[\"Would a smartphone work in space?\"]</td>\n      <td>[\"Here are a few key points about whether smar...</td>\n      <td>[\"No, a standard smartphone would not work in ...</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>7417163</td>\n      <td>[\"Give  10 useful suggestions how to best prep...</td>\n      <td>[\"1. **Embrace lifelong learning**: Maintain a...</td>\n      <td>[\"1. Stay Informed: Keep up to date with the l...</td>\n    </tr>\n  </tbody>\n</table>\n<p>800 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":140},{"cell_type":"code","source":"X_test","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.219983Z","iopub.execute_input":"2025-09-04T10:40:07.220235Z","iopub.status.idle":"2025-09-04T10:40:07.236566Z","shell.execute_reply.started":"2025-09-04T10:40:07.220217Z","shell.execute_reply":"2025-09-04T10:40:07.235909Z"}},"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"           id                                             prompt  \\\n521  38857249  [\"Trace the evolution of the ten main geopolit...   \n737  54551429  [\"Eight years ago, August was four times as ol...   \n740  54866362  [\"Here's a riddle.\\n\\nTry to reason carefully,...   \n660  49066516  [\"How should I train a reinforcement learning ...   \n411  31538450  [\"when was the first tanzania sara report publ...   \n..        ...                                                ...   \n408  31168210  [\"If I've got a 6 litre jug and a 12 litre jug...   \n332  25523100  [\"success or failure of eu energy efficiency p...   \n208  16441516  [\"element #1\\telement #2\\tcompound formed?\\tch...   \n613  45963597  [\"Write a Microsoft Word table with three colu...   \n78    5498037            [\"Why did the chicken cross the road?\"]   \n\n                                            response_a  \\\n521  [\"1. **Historical Rivalry:**\\n   - Centuries-o...   \n737  [\"Eight years ago, August was four times as ol...   \n740  [\"Based on the given information, we can reaso...   \n660  [\"Training a reinforcement learning bot to pla...   \n411  [\"The first Tanzania Social Action Fund (TASAF...   \n..                                                 ...   \n408  [\"Here is one way to measure out exactly 6 lit...   \n332  [\"The EU has had mixed success with its energy...   \n208  [\"element #1   element #2   compound formed?  ...   \n613  [\"I'm sorry, as an AI language model, I am una...   \n78                       [\"To get to the other side!\"]   \n\n                                            response_b  \n521  [\"1. Dissolution of the Soviet Union (1991): T...  \n737  [\"* 8 years ago, August was 4 times as old as ...  \n740  [\"In order to solve this riddle, let's break i...  \n660  [\"Training a reinforcement learning bot to pla...  \n411  [\"In Tanzania, the SARA was first published in...  \n..                                                 ...  \n408  [\"To measure out 6 litres using a 6 litre jug ...  \n332  [\"The success of EU energy efficiency policies...  \n208  [\"element #1\\telement #2\\tcompound formed?\\tch...  \n613  [\"Sure, here's how you could set up a table in...  \n78   [\"There are many possible reasons why a chicke...  \n\n[200 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>521</th>\n      <td>38857249</td>\n      <td>[\"Trace the evolution of the ten main geopolit...</td>\n      <td>[\"1. **Historical Rivalry:**\\n   - Centuries-o...</td>\n      <td>[\"1. Dissolution of the Soviet Union (1991): T...</td>\n    </tr>\n    <tr>\n      <th>737</th>\n      <td>54551429</td>\n      <td>[\"Eight years ago, August was four times as ol...</td>\n      <td>[\"Eight years ago, August was four times as ol...</td>\n      <td>[\"* 8 years ago, August was 4 times as old as ...</td>\n    </tr>\n    <tr>\n      <th>740</th>\n      <td>54866362</td>\n      <td>[\"Here's a riddle.\\n\\nTry to reason carefully,...</td>\n      <td>[\"Based on the given information, we can reaso...</td>\n      <td>[\"In order to solve this riddle, let's break i...</td>\n    </tr>\n    <tr>\n      <th>660</th>\n      <td>49066516</td>\n      <td>[\"How should I train a reinforcement learning ...</td>\n      <td>[\"Training a reinforcement learning bot to pla...</td>\n      <td>[\"Training a reinforcement learning bot to pla...</td>\n    </tr>\n    <tr>\n      <th>411</th>\n      <td>31538450</td>\n      <td>[\"when was the first tanzania sara report publ...</td>\n      <td>[\"The first Tanzania Social Action Fund (TASAF...</td>\n      <td>[\"In Tanzania, the SARA was first published in...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>408</th>\n      <td>31168210</td>\n      <td>[\"If I've got a 6 litre jug and a 12 litre jug...</td>\n      <td>[\"Here is one way to measure out exactly 6 lit...</td>\n      <td>[\"To measure out 6 litres using a 6 litre jug ...</td>\n    </tr>\n    <tr>\n      <th>332</th>\n      <td>25523100</td>\n      <td>[\"success or failure of eu energy efficiency p...</td>\n      <td>[\"The EU has had mixed success with its energy...</td>\n      <td>[\"The success of EU energy efficiency policies...</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>16441516</td>\n      <td>[\"element #1\\telement #2\\tcompound formed?\\tch...</td>\n      <td>[\"element #1   element #2   compound formed?  ...</td>\n      <td>[\"element #1\\telement #2\\tcompound formed?\\tch...</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>45963597</td>\n      <td>[\"Write a Microsoft Word table with three colu...</td>\n      <td>[\"I'm sorry, as an AI language model, I am una...</td>\n      <td>[\"Sure, here's how you could set up a table in...</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>5498037</td>\n      <td>[\"Why did the chicken cross the road?\"]</td>\n      <td>[\"To get to the other side!\"]</td>\n      <td>[\"There are many possible reasons why a chicke...</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":141},{"cell_type":"code","source":"y_train","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.237211Z","iopub.execute_input":"2025-09-04T10:40:07.237474Z","iopub.status.idle":"2025-09-04T10:40:07.251635Z","shell.execute_reply.started":"2025-09-04T10:40:07.237456Z","shell.execute_reply":"2025-09-04T10:40:07.251010Z"}},"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"array([2, 2, 2, 1, 0, 2, 1, 1, 2, 2, 1, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n       0, 1, 0, 2, 0, 2, 1, 2, 1, 1, 2, 0, 0, 2, 2, 0, 1, 0, 0, 2, 2, 1,\n       1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 0, 2, 0, 0, 2, 1, 1, 0, 2, 0, 1, 1,\n       0, 1, 0, 2, 2, 1, 2, 1, 0, 1, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2,\n       2, 2, 2, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 2, 2, 2, 1,\n       0, 0, 0, 1, 1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1,\n       2, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 1, 2, 0, 1, 1, 1, 0,\n       0, 2, 1, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n       0, 2, 2, 0, 1, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 2, 0, 1, 1, 2, 0, 2,\n       1, 0, 2, 2, 0, 1, 0, 2, 0, 1, 2, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 1,\n       2, 2, 0, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1,\n       1, 0, 0, 0, 1, 2, 1, 0, 1, 2, 0, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n       1, 2, 1, 2, 1, 0, 2, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n       0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 1, 0, 1, 1, 0, 1, 2, 1, 2, 2, 2, 0,\n       0, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 2, 0, 2, 0, 1, 1, 1,\n       1, 2, 2, 0, 2, 1, 0, 2, 2, 2, 2, 2, 1, 0, 0, 0, 2, 0, 0, 2, 0, 1,\n       0, 2, 0, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 2, 1, 2, 2, 2,\n       2, 1, 0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 2, 0, 2, 1, 0, 0, 2, 1, 0, 0,\n       1, 2, 1, 0, 1, 1, 0, 0, 2, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 2, 0, 0,\n       0, 0, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 2, 0, 0, 1, 0, 2, 2, 0, 0, 2,\n       2, 1, 1, 0, 2, 0, 1, 1, 0, 1, 0, 2, 2, 0, 1, 1, 0, 1, 1, 0, 2, 1,\n       0, 1, 1, 2, 1, 2, 2, 0, 2, 1, 2, 1, 0, 2, 0, 0, 2, 1, 2, 0, 1, 1,\n       2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 2, 0, 0, 1, 1, 0, 0, 1, 2, 2, 1, 1,\n       1, 1, 0, 1, 0, 1, 0, 2, 2, 1, 2, 0, 2, 0, 1, 0, 2, 1, 1, 0, 0, 2,\n       0, 1, 0, 0, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 0,\n       1, 2, 2, 1, 0, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 1, 1, 0,\n       0, 1, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 0, 2,\n       0, 2, 2, 0, 1, 0, 1, 0, 2, 0, 2, 1, 2, 2, 1, 1, 2, 1, 1, 0, 2, 0,\n       0, 2, 0, 2, 0, 0, 2, 2, 0, 2, 1, 2, 0, 1, 0, 1, 2, 2, 1, 0, 0, 2,\n       2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 1, 1, 2, 0, 1, 0, 1, 0, 2, 1,\n       0, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 2, 1, 2, 0, 2, 1, 0, 0,\n       0, 2, 0, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0,\n       0, 1, 2, 1, 1, 1, 1, 0, 2, 2, 0, 1, 2, 2, 2, 0, 1, 0, 1, 0, 2, 0,\n       0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 1, 2, 1, 1, 2, 0, 2, 2, 0, 0, 0, 2,\n       1, 0, 0, 0, 0, 2, 1, 2, 0, 1, 0, 2, 2, 0, 2, 0, 1, 1, 0, 1, 2, 2,\n       1, 2, 0, 2, 1, 0, 2, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1,\n       2, 0, 1, 2, 0, 1, 2, 1])"},"metadata":{}}],"execution_count":142},{"cell_type":"code","source":"catagorical_feature = [col for col in X.columns if X[col].dtype == 'object']\ncatagorical_feature","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.252359Z","iopub.execute_input":"2025-09-04T10:40:07.252561Z","iopub.status.idle":"2025-09-04T10:40:07.265502Z","shell.execute_reply.started":"2025-09-04T10:40:07.252546Z","shell.execute_reply":"2025-09-04T10:40:07.264744Z"}},"outputs":[{"execution_count":143,"output_type":"execute_result","data":{"text/plain":"['prompt', 'response_a', 'response_b']"},"metadata":{}}],"execution_count":143},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.266200Z","iopub.execute_input":"2025-09-04T10:40:07.266425Z","iopub.status.idle":"2025-09-04T10:40:07.279128Z","shell.execute_reply.started":"2025-09-04T10:40:07.266400Z","shell.execute_reply":"2025-09-04T10:40:07.278536Z"}},"outputs":[],"execution_count":144},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_name=\"all-mpnet-base-v2\"):\n        self.model_name = model_name  \n        self.model = SentenceTransformer(model_name)\n        if torch.cuda.is_available():\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self.model.encode(X.tolist(), show_progress_bar=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Example usage in ColumnTransformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", HFEmbedder(), \"prompt\"),\n    (\"resp_a_embed\", HFEmbedder(), \"response_a\"),\n    (\"resp_b_embed\", HFEmbedder(), \"response_b\"),\n    (\"num\", \"passthrough\", [\"id\"])\n])\n","metadata":{"execution":{"iopub.status.busy":"2025-08-23T12:09:38.196345Z","iopub.execute_input":"2025-08-23T12:09:38.197094Z","iopub.status.idle":"2025-08-23T12:09:43.999261Z","shell.execute_reply.started":"2025-08-23T12:09:38.197069Z","shell.execute_reply":"2025-08-23T12:09:43.998604Z"},"editable":false}},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/open-minilm-l6-v2\",use_auth_token=False , local_files_only=True,batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.model = SentenceTransformer(self.model_path)\n        if self.use_cuda:\n            self.model = self.model.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            embeddings = self.model.encode(\n                batch,\n                show_progress_bar=False,\n                device=\"cuda\" if self.use_cuda else \"cpu\"\n            )\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-24T00:06:01.592096Z","iopub.execute_input":"2025-08-24T00:06:01.592776Z","iopub.status.idle":"2025-08-24T00:06:01.599761Z","shell.execute_reply.started":"2025-08-24T00:06:01.592751Z","shell.execute_reply":"2025-08-24T00:06:01.59901Z"},"editable":false}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_deberta_v3_model(config):\n    # Extract config values\n    vocab_size = config[\"vocabulary_size\"]\n    num_layers = config[\"num_layers\"]\n    num_heads = config[\"num_heads\"]\n    hidden_dim = config[\"hidden_dim\"]\n    intermediate_dim = config[\"intermediate_dim\"]\n    dropout_rate = config[\"dropout\"]\n    max_seq_len = config[\"max_sequence_length\"]\n\n    # Input layers\n    input_ids = layers.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = layers.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"attention_mask\")\n\n    # Embedding layer\n    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=hidden_dim)(input_ids)\n\n    # Positional encoding (simplified)\n    position_embeddings = layers.Embedding(input_dim=max_seq_len, output_dim=hidden_dim)(tf.range(start=0, limit=max_seq_len, delta=1))\n    position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n    x = embedding_layer + position_embeddings\n\n    # Transformer blocks\n    for _ in range(num_layers):\n        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)(x, x, attention_mask=attention_mask)\n        attention_output = layers.Dropout(dropout_rate)(attention_output)\n        attention_output = layers.LayerNormalization()(x + attention_output)\n\n        ffn_output = layers.Dense(intermediate_dim, activation='gelu')(attention_output)\n        ffn_output = layers.Dense(hidden_dim)(ffn_output)\n        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n        x = layers.LayerNormalization()(attention_output + ffn_output)\n\n    # Output (for embedding use, you might just return x)\n    pooled_output = layers.GlobalAveragePooling1D()(x)\n\n    model = models.Model(inputs=[input_ids, attention_mask], outputs=pooled_output)\n    return model\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.279808Z","iopub.execute_input":"2025-09-04T10:40:07.280024Z","iopub.status.idle":"2025-09-04T10:40:07.292843Z","shell.execute_reply.started":"2025-09-04T10:40:07.280003Z","shell.execute_reply":"2025-09-04T10:40:07.292153Z"}},"outputs":[],"execution_count":145},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import DebertaV2Tokenizer\nfrom tensorflow.keras.models import load_model\nimport torch\nimport numpy as np\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\nconfig = {\n    \"vocabulary_size\": 128100,\n    \"num_layers\": 12,\n    \"num_heads\": 6,\n    \"hidden_dim\": 384,\n    \"intermediate_dim\": 1536,\n    \"dropout\": 0.1,\n    \"max_sequence_length\": 512,\n    \"bucket_size\": 256\n}\n\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=8, use_cuda=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.use_cuda = use_cuda and torch.cuda.is_available()\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/assets/tokenizer/vocabulary.spm\")\n        #self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        #self.model = AutoModel.from_pretrained(self.model_path)\n        self.model =  DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n  # Define your model architecture here\n        self.model.load_weights(\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3/model.weights.h5\")\n        print([input.name for input in self.model.inputs])\n\n        if self.use_cuda:\n            self.model = self.model#.to(\"cuda\")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            \n            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            \n            # Rename keys to match model input expectations\n            model_inputs = {\n                \"padding_mask\": inputs[\"attention_mask\"],\n                \"token_ids\": inputs[\"input_ids\"]\n            }\n            \n            if self.use_cuda:\n                model_inputs = {k: v.to(\"cuda\") for k, v in model_inputs.items()}\n            \n            with torch.no_grad():\n                outputs = self.model(model_inputs) \n\n            # Mean pooling over token embeddings\n            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2025-09-02T08:53:23.227426Z","iopub.execute_input":"2025-09-02T08:53:23.227655Z","iopub.status.idle":"2025-09-02T08:53:24.187917Z","shell.execute_reply.started":"2025-09-02T08:53:23.227636Z","shell.execute_reply":"2025-09-02T08:53:24.187308Z"}}},{"cell_type":"markdown","source":"from transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4):\n        self.model_path = model_path\n        self.batch_size = batch_size\n\n        # Load tokenizer\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n\n        # Load model\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X.tolist()\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize using PyTorch tokenizer\n            tokens = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n\n            # Convert PyTorch tensors to NumPy arrays\n            padding_mask_np = tokens[\"attention_mask\"].cpu().numpy()\n            token_ids_np = tokens[\"input_ids\"].cpu().numpy()\n\n            # Convert to TensorFlow tensors\n            model_inputs = {\n                \"padding_mask\": tf.convert_to_tensor(padding_mask_np),\n                \"token_ids\": tf.convert_to_tensor(token_ids_np)\n            }\n\n            # Forward pass\n            outputs = self.model(model_inputs)  # returns a tensor directly\n\n            # Mean pooling and convert to NumPy\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n# ...existing code...\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4, max_length=512):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        # enforce and cap max token length to backbone capacity\n        self.max_length = min(int(max_length), 512)\n\n        # Load tokenizer and set model max length\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        all_embeddings = []\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize with fixed max length so TF model receives fixed-size inputs\n            tokens = self.tokenizer(\n                batch,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\",\n            )\n\n            # Convert PyTorch tensors to NumPy arrays\n            padding_mask_np = tokens[\"attention_mask\"].cpu().numpy()\n            token_ids_np = tokens[\"input_ids\"].cpu().numpy()\n\n            # Convert to TensorFlow tensors for the Keras backbone\n            model_inputs = {\n                \"padding_mask\": tf.convert_to_tensor(padding_mask_np),\n                \"token_ids\": tf.convert_to_tensor(token_ids_np)\n            }\n\n            # Forward pass (returns tensor [batch, seq_len, hidden])\n            outputs = self.model(model_inputs)\n\n            # Mean pooling across sequence length, convert to NumPy\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n\n\n# ...existing code...\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\", batch_size=4, max_length=512):\n        import tensorflow as tf\n        import numpy as np\n\n        self.model_path = model_path\n        self.batch_size = batch_size\n        # enforce and cap max token length to backbone capacity\n        self.max_length = min(int(max_length), 512)\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # Load tokenizer and set model max length\n        self.tokenizer = DebertaV2Tokenizer(vocab_file=f\"{self.model_path}/assets/tokenizer/vocabulary.spm\")\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(f\"{self.model_path}/model.weights.h5\")\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        all_embeddings = []\n        hidden_dim = None\n        try:\n            # Try to infer hidden dim from model output_shape (Keras style)\n            hidden_dim = int(self.model.output_shape[-1])\n        except Exception:\n            hidden_dim = None\n\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            # Tokenize directly to TensorFlow tensors to avoid PyTorch tensors and extra copies\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",   # <-- use \"tf\" to get TF tensors directly\n                )\n            except Exception:\n                # fallback: tokenize each item separately and return numpy then convert to tf\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                # convert np arrays to tf tensors\n                tokens = {k: tf.convert_to_tensor(v, dtype=tf.int32) for k, v in tokens.items()}\n\n            # Ensure integer dtype expected by TF/Keras\n            token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n            attention_mask_tf = tf.cast(tokens[\"attention_mask\"], tf.int32)\n\n            model_inputs = {\n                \"padding_mask\": attention_mask_tf,\n                \"token_ids\": token_ids_tf\n            }\n\n            # Forward pass (Keras/TF will place ops on GPU if available)\n            outputs = self.model(model_inputs)  # [batch, seq_len, hidden]\n\n            # Mean pooling across sequence length, convert to NumPy for downstream sklearn pipelines\n            embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            all_embeddings.append(embeddings)\n\n            # update hidden_dim if unknown\n            if hidden_dim is None:\n                try:\n                    hidden_dim = int(embeddings.shape[1])\n                except Exception:\n                    hidden_dim = None\n\n        if not all_embeddings:\n            # return an empty 2D array with known hidden dim if possible\n            if hidden_dim is not None:\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            return np.empty((0,))\n\n        return np.vstack(all_embeddings)\n# ...existing\n\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport os # Import os module\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path, batch_size=8, max_length=512, use_fast_tokenizer=True, enable_mixed_precision=True):\n        import tensorflow as tf\n        import numpy as np\n        import os\n        import multiprocessing\n\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision # Add this line to store the parameter\n        self._cpu_count = multiprocessing.cpu_count()\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # optionally use mixed precision on GPUs (speeds up fp16 capable GPUs)\n        try:\n            if self.enable_mixed_precision: # Use self.enable_mixed_precision\n                from tensorflow.keras import mixed_precision\n                mixed_precision.set_global_policy(\"mixed_float16\")\n        except Exception:\n            pass\n\n        # Load tokenizer (prefer fast tokenizer if available)\n        try:\n            if self.use_fast_tokenizer:\n                from transformers import DebertaV2TokenizerFast as _TokFast\n                self.tokenizer = _TokFast(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            else:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n        except Exception:\n            # fallback to original import name/location\n            try:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            except Exception:\n                raise\n\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=12,\n            num_heads=6,\n            hidden_dim=384,\n            intermediate_dim=1536,\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n        self.model.load_weights(os.path.join(self.model_path, \"model.weights.h5\")) # Use os.path.join\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            # no data -> return empty with inferred dim if possible\n            try:\n                hidden_dim = int(self.model.output_shape[-1])\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            except Exception:\n                return np.empty((0,))\n\n        # If tokenizer is \"fast\" we can tokenize the whole dataset in one call (fast, Rust-backed)\n        try:\n            if self.use_fast_tokenizer:\n                # Vectorized tokenization to numpy arrays (fast)\n                tokens_np = self.tokenizer(\n                    texts,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                # Build a tf.data.Dataset to feed the model efficiently (reduces Python overhead)\n                input_ids = tf.cast(tokens_np[\"input_ids\"], tf.int32)\n                attention_mask = tf.cast(tokens_np[\"attention_mask\"], tf.int32)\n\n                ds = tf.data.Dataset.from_tensor_slices(\n                    {\"token_ids\": input_ids, \"padding_mask\": attention_mask}\n                )\n                ds = ds.batch(max(1, self.batch_size)).prefetch(tf.data.AUTOTUNE)\n\n                # Use model.predict on the dataset so Keras can optimize execution and improve GPU utilization\n                outputs = self.model.predict(ds, verbose=0)\n                embeddings = np.mean(outputs, axis=1)\n                return embeddings\n        except Exception:\n            # fall back to batching loop below if anything fails\n            pass\n\n        # Fallback: batch-tokenize and call model per-batch (keeps fewer copies)\n        all_embeddings = []\n        hidden_dim = None\n        try:\n            hidden_dim = int(self.model.output_shape[-1])\n        except Exception:\n            hidden_dim = None\n\n        for i in range(0, n, self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            # Tokenize directly to TensorFlow tensors where possible (reduces copies)\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",\n                )\n                token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n                attention_mask_tf = tf.cast(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            except Exception:\n                # safest fallback: numpy tokenization then tf convert\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                token_ids_tf = tf.convert_to_tensor(tokens[\"input_ids\"], dtype=tf.int32)\n                attention_mask_tf = tf.convert_to_tensor(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n\n            all_embeddings.append(embeddings)\n            if hidden_dim is None:\n                try:\n                    hidden_dim = int(embeddings.shape[1])\n                except Exception:\n                    hidden_dim = None\n\n        if not all_embeddings:\n            if hidden_dim is not None:\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            return np.empty((0,))\n\n        return np.vstack(all_embeddings)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2025-09-01T11:11:36.366153Z","iopub.execute_input":"2025-09-01T11:11:36.366354Z","iopub.status.idle":"2025-09-01T11:11:36.402906Z","shell.execute_reply.started":"2025-09-01T11:11:36.366339Z","shell.execute_reply":"2025-09-01T11:11:36.402327Z"}}},{"cell_type":"code","source":"\n\nfrom transformers import DebertaV2Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport torch\nimport numpy as np\nimport tensorflow as tf\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport os # Import os module\n\nclass HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3\", batch_size=16, max_length=512, use_fast_tokenizer=True, enable_mixed_precision=True):\n        import tensorflow as tf\n        import numpy as np\n        import os\n        import multiprocessing\n\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision # Add this line to store the parameter\n        self._cpu_count = multiprocessing.cpu_count()\n\n        # Make TF GPU usage explicit / safe\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # optionally use mixed precision on GPUs (speeds up fp16 capable GPUs)\n        try:\n            if self.enable_mixed_precision: # Use self.enable_mixed_precision\n                from tensorflow.keras import mixed_precision\n                mixed_precision.set_global_policy(\"mixed_float16\")\n        except Exception:\n            pass\n\n        # Load tokenizer (prefer fast tokenizer if available)\n        try:\n            if self.use_fast_tokenizer:\n                from transformers import DebertaV2TokenizerFast as _TokFast\n                self.tokenizer = _TokFast(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            else:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n        except Exception:\n            # fallback to original import name/location\n            try:\n                from transformers import DebertaV2Tokenizer as _Tok\n                self.tokenizer = _Tok(vocab_file=os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")) # Use os.path.join\n            except Exception:\n                raise\n\n        self.tokenizer.model_max_length = self.max_length\n\n        # Load model backbone (Keras)\n        config = self._detect_model_config()\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=config[\"num_layers\"],\n            num_heads=config[\"num_heads\"],\n            hidden_dim=config[\"hidden_dim\"],\n            intermediate_dim=config[\"intermediate_dim\"],\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n\n\n        self.model.load_weights(os.path.join(self.model_path, \"model.weights.h5\")) # Use os.path.join\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n        # optional: warm-up call with zeros to ensure TF places variables on GPU if available\n        try:\n            import tensorflow as tf\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n    def _detect_model_config(self):\n            import os\n            import json\n        \n            config_path = os.path.join(self.model_path, \"config.json\")\n            if os.path.exists(config_path):\n                try:\n                    with open(config_path, \"r\") as f:\n                        config = json.load(f)\n                    return {\n                        \"num_layers\": config.get(\"num_layers\", 12),\n                        \"num_heads\": config.get(\"num_attention_heads\", 12),\n                        \"hidden_dim\": config.get(\"hidden_size\", 768),\n                        \"intermediate_dim\": config.get(\"intermediate_size\", 3072),\n                    }\n                except Exception as e:\n                    print(f\"⚠️ Failed to read config.json: {e}\")\n        \n            # Fallback: infer from folder name\n            path_lower = self.model_path.lower()\n            if \"small\" in path_lower:\n                return {\n                    \"num_layers\": 12,\n                    \"num_heads\": 6,\n                    \"hidden_dim\": 768,\n                    \"intermediate_dim\": 3072,\n                }\n            elif \"base\" in path_lower:\n                return {\n                    \"num_layers\": 12,\n                    \"num_heads\": 12,\n                    \"hidden_dim\": 768,\n                    \"intermediate_dim\": 3072,\n                }\n        \n            print(\"⚠️ Could not auto-detect model config. Using default base config.\")\n            return {\n                \"num_layers\": 12,\n                \"num_heads\": 12,\n                \"hidden_dim\": 768,\n                \"intermediate_dim\": 3072,\n            }\n\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n\n        # Accept pandas Series / DataFrame / numpy array / list\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        # Coerce each row to a single text string.\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                if len(item) == 0:\n                    return \"\"\n                if len(item) == 1:\n                    return str(item[0])\n                return \" \".join(str(x) for x in item)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            # no data -> return empty with inferred dim if possible\n            try:\n                hidden_dim = int(self.model.output_shape[-1])\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            except Exception:\n                return np.empty((0,))\n\n        # If tokenizer is \"fast\" we can tokenize the whole dataset in one call (fast, Rust-backed)\n        try:\n            if self.use_fast_tokenizer:\n                # Vectorized tokenization to numpy arrays (fast)\n                tokens_np = self.tokenizer(\n                    texts,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                # Build a tf.data.Dataset to feed the model efficiently (reduces Python overhead)\n                input_ids = tf.cast(tokens_np[\"input_ids\"], tf.int32)\n                attention_mask = tf.cast(tokens_np[\"attention_mask\"], tf.int32)\n\n                ds = tf.data.Dataset.from_tensor_slices(\n                    {\"token_ids\": input_ids, \"padding_mask\": attention_mask}\n                )\n                ds = ds.batch(max(1, self.batch_size)).prefetch(tf.data.AUTOTUNE)\n\n                # Use model.predict on the dataset so Keras can optimize execution and improve GPU utilization\n                outputs = self.model.predict(ds, verbose=0)\n                embeddings = np.mean(outputs, axis=1)\n                return embeddings\n        except Exception:\n            # fall back to batching loop below if anything fails\n            pass\n\n        # Fallback: batch-tokenize and call model per-batch (keeps fewer copies)\n        all_embeddings = []\n        hidden_dim = None\n        try:\n            hidden_dim = int(self.model.output_shape[-1])\n        except Exception:\n            hidden_dim = None\n\n        for i in range(0, n, self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            # Tokenize directly to TensorFlow tensors where possible (reduces copies)\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",\n                )\n                token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n                attention_mask_tf = tf.cast(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            except Exception:\n                # safest fallback: numpy tokenization then tf convert\n                tokens = self.tokenizer(\n                    [str(t) for t in batch],\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                token_ids_tf = tf.convert_to_tensor(tokens[\"input_ids\"], dtype=tf.int32)\n                attention_mask_tf = tf.convert_to_tensor(tokens[\"attention_mask\"], dtype=tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n\n            all_embeddings.append(embeddings)\n            if hidden_dim is None:\n                try:\n                    hidden_dim = int(embeddings.shape[1])\n                except Exception:\n                    hidden_dim = None\n\n        if not all_embeddings:\n            if hidden_dim is not None:\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            return np.empty((0,))\n\n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.293737Z","iopub.execute_input":"2025-09-04T10:40:07.293983Z","iopub.status.idle":"2025-09-04T10:40:07.317136Z","shell.execute_reply.started":"2025-09-04T10:40:07.293962Z","shell.execute_reply":"2025-09-04T10:40:07.316342Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom transformers import TFAutoModel, AutoTokenizer\nimport tensorflow as tf\nimport numpy as np\n\nclass HFEmbedder1(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path=\"microsoft/deberta-v3-xsmall\", batch_size=4, max_length=512, enable_mixed_precision=True):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.max_length = max_length\n        self.enable_mixed_precision = enable_mixed_precision\n\n        # Enable mixed precision if requested\n        if self.enable_mixed_precision:\n            try:\n                from tensorflow.keras import mixed_precision\n                mixed_precision.set_global_policy(\"mixed_float16\")\n            except Exception:\n                pass\n\n        # Load tokenizer and model from HuggingFace\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.model = TFAutoModel.from_pretrained(self.model_path)\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import pandas as pd\n    \n        # Handle different input types from ColumnTransformer\n        if isinstance(X, pd.DataFrame):\n            # If multiple columns are passed, take the first one\n            if X.shape[1] > 1:\n                texts = X.iloc[:, 0].astype(str).tolist()\n            else:\n                texts = X.squeeze().astype(str).tolist()\n        elif hasattr(X, \"to_list\"):\n            texts = X.astype(str).to_list()\n        else:\n            texts = [str(x) for x in X]\n    \n        if not texts:\n            return np.empty((0, self.model.config.hidden_size), dtype=np.float32)\n    \n        # Tokenize\n        tokens = self.tokenizer(\n            texts,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"tf\"\n        )\n    \n        # Forward pass\n        outputs = self.model(**tokens)\n        embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()\n    \n        return embeddings\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.317991Z","iopub.execute_input":"2025-09-04T10:40:07.318339Z","iopub.status.idle":"2025-09-04T10:40:07.334976Z","shell.execute_reply.started":"2025-09-04T10:40:07.318290Z","shell.execute_reply":"2025-09-04T10:40:07.334331Z"}},"outputs":[],"execution_count":147},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text_for_common_words(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    tokens = text.split()\n    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n\n\nclass CommonWordsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        common_meaningful_words_a = []\n        common_meaningful_words_b = []\n\n        for index, row in X.iterrows():\n            prompt_tokens = clean_text_for_common_words(row['prompt'])\n            response_a_tokens = clean_text_for_common_words(row['response_a'])\n            response_b_tokens = clean_text_for_common_words(row['response_b'])\n\n            common_meaningful_a = len(set(prompt_tokens) & set(response_a_tokens))\n            common_meaningful_b = len(set(prompt_tokens) & set(response_b_tokens))\n\n            common_meaningful_words_a.append(common_meaningful_a)\n            common_meaningful_words_b.append(common_meaningful_b)\n\n        return np.array([common_meaningful_words_a, common_meaningful_words_b]).T","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.335622Z","iopub.execute_input":"2025-09-04T10:40:07.335883Z","iopub.status.idle":"2025-09-04T10:40:07.355233Z","shell.execute_reply.started":"2025-09-04T10:40:07.335858Z","shell.execute_reply":"2025-09-04T10:40:07.354531Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":148},{"cell_type":"code","source":"X_train[\"prompt_clean\"] = X_train[\"prompt\"].apply(clean_text_for_common_words)\nX_train[\"response_a_clean\"] = X_train[\"response_a\"].apply(clean_text_for_common_words)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"].apply(clean_text_for_common_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:07.356042Z","iopub.execute_input":"2025-09-04T10:40:07.356240Z","iopub.status.idle":"2025-09-04T10:40:08.225392Z","shell.execute_reply.started":"2025-09-04T10:40:07.356224Z","shell.execute_reply":"2025-09-04T10:40:08.224717Z"}},"outputs":[],"execution_count":149},{"cell_type":"markdown","source":"embedder = HFEmbedder(model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/3\")\nX_train[\"prompt_clean\"] = X_train[\"prompt\"].apply(clean_text_for_common_words)\nX_train[\"response_a_clean\"] = X_train[\"response_a\"].apply(clean_text_for_common_words)\nX_train[\"response_b_clean\"] = X_train[\"response_b\"].apply(clean_text_for_common_words)\n\ncommon_words_transformer = CommonWordsTransformer()\n# Updated preprocessing transformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", embedder, \"prompt_clean\"),\n    (\"resp_a_embed\", embedder, \"response_a_clean\"),\n    (\"resp_b_embed\", embedder, \"response_a_clean\"),\n    (\"common_words\", common_words_transformer, [\"prompt\", \"response_a\", \"response_b\"]),\n    #(\"num\", \"passthrough\", [\"id\"])\n])\n\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2025-09-02T10:34:24.453915Z","iopub.execute_input":"2025-09-02T10:34:24.456071Z","iopub.status.idle":"2025-09-02T10:34:24.614454Z","shell.execute_reply.started":"2025-09-02T10:34:24.455996Z","shell.execute_reply":"2025-09-02T10:34:24.611660Z"}}},{"cell_type":"code","source":"class HFEmbedder(BaseEstimator, TransformerMixin):\n    def __init__(self, model_path, batch_size=16, max_length=512, use_fast_tokenizer=True, enable_mixed_precision=True):\n        import tensorflow as tf\n        import numpy as np\n        import os\n        import multiprocessing\n\n        self.model_path = model_path\n        self.batch_size = int(batch_size)\n        self.max_length = min(int(max_length), 512)\n        self.use_fast_tokenizer = use_fast_tokenizer\n        self.enable_mixed_precision = enable_mixed_precision\n        self._cpu_count = multiprocessing.cpu_count()\n\n        # GPU memory growth\n        try:\n            gpus = tf.config.list_physical_devices(\"GPU\")\n            if gpus:\n                for g in gpus:\n                    tf.config.experimental.set_memory_growth(g, True)\n        except Exception:\n            pass\n\n        # Mixed precision\n        try:\n            if self.enable_mixed_precision:\n                from tensorflow.keras import mixed_precision\n                mixed_precision.set_global_policy(\"mixed_float16\")\n        except Exception:\n            pass\n\n        # Tokenizer\n        tokenizer_path = os.path.join(self.model_path, \"assets/tokenizer/vocabulary.spm\")\n        try:\n            if self.use_fast_tokenizer:\n                from transformers import DebertaV2TokenizerFast\n                self.tokenizer = DebertaV2TokenizerFast(vocab_file=tokenizer_path)\n            else:\n                from transformers import DebertaV2Tokenizer\n                self.tokenizer = DebertaV2Tokenizer(vocab_file=tokenizer_path)\n        except Exception:\n            from transformers import DebertaV2Tokenizer\n            self.tokenizer = DebertaV2Tokenizer(vocab_file=tokenizer_path)\n\n        self.tokenizer.model_max_length = self.max_length\n\n        # Auto-detect model config\n        config = self._detect_model_config()\n\n        # Load model\n        from keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\n        self.model = DebertaV3Backbone(\n            vocabulary_size=128100,\n            num_layers=config[\"num_layers\"],\n            num_heads=config[\"num_heads\"],\n            hidden_dim=config[\"hidden_dim\"],\n            intermediate_dim=config[\"intermediate_dim\"],\n            dropout=0.1,\n            max_sequence_length=512,\n            bucket_size=256\n        )\n\n        weights_path = os.path.join(self.model_path, \"model.weights.h5\")\n        self.model.load_weights(weights_path)\n\n        print(\"✅ Model input names:\", [input.name for input in self.model.inputs])\n\n        # Warm-up\n        try:\n            dummy_input = {\n                \"padding_mask\": tf.zeros((1, self.max_length), dtype=tf.int32),\n                \"token_ids\": tf.zeros((1, self.max_length), dtype=tf.int32),\n            }\n            _ = self.model(dummy_input)\n        except Exception:\n            pass\n\n    def _detect_model_config(self):\n        import os\n        import json\n\n        config_path = os.path.join(self.model_path, \"config.json\")\n        if os.path.exists(config_path):\n            try:\n                with open(config_path, \"r\") as f:\n                    config = json.load(f)\n                return {\n                    \"num_layers\": config.get(\"num_layers\", 12),\n                    \"num_heads\": config.get(\"num_attention_heads\", 12),\n                    \"hidden_dim\": config.get(\"hidden_size\", 768),\n                    \"intermediate_dim\": config.get(\"intermediate_size\", 3072),\n                }\n            except Exception as e:\n                print(f\"⚠️ Failed to read config.json: {e}\")\n\n        # Fallback: infer from folder name\n        path_lower = self.model_path.lower()\n        if \"small\" in path_lower:\n            return {\"num_layers\": 12, \"num_heads\": 6, \"hidden_dim\": 768, \"intermediate_dim\": 3072}\n        elif \"base\" in path_lower:\n            return {\"num_layers\": 12, \"num_heads\": 12, \"hidden_dim\": 768, \"intermediate_dim\": 3072}\n\n        print(\"⚠️ Could not auto-detect model config. Using default base config.\")\n        return {\"num_layers\": 12, \"num_heads\": 12, \"hidden_dim\": 768, \"intermediate_dim\": 3072}\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        import tensorflow as tf\n        import numpy as np\n\n        if hasattr(X, \"to_list\"):\n            texts = X.to_list()\n        else:\n            texts = list(X)\n\n        def _to_text(item):\n            if isinstance(item, (list, tuple, np.ndarray)):\n                return \" \".join(str(x) for x in item if x)\n            return \"\" if item is None else str(item)\n\n        texts = [_to_text(t) for t in texts]\n        n = len(texts)\n        if n == 0:\n            try:\n                hidden_dim = int(self.model.output_shape[-1])\n                return np.zeros((0, hidden_dim), dtype=np.float32)\n            except Exception:\n                return np.empty((0,))\n\n        try:\n            if self.use_fast_tokenizer:\n                tokens_np = self.tokenizer(\n                    texts,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"np\",\n                )\n                input_ids = tf.cast(tokens_np[\"input_ids\"], tf.int32)\n                attention_mask = tf.cast(tokens_np[\"attention_mask\"], tf.int32)\n\n                ds = tf.data.Dataset.from_tensor_slices(\n                    {\"token_ids\": input_ids, \"padding_mask\": attention_mask}\n                ).batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n\n                outputs = self.model.predict(ds, verbose=0)\n                if isinstance(outputs, dict) and \"last_hidden_state\" in outputs:\n                    outputs = outputs[\"last_hidden_state\"]\n                embeddings = np.mean(outputs, axis=1)\n                return embeddings\n        except Exception as e:\n            print(f\"⚠️ Fast path failed: {e}\")\n\n        # Fallback\n        all_embeddings = []\n        for i in range(0, n, self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            try:\n                tokens = self.tokenizer(\n                    batch,\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.max_length,\n                    return_tensors=\"tf\",\n                )\n                token_ids_tf = tf.cast(tokens[\"input_ids\"], tf.int32)\n                attention_mask_tf = tf.cast(tokens[\"attention_mask\"], tf.int32)\n                model_inputs = {\"padding_mask\": attention_mask_tf, \"token_ids\": token_ids_tf}\n                outputs = self.model(model_inputs)\n                if isinstance(outputs, dict) and \"last_hidden_state\" in outputs:\n                    outputs = outputs[\"last_hidden_state\"]\n                embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            except Exception as e:\n                print(f\"⚠️ Batch fallback failed: {e}\")\n                embeddings = np.zeros((len(batch), self.model.output_shape[-1]), dtype=np.float32)\n\n            all_embeddings.append(embeddings)\n\n        return np.vstack(all_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:08.226182Z","iopub.execute_input":"2025-09-04T10:40:08.226404Z","iopub.status.idle":"2025-09-04T10:40:08.250904Z","shell.execute_reply.started":"2025-09-04T10:40:08.226387Z","shell.execute_reply":"2025-09-04T10:40:08.250230Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"embedder = HFEmbedder(model_path=\"/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3\")\n#/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3\ncommon_words_transformer = CommonWordsTransformer()\n# Updated preprocessing transformer\npreprocessing = ColumnTransformer([\n    (\"prompt_embed\", embedder, \"prompt_clean\"),\n    (\"resp_a_embed\", embedder, \"response_a_clean\"),\n    (\"resp_b_embed\", embedder, \"response_a_clean\"),\n    (\"common_words\", common_words_transformer, [\"prompt\", \"response_a\", \"response_b\"]),\n    #(\"num\", \"passthrough\", [\"id\"])\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:08.251769Z","iopub.execute_input":"2025-09-04T10:40:08.252062Z","iopub.status.idle":"2025-09-04T10:40:15.440658Z","shell.execute_reply.started":"2025-09-04T10:40:08.252043Z","shell.execute_reply":"2025-09-04T10:40:15.438715Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1560147637.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHFEmbedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#/kaggle/input/deberta_v3/keras/deberta_v3_base_en/3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcommon_words_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCommonWordsTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Updated preprocessing transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m preprocessing = ColumnTransformer([\n","\u001b[0;32m/tmp/ipykernel_36/1166875811.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, batch_size, max_length, use_fast_tokenizer, enable_mixed_precision)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Model input names:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_raise_loading_failure\u001b[0;34m(error_msgs, warn_only)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: A total of 48 objects could not be loaded. Example error message for object <Dense name=feedforward_intermediate_dense, built=True>:\n\nLayer 'feedforward_intermediate_dense' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>]"],"ename":"ValueError","evalue":"A total of 48 objects could not be loaded. Example error message for object <Dense name=feedforward_intermediate_dense, built=True>:\n\nLayer 'feedforward_intermediate_dense' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>]","output_type":"error"}],"execution_count":151},{"cell_type":"code","source":"preprocessing","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.441106Z","iopub.status.idle":"2025-09-04T10:40:15.441339Z","shell.execute_reply.started":"2025-09-04T10:40:15.441208Z","shell.execute_reply":"2025-09-04T10:40:15.441217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#feature_selection = SelectKBest(score_func = chi2, k=6)\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfeature_selection = SelectKBest(score_func=f_classif, k=6)\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.442583Z","iopub.status.idle":"2025-09-04T10:40:15.442784Z","shell.execute_reply.started":"2025-09-04T10:40:15.442689Z","shell.execute_reply":"2025-09-04T10:40:15.442697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunk_size = 10000\nchunks = [X_train[i:i + chunk_size] for i in range(0, len(X_train), chunk_size)]\ny_chunks = [y_train[i:i + chunk_size] for i in range(0, len(y_train), chunk_size)]\n\nX_processed_chunks = []\ny_processed_chunks = []\n\n# Fit the preprocessing step once on the full training data\nmy_pipeline = Pipeline([\n    (\"preprocessing\", preprocessing),\n])\nmy_pipeline.named_steps['preprocessing'].fit(X_train)\n\n# Transform each chunk\nfor i, chunk in enumerate(chunks):\n    print(f\"Transforming chunk {i+1}/{len(chunks)}\")\n    processed_chunk = my_pipeline.named_steps['preprocessing'].transform(chunk)\n    X_processed_chunks.append(processed_chunk)\n    y_processed_chunks.append(y_chunks[i])\n\n# Combine all processed chunks\nimport numpy as np\nX_final = np.vstack(X_processed_chunks)\ny_final = np.concatenate(y_processed_chunks)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.444073Z","iopub.status.idle":"2025-09-04T10:40:15.444285Z","shell.execute_reply.started":"2025-09-04T10:40:15.444180Z","shell.execute_reply":"2025-09-04T10:40:15.444189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.dataframe(X_final).to_csv(\"X_final.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.445380Z","iopub.status.idle":"2025-09-04T10:40:15.445745Z","shell.execute_reply.started":"2025-09-04T10:40:15.445591Z","shell.execute_reply":"2025-09-04T10:40:15.445605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBClassifier(\n    objective=\"multi:softprob\",  \n    num_class=3,                  \n    eval_metric=\"mlogloss\",       \n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42\n)","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.446911Z","iopub.status.idle":"2025-09-04T10:40:15.447249Z","shell.execute_reply.started":"2025-09-04T10:40:15.447077Z","shell.execute_reply":"2025-09-04T10:40:15.447091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_pipeline = Pipeline([\n    (\"preprocessing\", preprocessing),\n    #(\"feature_selection\", feature_selection),\n    (\"model\", model)\n])\nmy_pipeline","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.448082Z","iopub.status.idle":"2025-09-04T10:40:15.448291Z","shell.execute_reply.started":"2025-09-04T10:40:15.448183Z","shell.execute_reply":"2025-09-04T10:40:15.448191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X_final, y_final, cv=3, scoring=\"accuracy\")\nprint(\"Cross-validated Accuracy:\", scores.mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.449600Z","iopub.status.idle":"2025-09-04T10:40:15.449936Z","shell.execute_reply.started":"2025-09-04T10:40:15.449763Z","shell.execute_reply":"2025-09-04T10:40:15.449778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(X_final, y_final)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.451106Z","iopub.status.idle":"2025-09-04T10:40:15.451414Z","shell.execute_reply.started":"2025-09-04T10:40:15.451255Z","shell.execute_reply":"2025-09-04T10:40:15.451268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"prompt_clean\"] = df_test[\"prompt\"].apply(clean_text_for_common_words)\ndf_test[\"response_a_clean\"] = df_test[\"response_a\"].apply(clean_text_for_common_words)\ndf_test[\"response_b_clean\"] = df_test[\"response_b\"].apply(clean_text_for_common_words)","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.451982Z","iopub.status.idle":"2025-09-04T10:40:15.452207Z","shell.execute_reply.started":"2025-09-04T10:40:15.452094Z","shell.execute_reply":"2025-09-04T10:40:15.452106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DebertaV2Tokenizer\nimport torch\nimport numpy as np\nfrom keras_hub.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone\nimport tensorflow as tf\n\n# Configuration\nmodel_path = \"/kaggle/input/deberta_v3/keras/deberta_v3_small_en/3\"\nvocab_file = f\"{model_path}/assets/tokenizer/vocabulary.spm\"\nweights_file = f\"{model_path}/model.weights.h5\"\n\n# Initialize tokenizer\ntokenizer = DebertaV2Tokenizer(vocab_file=vocab_file)\nprint(\"✅ Tokenizer initialized:\", type(tokenizer))\n\n# Initialize model with architecture matching the weights\nmodel = DebertaV3Backbone(\n    vocabulary_size=128100,\n    num_layers=12,\n    num_heads=12,  # <-- updated\n    hidden_dim=768,\n    intermediate_dim=3072,\n    dropout=0.1,\n    max_sequence_length=512,\n    bucket_size=256\n)\n\nmodel.load_weights(weights_file, skip_mismatch=True)\nprint(\"✅ Model loaded and weights applied.\")\n\n# Display model summary\ntry:\n    model.summary()\nexcept Exception as e:\n    print(\"⚠️ Could not display model summary:\", e)\n\n# Example input text\ntext = \"DeBERTa is a powerful transformer model.\"\n\n# Tokenize the input\ntokens = tokenizer(text, return_tensors=\"pt\")\ninput_ids = tokens[\"input_ids\"]\nattention_mask = tokens[\"attention_mask\"]\n\n# Convert token IDs to token strings\ntoken_names = tokenizer.convert_ids_to_tokens(input_ids[0])\nprint(\"✅ Token names:\", token_names)\n\n# Convert PyTorch tensors to NumPy arrays\ntoken_ids_np = input_ids.cpu().numpy()\npadding_mask_np = attention_mask.cpu().numpy()\n\n# Convert to TensorFlow tensors\nmodel_inputs = {\n    \"token_ids\": tf.convert_to_tensor(token_ids_np),\n    \"padding_mask\": tf.convert_to_tensor(padding_mask_np)\n}\n\n# Run forward pass\nprint(\"🚀 Running model forward pass...\")\noutputs = model(model_inputs)\n\n# Inspect model output\nif isinstance(outputs, dict):\n    print(\"✅ Model output keys:\", outputs.keys())\n    if \"last_hidden_state\" in outputs:\n        hidden_state = outputs[\"last_hidden_state\"]\n        print(\"Last hidden state shape:\", hidden_state.shape)\n\n        # Mean pooling\n        embeddings = tf.reduce_mean(hidden_state, axis=1).numpy()\n        print(\"✅ Mean pooled embeddings shape:\", embeddings.shape)\n    else:\n        print(\"⚠️ 'last_hidden_state' not found in model output.\")\nelse:\n    print(\"⚠️ Model returned a non-dict output:\", type(outputs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:50:15.765176Z","iopub.execute_input":"2025-09-04T10:50:15.766038Z","iopub.status.idle":"2025-09-04T10:50:25.234849Z","shell.execute_reply.started":"2025-09-04T10:50:15.766002Z","shell.execute_reply":"2025-09-04T10:50:25.234146Z"}},"outputs":[{"name":"stdout","text":"✅ Tokenizer initialized: <class 'transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer'>\n✅ Model loaded and weights applied.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:629: UserWarning: A total of 48 objects could not be loaded. Example error message for object <Dense name=feedforward_intermediate_dense, built=True>:\n\nLayer 'feedforward_intermediate_dense' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>, <Dense name=feedforward_intermediate_dense, built=True>, <LayerNormalization name=feedforward_layer_norm, built=True>, <Dense name=feedforward_output_dense, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=self_attention_layer_norm, built=True>]\n  warnings.warn(msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"deberta_v3_backbone_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"deberta_v3_backbone_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m98,380,800\u001b[0m │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mReversibleEmbeddi…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_layer_n… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │      \u001b[38;5;34m1,536\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_dropout  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ embeddings_layer… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ rel_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │    \u001b[38;5;34m394,752\u001b[0m │ embeddings_dropo… │\n│ (\u001b[38;5;33mRelativeEmbedding\u001b[0m) │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ embeddings_dropo… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ disentangled_att… │\n│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ rel_embedding[\u001b[38;5;34m0\u001b[0m]… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">98,380,800</span> │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbeddi…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_layer_n… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_dropout  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embeddings_layer… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ rel_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,752</span> │ embeddings_dropo… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RelativeEmbedding</span>) │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ embeddings_dropo… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ disentangled_att… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m183,831,552\u001b[0m (701.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">183,831,552</span> (701.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m183,831,552\u001b[0m (701.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">183,831,552</span> (701.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"✅ Token names: ['[CLS]', '▁De', 'BERT', 'a', '▁is', '▁a', '▁powerful', '▁transformer', '▁model', '.', '[SEP]']\n🚀 Running model forward pass...\n⚠️ Model returned a non-dict output: <class 'tensorflow.python.framework.ops.EagerTensor'>\n","output_type":"stream"}],"execution_count":152},{"cell_type":"code","source":"for i, layer in enumerate(model.layers):\n    try:\n        print(f\"{i}: {layer.name} - {layer.output_shape}\")\n    except AttributeError:\n        print(f\"{i}: {layer.name} - (no output_shape)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.454584Z","iopub.status.idle":"2025-09-04T10:40:15.454824Z","shell.execute_reply.started":"2025-09-04T10:40:15.454687Z","shell.execute_reply":"2025-09-04T10:40:15.454695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.456235Z","iopub.status.idle":"2025-09-04T10:40:15.456529Z","shell.execute_reply.started":"2025-09-04T10:40:15.456389Z","shell.execute_reply":"2025-09-04T10:40:15.456404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\nwith h5py.File(weights_file, \"r\") as f:\n    print(list(f.keys()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:40:15.457511Z","iopub.status.idle":"2025-09-04T10:40:15.457859Z","shell.execute_reply.started":"2025-09-04T10:40:15.457675Z","shell.execute_reply":"2025-09-04T10:40:15.457693Z"}},"outputs":[],"execution_count":null}]}